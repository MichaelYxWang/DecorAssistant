{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e5cef3f-197e-443c-a29d-dd57e66606a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from text_preprocessing import Tokenizer, pad_sequences\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498316a-1b06-444d-a007-b9bac95a2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "BASE_DIR = \"../dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7805788-e626-4e43-859a-1ae4b935f0e7",
   "metadata": {
    "id": "r9tNjmBYeon6",
    "tags": []
   },
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "333006e7-80d3-4115-8752-8f4add91c346",
   "metadata": {
    "id": "7DW6oxGPmC9-"
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = BASE_DIR + \"text_data/\"\n",
    "IMAGES_DIR = BASE_DIR + \"images/all_items/\"\n",
    "\n",
    "# Global Parameter Variables\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "NUM_WORDS_TOKENIZER = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a713ae50-dddd-4605-b09f-2222d8a56c04",
   "metadata": {
    "id": "qmCx_kVEmF-s"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(path):\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.resize(img, (256, 256))\n",
    "  img = img.astype(np.float32) / 255\n",
    "  return np.moveaxis(img, 2, 0)\n",
    "\n",
    "def read_pickle(fn):\n",
    "\twith open(fn, \"rb\") as f:\n",
    "\t\treturn pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d10f6439-4ed5-40ba-9a9e-94bd752e72fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abwrNXYP8ibS",
    "outputId": "87f87e2d-b55a-490f-c05e-0a9d376f90f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/890.333.75.jpg does not exist\n",
      "images/991.333.98.jpg does not exist\n",
      "images/990.612.97.jpg does not exist\n"
     ]
    }
   ],
   "source": [
    "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
    "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
    "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
    "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
    "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
    "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
    "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
    "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
    "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
    "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
    "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
    "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
    "\n",
    "room_to_items = {}\n",
    "\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "  if not os.path.exists(IMAGES_DIR + item_id + \".jpg\"):\n",
    "      print(item_url + \" does not exist\")\n",
    "      continue\n",
    "\n",
    "  for room_url in room_url_list:\n",
    "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if room_id not in room_to_items:\n",
    "      room_to_items[room_id] = [item_id]\n",
    "    else:\n",
    "      room_to_items[room_id].append(item_id)\n",
    "    \n",
    "with open(BASE_DIR + \"train_sets_reweighted.pkl\", \"rb\") as file:\n",
    "    train_sets = pickle.load(file)\n",
    "with open(BASE_DIR + \"val_data_reweighted.pkl\", \"rb\") as file:\n",
    "    val_pairs, y_val = pickle.load(file)\n",
    "with open(BASE_DIR + \"preprocessed_text.pkl\", \"rb\") as file:\n",
    "    item_to_info = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2cf40b4e-3474-49ed-9629-d385b37f8f6c",
   "metadata": {
    "id": "jtQJWkJ0fEPT"
   },
   "outputs": [],
   "source": [
    "class FurnitureImagePairsDataset(Dataset):\n",
    "    \"\"\"Dataset containing pairs of furniture items.\"\"\"\n",
    "\n",
    "    def __init__(self, image_path, pairs, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_path (string): Path to the directory containing images.\n",
    "            pairs (list of tuples of strings): Pairs of image IDs to be used as training samples.\n",
    "            labels (array of integers): Labels for the training samples.\n",
    "        \"\"\"\n",
    "        super(FurnitureImagePairsDataset, self).__init__()\n",
    "        self.image_ids = list(set(x for pair in pairs for x in pair))\n",
    "        self.index_mapping = {image_id: i for i, image_id in enumerate(self.image_ids)}\n",
    "        self.images = [preprocess_img(image_path + image_id + \".jpg\") for image_id in tqdm.tqdm(self.image_ids, ncols=80)]\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if isinstance(idx, (list, tuple)):\n",
    "            x1, x2, y = zip(*[self[i] for i in idx])\n",
    "            return torch.stack(x1), torch.stack(x2), torch.from_numpy(np.array(y))\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "        return self.images[self.index_mapping[pair[0]]], self.images[self.index_mapping[pair[1]]], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f91bd-5770-4c64-9fda-18ead29560c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "This generates `text_preprocessing_reweighted.pkl` - don't need to run if you already have this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf801362-6ec2-4752-a9fa-6e409ca9a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "# !gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "PATH_TO_WORD2VEC = \"../../../research/uncertainty_benchmark/embeddings/GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54bd8807-924e-4174-a520-0a79509670c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lO1ZkgcUK1m7",
    "outputId": "b13d2de8-5d78-4e91-81c5-32011f50f847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2005 unique tokens.\n",
      "Max len: 100\n",
      "total embedded: 1907 common words\n"
     ]
    }
   ],
   "source": [
    "word2vecDict = KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True)\n",
    "\n",
    "def get_embedding_matrix(word_index):\n",
    "    # One for zero, one for len(WORD_INDEX) which will be used as the EOT token\n",
    "    embedding_matrix = np.random.randn(len(word_index)+2, EMBEDDING_DIM)\n",
    "    embedding_matrix /= np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "    embeddedCount = 0\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word2vecDict[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount += 1\n",
    "    print(\"total embedded:\", embeddedCount, \"common words\")\n",
    "    return embedding_matrix\n",
    "\n",
    "train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
    "train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
    "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
    "WORD_INDEX = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
    "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
    "\n",
    "X_train_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_premise_texts)]\n",
    "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_train_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_hypothesis_texts)]\n",
    "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bcbeab55-c8c9-4d4e-8d16-9e9fc1024475",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_preprocessing_info_reweighted.pkl\", \"wb\") as file:\n",
    "    pickle.dump((WORD_INDEX, WORD2VEC_EMBEDDING_MATRIX), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c4e5f-850c-4e37-a4f7-31c782ec5c1b",
   "metadata": {},
   "source": [
    "# Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57071d10-dce0-4bb3-929e-ea65d8cc20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_RELU_LAYERS = [3, 8, 15, 22, 29]\n",
    "VGG_RELU_LAYER_SIZES = [64, 128, 256, 512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "099fc49d-0bc9-4928-ad4c-91c77c556b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OutputClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OutputClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, first_embs, second_embs):\n",
    "        output = torch.cat([first_embs, second_embs], 1) # (emb_1 * emb_2).sum(1)\n",
    "        output = self.fc1(output)\n",
    "        output = self.dropout1(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "        \n",
    "class IntermediateFusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_matrix, stats_layer_idxs, stats_dims, output_dim, pre_fusion_layers=0):\n",
    "        super(IntermediateFusionModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pre_fusion_layers = pre_fusion_layers\n",
    "\n",
    "        self.vgg_model = models.vgg16(pretrained=True)\n",
    "        self.image_encoder = self.vgg_model._modules.get(\"features\")\n",
    "        \n",
    "        self.stats_layer_idxs = stats_layer_idxs\n",
    "        self.stats_dims = stats_dims\n",
    "        self.image_fcs = nn.ModuleList()\n",
    "        self.image_dropouts = nn.ModuleList()\n",
    "\n",
    "        for dim in self.stats_dims:\n",
    "            self.image_fcs.append(nn.Linear(dim * 2, embedding_dim))\n",
    "            self.image_dropouts.append(nn.Dropout(0.5))\n",
    "\n",
    "        self.text_emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.text_emb.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.text_emb.weight.requires_grad = True\n",
    "\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.attention_lns = nn.ModuleList()\n",
    "        for i in range(pre_fusion_layers + len(self.stats_dims)):\n",
    "            self.attention_layers.append(nn.MultiheadAttention(embedding_dim, 1))\n",
    "            self.attention_lns.append(nn.LayerNorm(embedding_dim))\n",
    "        self.text_projection = nn.Parameter(torch.empty(embedding_dim, 128))\n",
    "                \n",
    "        self.classifier = OutputClassifier(256, output_dim)\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.embedding_dim ** -0.5)\n",
    "\n",
    "    def encode(self, text_inp, img_inp):\n",
    "\n",
    "        enc_inp = img_inp\n",
    "        stats_outputs = [None for _ in self.stats_layer_idxs]\n",
    "        # img_outputs = [None for _ in self.stats_layer_idxs]\n",
    "        for i in range(len(self.image_encoder)): # max(self.stats_layer_idxs) + 1):\n",
    "            enc_inp = self.image_encoder[i](enc_inp)\n",
    "            if i in self.stats_layer_idxs:\n",
    "                idx = self.stats_layer_idxs.index(i)\n",
    "                stat_enc = enc_inp.flatten(2)\n",
    "                stat_enc = torch.cat([stat_enc.mean(2), stat_enc.std(2)], 1).squeeze()\n",
    "                \n",
    "                stat_fc = self.image_fcs[idx](stat_enc)\n",
    "                stat_fc = F.relu(stat_fc)\n",
    "                stat_fc = self.image_dropouts[idx](stat_fc)\n",
    "                stats_outputs[idx] = stat_fc\n",
    "        \n",
    "        text_enc = self.text_emb(text_inp).transpose(0, 1)\n",
    "        # text_enc shape: (seq length, batch size, embedding dim)\n",
    "        # We will add one value to the sequence (first axis) at each layer, \n",
    "        # representing the instance norm statistics from VGG\n",
    "    \n",
    "        for attn, lnorm in zip(self.attention_layers[:self.pre_fusion_layers], self.attention_lns[:self.pre_fusion_layers]):\n",
    "            text_enc, _ = attn(text_enc, text_enc, text_enc)\n",
    "            text_enc = lnorm(text_enc.transpose(0, 1)).transpose(0, 1)\n",
    "            \n",
    "        for stats, attn, lnorm in zip(stats_outputs, self.attention_layers[self.pre_fusion_layers:], self.attention_lns[self.pre_fusion_layers:]):\n",
    "            text_enc = torch.cat([stats.unsqueeze(0), text_enc], 0)\n",
    "            text_enc, _ = attn(text_enc, text_enc, text_enc)\n",
    "            text_enc = lnorm(text_enc.transpose(0, 1)).transpose(0, 1)\n",
    "            \n",
    "        text_enc = text_enc.transpose(0, 1)\n",
    "        text_enc = text_enc[torch.arange(text_enc.size(0)), text_inp.argmax(dim=-1) + len(stats_outputs)] @ self.text_projection\n",
    "                \n",
    "        return text_enc\n",
    "\n",
    "    def forward(self, text_inp1, text_inp2, img_inp1, img_inp2):\n",
    "        emb_1 = self.encode(text_inp1, img_inp1)\n",
    "        emb_2 = self.encode(text_inp2, img_inp2)\n",
    "        output = self.classifier(emb_1, emb_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d850b3a-ab22-409a-bce5-e222218c7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak input parameters to model constructor here - they will be used in training loops below\n",
    "def make_model():\n",
    "    return IntermediateFusionModel(len(WORD_INDEX)+1,\n",
    "                                   WORD2VEC_EMBEDDING_MATRIX,\n",
    "                                   [VGG_RELU_LAYERS[0], VGG_RELU_LAYERS[2]],\n",
    "                                   [VGG_RELU_LAYER_SIZES[0], VGG_RELU_LAYER_SIZES[2]],\n",
    "                                   1,\n",
    "                                   pre_fusion_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20455b78-1f61-49c5-9016-b13a82a7d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "MAIN_LEARNING_RATE = 1e-5\n",
    "FINE_TUNE_LEARNING_RATE = 1e-7\n",
    "EPOCHS = 20\n",
    "CLIP = 5\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fb64a7b-fc3d-4d1c-b8bc-9eb1be295059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to train and evaluate model\n",
    "\n",
    "def train_model(model, criterion, optimizer, text_train_loader, img_train_loader):\n",
    "    \"\"\"Train model for one epoch and return accuracy and loss on training set\"\"\"\n",
    "    model.train()\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for lstm, cnn in tqdm.tqdm(zip(text_train_loader, img_train_loader),\n",
    "                               total=len(text_train_loader),\n",
    "                               ncols=80):\n",
    "        lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
    "        cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
    "        lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.long().to(DEVICE), lstm_inp2.long().to(DEVICE), lstm_labels.to(DEVICE)\n",
    "        cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
    "        loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "  \n",
    "    return total_acc_train / len(text_train_loader), total_loss_train / len(text_train_loader)\n",
    "\n",
    "def eval_model_pairs(model, criterion, optimizer, text_val_loader, img_val_loader):\n",
    "    \"\"\"Return accuracy and loss on a validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in tqdm.tqdm(zip(text_val_loader, img_val_loader),\n",
    "                                   total=len(text_val_loader),\n",
    "                                   ncols=80):\n",
    "            lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
    "            cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
    "            lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.long().to(DEVICE), lstm_inp2.long().to(DEVICE), lstm_labels.to(DEVICE)\n",
    "            cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
    "            val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += val_loss.item()\n",
    "    return total_acc_val / len(text_val_loader), total_loss_val / len(text_val_loader)\n",
    "\n",
    "def eval_model_single(model, single_loader):\n",
    "    \"\"\"Returns embeddings for each product in the single product loader.\"\"\"\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for image_inp, text_inp in tqdm.tqdm(single_loader,\n",
    "                                             total=len(single_loader),\n",
    "                                             ncols=80):\n",
    "            image_inp = image_inp.to(DEVICE)\n",
    "            text_inp = text_inp.long().to(DEVICE)\n",
    "            embs.append(model.encode(text_inp, image_inp).cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29b8c3-b431-4d5d-a169-1904630066fd",
   "metadata": {},
   "source": [
    "# Single Training Loop\n",
    "\n",
    "Run this code to just train the model on the first of `train_sets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60843829-34f9-474b-8428-568a101c9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2030 unique tokens.\n",
      "Max len: 100\n"
     ]
    }
   ],
   "source": [
    "train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
    "train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
    "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
    "\n",
    "with open(\"text_preprocessing_info_reweighted.pkl\", \"rb\") as file:\n",
    "    WORD_INDEX, WORD2VEC_EMBEDDING_MATRIX = pickle.load(file)\n",
    "    assert WORD_INDEX == tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_premise_texts)]\n",
    "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_train_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_hypothesis_texts)]\n",
    "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5999adb3-3daa-4675-a79d-2b5aaac6a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, only use the first training set. We will try using PU bagging later.\n",
    "train_pairs, y_train, train_scenes = train_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "628401ec-7277-4ceb-9685-858bf10b40e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQygJsUIOMeb",
    "outputId": "c43bde63-7efb-4ee5-b2be-ec8c09e1b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1379/1379 [00:04<00:00, 292.15it/s]\n",
      "100%|████████████████████████████████████████| 664/664 [00:02<00:00, 291.29it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_image = FurnitureImagePairsDataset(IMAGES_DIR, train_pairs, y_train)\n",
    "X_val_image = FurnitureImagePairsDataset(IMAGES_DIR, val_pairs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d36d278a-6a04-48fb-9c8b-c9bb60c82521",
   "metadata": {
    "id": "0iAakls6kMkX"
   },
   "outputs": [],
   "source": [
    "val_premise_texts = [item_to_info[id] for id, _ in val_pairs]\n",
    "val_hypothesis_texts = [item_to_info[id] for _, id in val_pairs]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "X_val_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_premise_texts)]\n",
    "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_val_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_hypothesis_texts)]\n",
    "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20bffe01-bfa7-4ef0-8109-9e951b8fd2d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Vm9RcV-MWk7",
    "outputId": "d8c7ec3c-e9b4-4ec4-ddc9-833d9af0e9c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919 919\n",
      "218 218\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "img_train_data = X_train_image\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise),\n",
    "                                torch.from_numpy(X_train_text_hypothesis),\n",
    "                                torch.from_numpy(y_train))\n",
    "\n",
    "img_val_data = X_val_image\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise),\n",
    "                              torch.from_numpy(X_val_text_hypothesis), \n",
    "                              torch.from_numpy(y_val))\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(len(text_train_loader), len(img_train_loader))\n",
    "print(len(text_val_loader), len(img_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30cbd911-e91c-4be4-8320-376b30e57ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQygJsUIOMeb",
    "outputId": "c43bde63-7efb-4ee5-b2be-ec8c09e1b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [00:06<00:00, 337.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Also load a dataset of individual products to get embeddings\n",
    "\n",
    "val_product_ids = sorted(list(set(x for pair in train_pairs + val_pairs for x in pair)))\n",
    "single_images = torch.stack([torch.from_numpy(preprocess_img(IMAGES_DIR + image_id + \".jpg\"))\n",
    "                             for image_id in tqdm.tqdm(val_product_ids)])\n",
    "\n",
    "single_texts = [item_to_info[id] for id in val_product_ids]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "single_texts = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(single_texts)]\n",
    "single_texts = torch.from_numpy(pad_sequences(single_texts, maxlen=MAX_SEQUENCE_LENGTH, padding='post'))\n",
    "\n",
    "single_data = TensorDataset(single_images, single_texts)\n",
    "single_loader = DataLoader(single_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf0473-b624-4ed0-bd9f-d0213f957141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "xX2rumtEQYv8",
    "outputId": "3df96bb3-06ce-4d95-bac3-d61fe628a3ac"
   },
   "outputs": [],
   "source": [
    "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
    "\n",
    "model = make_model()\n",
    "# state = torch.load(\"checkpoint_intermediate_fusion.p\", map_location=torch.device('cpu'))\n",
    "# model.load_state_dict(state)\n",
    "\n",
    "for param in model.vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in model.image_encoder.parameters():\n",
    "#     param.requires_grad = True\n",
    "model.to(DEVICE)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MAIN_LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "print(\"Training Started...\")\n",
    "last_loss = 1e9\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    train_acc, train_loss = train_model(model, criterion, optimizer, text_train_loader, img_train_loader)\n",
    "    val_acc, val_loss = eval_model_pairs(model, criterion, optimizer, text_val_loader, img_val_loader)\n",
    "    \n",
    "    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    torch.cuda.empty_cache()\n",
    "    if val_loss < last_loss:\n",
    "        torch.save(model.state_dict(), \"checkpoint_fine_tune.p\")\n",
    "    else:\n",
    "        print(\"Loss increased - early stopping.\")\n",
    "        break\n",
    "    last_loss = val_loss\n",
    "    \n",
    "embs = eval_model_single(model, single_loader)\n",
    "\n",
    "with open(\"embeddings_fine_tune_{}.p\".format(model_number), \"wb\") as file:\n",
    "    pickle.dump((val_product_ids, embs), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b12da47-3b94-4b67-be19-4c8fd313489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:07<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "state = torch.load(\"intermediate_fusion_controls/checkpoint_text_only.p\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state)\n",
    "\n",
    "embs = eval_model_single(model, single_loader)\n",
    "\n",
    "with open(\"intermediate_fusion_controls/embeddings_text_only.p\", \"wb\") as file:\n",
    "    pickle.dump((val_product_ids, embs), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9539a75-3b6b-4120-8d81-6c856b20ccea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PU Bagging Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8996e01d-d311-499c-a0ee-ba0bf3828178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2030 unique tokens.\n",
      "Max len: 100\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer for text processing based on the first training set\n",
    "train_premise_texts = [item_to_info[id] for id, _ in train_sets[0][0]]\n",
    "train_hypothesis_texts = [item_to_info[id] for _, id in train_sets[0][0]]\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
    "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
    "\n",
    "with open(\"text_preprocessing_info.pkl\", \"rb\") as file:\n",
    "    WORD_INDEX, WORD2VEC_EMBEDDING_MATRIX = pickle.load(file)\n",
    "    assert WORD_INDEX == tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fc6c3e7-07d1-4394-8f6e-df1f88ab9aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 704/704 [00:02<00:00, 282.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize validation data - same as above\n",
    "\n",
    "X_val_image = FurnitureImagePairsDataset(IMAGES_DIR, val_pairs, y_val)\n",
    "\n",
    "val_premise_texts = [item_to_info[id] for id, _ in val_pairs]\n",
    "val_hypothesis_texts = [item_to_info[id] for _, id in val_pairs]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "X_val_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_premise_texts)]\n",
    "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_val_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_hypothesis_texts)]\n",
    "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "img_val_data = X_val_image\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise),\n",
    "                              torch.from_numpy(X_val_text_hypothesis), \n",
    "                              torch.from_numpy(y_val))\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03b35df4-113a-41d4-8f92-a664cd31bc62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQygJsUIOMeb",
    "outputId": "c43bde63-7efb-4ee5-b2be-ec8c09e1b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [00:06<00:00, 337.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Also load a dataset of individual products to get embeddings\n",
    "\n",
    "val_product_ids = sorted(list(set(x for pair in train_pairs + val_pairs for x in pair)))\n",
    "single_images = torch.stack([torch.from_numpy(preprocess_img(IMAGES_DIR + image_id + \".jpg\"))\n",
    "                             for image_id in tqdm.tqdm(val_product_ids)])\n",
    "\n",
    "single_texts = [item_to_info[id] for id in val_product_ids]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "single_texts = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(single_texts)]\n",
    "single_texts = torch.from_numpy(pad_sequences(single_texts, maxlen=MAX_SEQUENCE_LENGTH, padding='post'))\n",
    "\n",
    "single_data = TensorDataset(single_images, single_texts)\n",
    "single_loader = DataLoader(single_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1488c8c-b20c-463b-81e6-8ff9db793cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:04<00:00, 294.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.78it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.0953 train_acc: 0.5168 | val_loss: 0.7048 val_acc: 0.5391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.78it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6968 train_acc: 0.5397 | val_loss: 0.6911 val_acc: 0.5557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6789 train_acc: 0.5676 | val_loss: 0.6809 val_acc: 0.5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6626 train_acc: 0.5953 | val_loss: 0.6727 val_acc: 0.5873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.73it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6468 train_acc: 0.6167 | val_loss: 0.6675 val_acc: 0.5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6325 train_acc: 0.6365 | val_loss: 0.6609 val_acc: 0.6110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:38<00:00,  5.79it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss: 0.6182 train_acc: 0.6545 | val_loss: 0.6621 val_acc: 0.6168\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5977 train_acc: 0.6774 | val_loss: 0.6534 val_acc: 0.6186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.5944 train_acc: 0.6799 | val_loss: 0.6534 val_acc: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.5939 train_acc: 0.6812 | val_loss: 0.6534 val_acc: 0.6204\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:05<00:00, 281.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.0403 train_acc: 0.5185 | val_loss: 0.6996 val_acc: 0.5341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6928 train_acc: 0.5379 | val_loss: 0.6914 val_acc: 0.5479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6810 train_acc: 0.5624 | val_loss: 0.6865 val_acc: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6671 train_acc: 0.5896 | val_loss: 0.6762 val_acc: 0.5860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6518 train_acc: 0.6135 | val_loss: 0.6743 val_acc: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6364 train_acc: 0.6368 | val_loss: 0.6759 val_acc: 0.6013\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6155 train_acc: 0.6623 | val_loss: 0.6583 val_acc: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6132 train_acc: 0.6660 | val_loss: 0.6582 val_acc: 0.6106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6124 train_acc: 0.6675 | val_loss: 0.6583 val_acc: 0.6111\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:05<00:00, 288.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.0637 train_acc: 0.5192 | val_loss: 0.7034 val_acc: 0.5203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6929 train_acc: 0.5368 | val_loss: 0.6931 val_acc: 0.5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6805 train_acc: 0.5577 | val_loss: 0.6857 val_acc: 0.5474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:38<00:00,  5.80it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6655 train_acc: 0.5903 | val_loss: 0.6787 val_acc: 0.5643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6498 train_acc: 0.6165 | val_loss: 0.6751 val_acc: 0.5755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6346 train_acc: 0.6371 | val_loss: 0.6687 val_acc: 0.5873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss: 0.6199 train_acc: 0.6556 | val_loss: 0.6680 val_acc: 0.5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss: 0.6091 train_acc: 0.6676 | val_loss: 0.6653 val_acc: 0.6028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_loss: 0.5961 train_acc: 0.6799 | val_loss: 0.6656 val_acc: 0.6052\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5758 train_acc: 0.7011 | val_loss: 0.6574 val_acc: 0.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.5734 train_acc: 0.7011 | val_loss: 0.6574 val_acc: 0.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.5717 train_acc: 0.7044 | val_loss: 0.6576 val_acc: 0.6138\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:04<00:00, 291.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.2026 train_acc: 0.5249 | val_loss: 0.7044 val_acc: 0.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6936 train_acc: 0.5408 | val_loss: 0.6925 val_acc: 0.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6801 train_acc: 0.5595 | val_loss: 0.6858 val_acc: 0.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6656 train_acc: 0.5869 | val_loss: 0.6801 val_acc: 0.5783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6509 train_acc: 0.6108 | val_loss: 0.6773 val_acc: 0.5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6363 train_acc: 0.6323 | val_loss: 0.6756 val_acc: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss: 0.6236 train_acc: 0.6473 | val_loss: 0.6724 val_acc: 0.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss: 0.6096 train_acc: 0.6647 | val_loss: 0.6751 val_acc: 0.6101\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5913 train_acc: 0.6874 | val_loss: 0.6570 val_acc: 0.6193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:37<00:00,  2.73it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.5859 train_acc: 0.6899 | val_loss: 0.6569 val_acc: 0.6204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:37<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.5864 train_acc: 0.6909 | val_loss: 0.6569 val_acc: 0.6216\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:05<00:00, 289.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:34<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 1.7823 train_acc: 0.5198 | val_loss: 0.7122 val_acc: 0.5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:34<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6933 train_acc: 0.5407 | val_loss: 0.7009 val_acc: 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6800 train_acc: 0.5622 | val_loss: 0.6922 val_acc: 0.5401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▌                           | 303/919 [00:52<01:46,  5.80it/s]"
     ]
    }
   ],
   "source": [
    "def training_loop(model, lr, n_epochs, text_train_loader, img_train_loader, text_val_loader, img_val_loader):\n",
    "    criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    last_loss = 1e9\n",
    "    for i in range(n_epochs):\n",
    "        train_acc, train_loss = train_model(model, criterion, optimizer, text_train_loader, img_train_loader)\n",
    "        val_acc, val_loss = eval_model_pairs(model, criterion, optimizer, text_val_loader, img_val_loader)\n",
    "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if val_loss > last_loss:\n",
    "            print(\"Loss increased - early stopping.\")\n",
    "            break\n",
    "        last_loss = val_loss\n",
    "\n",
    "for train_set_num, (train_pairs, y_train, train_scenes) in enumerate(train_sets):\n",
    "    print(\"TRAINING SET\", train_set_num)\n",
    "    # Empty caches\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize image dataset\n",
    "    img_train_data = FurnitureImagePairsDataset(IMAGES_DIR, train_pairs, y_train)\n",
    "    \n",
    "    # Initialize text dataset\n",
    "    train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
    "    train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
    "\n",
    "    X_train_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_premise_texts)]\n",
    "    X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    X_train_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_hypothesis_texts)]\n",
    "    X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise),\n",
    "                                    torch.from_numpy(X_train_text_hypothesis),\n",
    "                                    torch.from_numpy(y_train))\n",
    "\n",
    "    text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
    "    img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Build model\n",
    "    model = make_model()\n",
    "    for param in model.vgg_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.to(DEVICE)\n",
    "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    training_loop(model, MAIN_LEARNING_RATE, EPOCHS,\n",
    "                  text_train_loader, img_train_loader,\n",
    "                  text_val_loader, img_val_loader)\n",
    "    \n",
    "    # Fine-tune VGG parameters - this is pretty slow and can be omitted\n",
    "    for param in model.image_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    training_loop(model, FINE_TUNE_LEARNING_RATE, EPOCHS,\n",
    "                  text_train_loader, img_train_loader,\n",
    "                  text_val_loader, img_val_loader)\n",
    "    \n",
    "    embs = eval_model_single(model, single_loader)\n",
    "    with open(\"embeddings_{}.p\".format(train_set_num), \"wb\") as file:\n",
    "        pickle.dump((val_product_ids, embs), file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
