{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multimodal_vae_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JvNGzXEy7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5e13ad-f11f-461c-ccb1-0ab38d3fd4d9"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 01:18:22--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.166.176\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.166.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  47.2MB/s    in 34s     \n",
            "\n",
            "2021-12-01 01:18:56 (46.4 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTwmizF0d3yQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60b6f09-a677-4d65-d12c-34fdb060f76b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLocJQsNqoT"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import itertools\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "# random.seed(517)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwYoWs8Ns-S"
      },
      "source": [
        "# Global Path Vairables\n",
        "# ROOT_DIR =  \"drive/MyDrive/DecorAssist/\"\n",
        "# DATASET_DIR = ROOT_DIR + \"IKEA/text_data/\"\n",
        "# IMAGES_DIR = ROOT_DIR + \"IKEA/images/all_items/\"\n",
        "ROOT_DIR =  \"/content/drive/Othercomputers/My MacBook Pro/GitHub/DecorAssistant\"\n",
        "DATASET_DIR = ROOT_DIR + \"/dataset/text_data/\"\n",
        "IMAGES_DIR = ROOT_DIR + \"/dataset/images/all_items/\"\n",
        "\n",
        "# Global Parameter Variables\n",
        "MAX_SEQUENCE_LENGTH = 1950\n",
        "NUM_WORDS_TOKENIZER = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 16\n",
        "POSITIVE_SIZE = 1000 # We might only use a subset of the positive pairs\n",
        "TRAIN_TEST_RATIO = 0.33\n",
        "\n",
        "# Model Hyperparameters\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LEARNING_RATE = 2e-5 # 0.001\n",
        "HIDDEN_DIM = 64 # 64\n",
        "N_LAYERS = 2 # 2\n",
        "EPOCHS = 10\n",
        "CLIP = 5\n",
        "DROPOUT = 0.1"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW6zSRqb9i5e"
      },
      "source": [
        "def preprocess_img(path):\n",
        "  img = cv2.imread(path)\n",
        "  try:\n",
        "    img = cv2.resize(img, (28, 28))\n",
        "  except:\n",
        "    print(path)\n",
        "  img = img.astype(np.float32) / 255\n",
        "  img = np.reshape(img, (3, 28 ,28))\n",
        "  # print(img.shape)\n",
        "  return img\n",
        "\n",
        "\n",
        "def read_pickle(fn):\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)\n",
        "\n",
        "\n",
        "# Train-val split that does not share products between training and validation sets.\n",
        "def generate_product_limited_samples(products, all_positive_pairs, random_state=None):\n",
        "    \"\"\"\n",
        "    Generates positive and negative examples for the given products using shared\n",
        "    occurence in rooms to indicate whether two products are compatible.\n",
        "\n",
        "    products: A sequence of product IDs; ALL positive and negative pairs must\n",
        "        contain only these product IDs.\n",
        "    all_positive_pairs: A set of product ID pairs that are positive.\n",
        "    \n",
        "    Returns: A tuple (x, y), where x is a sequence of product ID pairs and y is\n",
        "        the array of [0 or 1] labels indicating presence in all_positive_pairs.\n",
        "    \"\"\"\n",
        "    product_set = set(products)\n",
        "    within_positive_pairs = [p for p in sorted(all_positive_pairs) if p[0] in product_set and p[1] in product_set]\n",
        "    negative_pairs = random_negative_sampling(products, all_positive_pairs, count=len(within_positive_pairs), random_state=random_state)\n",
        "    x = within_positive_pairs + negative_pairs\n",
        "    y = np.array([1] * len(within_positive_pairs) + [0] * len(negative_pairs))\n",
        "    if random_state is not None: np.random.seed(random_state)\n",
        "    indices = np.random.permutation(np.arange(len(x)))\n",
        "    return [x[i] for i in indices], y[indices]\n",
        "\n",
        "\n",
        "def random_negative_sampling(products, all_positive_pairs, count=None, random_state=None):\n",
        "  selected_negative_pairs = []\n",
        "  if random_state is not None: random.seed(random_state)\n",
        "  while len(selected_negative_pairs) < (count or len(all_positive_pairs)):\n",
        "    random_pair = tuple(random.sample(products, 2))\n",
        "    if random_pair in all_positive_pairs:\n",
        "      continue\n",
        "    else:\n",
        "      selected_negative_pairs.append(random_pair)\n",
        "  return selected_negative_pairs\n",
        "\n",
        "\n",
        "def get_embedding_matrix(word_index, weights_path=\"/content/GoogleNews-vectors-negative300.bin\"):\n",
        "  word2vecDict = KeyedVectors.load_word2vec_format(weights_path, binary=True)\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "  for word in word2vecDict.wv.vocab:\n",
        "    embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "  print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
        "        \n",
        "  embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
        "\n",
        "  embeddedCount = 0\n",
        "  for word, i in word_index.items():\n",
        "    i-=1\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "  print(\"total embedded:\", embeddedCount, \"common words\")\n",
        "  del(embeddings_index)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5tNV3yP9i5m"
      },
      "source": [
        "# Build Train and Eval Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJPk6ZK5mSw-"
      },
      "source": [
        "#### Load raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-LQtBGE9i5n",
        "outputId": "dc2f7838-6088-4ec6-a329-3535913c3a49"
      },
      "source": [
        "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
        "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
        "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
        "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
        "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
        "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
        "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
        "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
        "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
        "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
        "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
        "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
        "\n",
        "# Some simple preprossing\n",
        "item_to_info = {key : value[\"type\"] + \" \" +\n",
        "                             value[\"desc\"]\n",
        "                       for key, value in item_property.items()} # remove view more info\n",
        "\n",
        "room_to_items = {}\n",
        "\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "  if not os.path.exists(IMAGES_DIR + item_id + \".jpg\"):\n",
        "      print(item_url + \" does not exist\")\n",
        "      continue\n",
        "\n",
        "  for room_url in room_url_list:\n",
        "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if room_id not in room_to_items:\n",
        "      room_to_items[room_id] = [item_id]\n",
        "    else:\n",
        "      room_to_items[room_id].append(item_id)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images/890.333.75.jpg does not exist\n",
            "images/991.333.98.jpg does not exist\n",
            "images/990.612.97.jpg does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDaGXCr4mV0I"
      },
      "source": [
        "#### Construct positive and negative pairs\n",
        "\n",
        "For IR-style problem, seen and unseen can be tricky. We need to discuss whether unseen means \"unseen pairs\" or \"unseen image or text\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0MZqyUWmN87",
        "outputId": "4b20a2d5-ed7c-4f56-c2b3-0eb034495029"
      },
      "source": [
        "all_positive_pairs = set()\n",
        "for room, item_id_list in room_to_items.items():\n",
        "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
        "  all_positive_pairs |= set(pairs_for_current_room)\n",
        "\n",
        "all_products = sorted(set([x for pair in all_positive_pairs for x in pair]))\n",
        "train_products, val_products = train_test_split(all_products, test_size=TRAIN_TEST_RATIO, random_state=72)\n",
        "\n",
        "train_pairs, y_train = generate_product_limited_samples(train_products, all_positive_pairs, random_state=72)\n",
        "val_pairs, y_val = generate_product_limited_samples(val_products, all_positive_pairs, random_state=72)\n",
        "print(len(train_pairs), len(val_pairs))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21666 5810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tB_dsGrE-h1"
      },
      "source": [
        "# # To read the training and validation sets\n",
        "# with open(ROOT_DIR + \"train_data.pkl\", \"rb\") as file:\n",
        "#     train_pairs, y_train = pickle.load(file)\n",
        "# with open(ROOT_DIR + \"val_data.pkl\", \"rb\") as file:\n",
        "#     val_pairs, y_val = pickle.load(file)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRaA0zbGmeyt"
      },
      "source": [
        "#### Build PyTorch dataloader for train/val image/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcOsiU0wFNnw"
      },
      "source": [
        "class FurnitureImagePairsDataset(Dataset):\n",
        "    \"\"\"Dataset containing pairs of furniture items.\"\"\"\n",
        "\n",
        "    def __init__(self, image_path, pairs, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_path (string): Path to the directory containing images.\n",
        "            pairs (list of tuples of strings): Pairs of image IDs to be used as training samples.\n",
        "            labels (array of integers): Labels for the training samples.\n",
        "        \"\"\"\n",
        "        super(FurnitureImagePairsDataset, self).__init__()\n",
        "        self.image_ids = list(set(x for pair in pairs for x in pair))\n",
        "        self.index_mapping = {image_id: i for i, image_id in enumerate(self.image_ids)}\n",
        "        self.images = [preprocess_img(image_path + image_id + \".jpg\") for image_id in tqdm.tqdm(self.image_ids)]\n",
        "        self.pairs = pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if isinstance(idx, (list, tuple)):\n",
        "            x1, x2, y = zip(*[self[i] for i in idx])\n",
        "            return torch.stack(x1), torch.stack(x2), torch.from_numpy(np.array(y))\n",
        "        pair = self.pairs[idx]\n",
        "        return self.images[self.index_mapping[pair[0]]], self.images[self.index_mapping[pair[1]]], self.labels[idx]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGMo8N_wFP0_",
        "outputId": "cfcc2052-fad8-4ceb-901e-aaf5b6696e96"
      },
      "source": [
        "X_train_image = FurnitureImagePairsDataset(IMAGES_DIR, train_pairs, y_train)\n",
        "X_val_image = FurnitureImagePairsDataset(IMAGES_DIR, val_pairs, y_val)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1456/1456 [00:07<00:00, 190.23it/s]\n",
            "100%|██████████| 718/718 [00:03<00:00, 189.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_UO2Szk9i5o",
        "outputId": "6061690f-30da-4a85-b894-c23627a306c9"
      },
      "source": [
        "train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
        "train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
        "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
        "WORD_INDEX = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
        "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
        "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
        "\n",
        "X_train_text_premise = tokenizer.texts_to_sequences(train_premise_texts)\n",
        "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_train_text_hypothesis = tokenizer.texts_to_sequences(train_hypothesis_texts)\n",
        "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2037 unique tokens.\n",
            "Max len: 1950\n",
            "Loaded 3000000 word vectors.\n",
            "total embedded: 1904 common words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAakls6kMkX"
      },
      "source": [
        "val_premise_texts = [item_to_info[id] for id, _ in val_pairs]\n",
        "val_hypothesis_texts = [item_to_info[id] for _, id in val_pairs]\n",
        "\n",
        "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
        "X_val_text_premise = tokenizer.texts_to_sequences(val_premise_texts)\n",
        "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_val_text_hypothesis = tokenizer.texts_to_sequences(val_hypothesis_texts)\n",
        "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvSVQ8C29i5p",
        "outputId": "3b652533-1991-49c2-a5d4-6b69794ed754"
      },
      "source": [
        "img_train_data = X_train_image\n",
        "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise), torch.from_numpy(X_train_text_hypothesis), torch.from_numpy(y_train))\n",
        "\n",
        "img_val_data = X_val_image\n",
        "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise), torch.from_numpy(X_val_text_hypothesis), torch.from_numpy(y_val))\n",
        "\n",
        "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
        "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
        "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_train_loader), len(img_train_loader))\n",
        "print(len(text_val_loader), len(img_val_loader))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1355 1355\n",
            "364 364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PunK-A-aEE6p",
        "outputId": "51f27e28-5c36-4176-bcd6-68ccf9c54844"
      },
      "source": [
        "for lstm in text_train_loader:\n",
        "  print(type(lstm))\n",
        "  lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "  print(type(lstm_inp1))\n",
        "  print(type(lstm_inp2))\n",
        "  print(type(lstm_labels))\n",
        "  print(lstm_inp1)\n",
        "  break"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[   0,    0,    0,  ...,  572,  860,  636],\n",
            "        [   0,    0,    0,  ..., 1347,    9,  372],\n",
            "        [   0,    0,    0,  ...,  145,   45,   62],\n",
            "        ...,\n",
            "        [   0,    0,    0,  ...,   14,    3,  179],\n",
            "        [   0,    0,    0,  ...,  309,    2, 1166],\n",
            "        [   0,    0,    0,  ...,    8,   10,   51]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lLhgGQ19i5p"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YFzao3vyEAW"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, img_channels=3, feature_dim=32*20*20, z_dim=256): # IKEA\n",
        "    super(VAE, self).__init__()\n",
        "    self.image_encoder_conv1 = nn.Conv2d(img_channels, 16, 5)\n",
        "    self.image_encoder_conv2 = nn.Conv2d(16, 32, 5)\n",
        "    self.image_encoder_fc1 = nn.Linear(feature_dim, z_dim // 2)\n",
        "    self.image_encoder_fc2 = nn.Linear(feature_dim, z_dim // 2)\n",
        "    self.text_encoder_fc1 = nn.Linear(MAX_SEQUENCE_LENGTH, 512)\n",
        "    self.text_encoder_fc2 = nn.Linear(512, 512)\n",
        "    self.text_encoder_fc3 = nn.Linear(512, z_dim // 2)\n",
        "    self.text_encoder_fc4 = nn.Linear(512, z_dim // 2)\n",
        "    self.decoder_fc = nn.Linear(z_dim, feature_dim)\n",
        "    self.decoder_conv1 = nn.ConvTranspose2d(32, 16, 5)\n",
        "    self.decoder_conv2 = nn.ConvTranspose2d(16, img_channels, 5)\n",
        "      \n",
        "  def image_encoder(self, x):\n",
        "    x = F.relu(self.image_encoder_conv1(x))\n",
        "    x = F.relu(self.image_encoder_conv2(x))\n",
        "    x = x.view(-1, 32*20*20)\n",
        "    mu = self.image_encoder_fc1(x)\n",
        "    logVar = self.image_encoder_fc2(x)\n",
        "    return mu, logVar\n",
        "  \n",
        "  def text_encoder(self, x):\n",
        "    x = F.relu(self.text_encoder_fc1(x))\n",
        "    x = F.relu(self.text_encoder_fc2(x))\n",
        "    mu = F.relu(self.text_encoder_fc3(x))\n",
        "    logVar = F.relu(self.text_encoder_fc4(x))\n",
        "    return mu, logVar\n",
        "\n",
        "  def reparameterize(self, mu, logVar):\n",
        "    std = torch.exp(logVar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + std * eps, std, eps\n",
        "  \n",
        "  def decoder(self, z):\n",
        "    x = F.relu(self.decoder_fc(z))\n",
        "    x = x.view(-1, 32, 20, 20)\n",
        "    x = F.relu(self.decoder_conv1(x))\n",
        "    x = torch.sigmoid(self.decoder_conv2(x))\n",
        "    return x\n",
        "  \n",
        "  def forward(self, image, text):\n",
        "    mu1, logVar1 = self.image_encoder(image)\n",
        "    mu2, logVar2 = self.text_encoder(text)\n",
        "    mu = torch.cat((mu1, mu2), dim = 1)\n",
        "    logVar = torch.cat((logVar1, logVar2), dim = 1)\n",
        "    z, std, eps = self.reparameterize(mu, logVar)\n",
        "    out = self.decoder(z)\n",
        "    return out, mu, std, eps, logVar"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7w-DGAxyiZ"
      },
      "source": [
        "class VAE_Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VAE_Classifier, self).__init__()\n",
        "    self.vae = VAE()\n",
        "    self.fc1 = nn.Linear(256 * 2, 512)\n",
        "    self.fc2 = nn.Linear(512, 128)\n",
        "    self.output_fc = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self, text_inp1, text_inp2, img_inp1, img_inp2):\n",
        "    vae = torch.load('/content/drive/Othercomputers/My MacBook Pro/GitHub/DecorAssistant/multimodal_vae.pth').to(DEVICE)\n",
        "    vae.eval()\n",
        "    out1, mu1, std1, eps1, logVar1 = vae(img_inp1, text_inp1)\n",
        "    out2, mu2, std2, eps2, logVar2 = vae(img_inp2, text_inp2)\n",
        "    bottleneck1 = mu1 + std1 * eps1\n",
        "    bottleneck2 = mu2 + std2 * eps2\n",
        "    combined_bottleneck = torch.cat((bottleneck1, bottleneck2), 1)\n",
        "    x_comb = F.relu(self.fc1(combined_bottleneck))\n",
        "    x_comb = F.relu(self.fc2(x_comb))\n",
        "    x = self.output_fc(x_comb)\n",
        "    return x"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2rumtEQYv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "6ed90f0b-5858-4b9a-99fb-3df714bea65d"
      },
      "source": [
        "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
        "\n",
        "model = VAE_Classifier()\n",
        "model.to(DEVICE)\n",
        "print(\"Model Architecture {}\\n\".format(model))\n",
        "\n",
        "# lr = LEARNING_RATE\n",
        "lr = 1e-4\n",
        "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "print(\"Training Started...\")\n",
        "model.train()\n",
        "best_loss = float('inf')\n",
        "for i in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "    \n",
        "  for lstm, cnn in tqdm.tqdm(zip(text_train_loader, img_train_loader), total=len(text_train_loader)):\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.float().to(DEVICE), lstm_inp2.float().to(DEVICE), lstm_labels.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    loss = criterion(torch.round(torch.sigmoid(output.squeeze())), lstm_labels.float())\n",
        "    loss.backward()\n",
        "    # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc\n",
        "      total_loss_train += loss.item()\n",
        "  \n",
        "  train_acc = total_acc_train/len(text_train_loader)\n",
        "  train_loss = total_loss_train/len(text_train_loader)\n",
        "  model.eval()\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  with torch.no_grad():\n",
        "    for lstm, cnn in tqdm.tqdm(zip(text_val_loader, img_val_loader), total=len(text_val_loader)):\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.float().to(DEVICE), lstm_inp2.float().to(DEVICE), lstm_labels.to(DEVICE)\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "      model.zero_grad()\n",
        "      output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "      if val_loss < best_loss:\n",
        "       best_loss = val_loss\n",
        "       torch.save(model, 'multimodal_vae_classifier.pth')\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_val += acc\n",
        "      total_loss_val += val_loss.item()\n",
        "  val_acc = total_acc_val/len(text_val_loader)\n",
        "  val_loss = total_loss_val/len(text_val_loader)\n",
        "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "  model.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device: cuda\n",
            "\n",
            "Model Architecture VAE_Classifier(\n",
            "  (vae): VAE(\n",
            "    (image_encoder_conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (image_encoder_conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (image_encoder_fc1): Linear(in_features=12800, out_features=128, bias=True)\n",
            "    (image_encoder_fc2): Linear(in_features=12800, out_features=128, bias=True)\n",
            "    (text_encoder_fc1): Linear(in_features=1950, out_features=512, bias=True)\n",
            "    (text_encoder_fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (text_encoder_fc3): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (text_encoder_fc4): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (decoder_fc): Linear(in_features=256, out_features=12800, bias=True)\n",
            "    (decoder_conv1): ConvTranspose2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (decoder_conv2): ConvTranspose2d(16, 3, kernel_size=(5, 5), stride=(1, 1))\n",
            "  )\n",
            "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (output_fc): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Training Started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 78/1355 [00:04<01:17, 16.43it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-222-7104efa228f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcnn_inp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_inp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_inp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_inp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_inp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_inp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_inp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_inp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-a7001f179ef9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_inp1, text_inp2, img_inp1, img_inp2)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_inp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_inp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_inp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_inp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My MacBook Pro/GitHub/DecorAssistant/multimodal_vae.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogVar1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_inp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_inp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRkG-JWjfCj"
      },
      "source": [
        "# Ranker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfYZScTJS4Kd"
      },
      "source": [
        "model = torch.load('/content/drive/Othercomputers/My MacBook Pro/GitHub/DecorAssistant/multimodal_vae_classifier.pth').to(DEVICE)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOlqTLXknYN"
      },
      "source": [
        "def single_pair_inference(premise_image_path, hypothesis_image_path, premise_text, hypothesis_text, model, tokenizer, threshold, do_plot=False):\n",
        "  premise_sequence = tokenizer.texts_to_sequences([premise_text])\n",
        "  premise_sequence = pad_sequences(premise_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  hypothesis_sequence = tokenizer.texts_to_sequences([hypothesis_text])\n",
        "  hypothesis_sequence = pad_sequences(hypothesis_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  image_premise, image_hypothesis = preprocess_img(premise_image_path), preprocess_img(hypothesis_image_path)\n",
        "\n",
        "  if do_plot:\n",
        "    image_1 = Image.open(premise_image_path)\n",
        "    image_2 = Image.open(hypothesis_image_path)\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    ax1 = fig.add_subplot(2,2,1)\n",
        "    ax1.imshow(image_1)\n",
        "    ax2 = fig.add_subplot(2,2,2)\n",
        "    ax2.imshow(image_2)\n",
        "    print(\"Left item description ------ {}\".format(premise_text))\n",
        "    print(\"Right item description ------  {}\".format(hypothesis_text))\n",
        "\n",
        "\n",
        "  image_premise = np.reshape(image_premise, (1, 3, 28, 28))\n",
        "  image_hypothesis = np.reshape(image_hypothesis, (1, 3, 28, 28))\n",
        "\n",
        "  img_data = TensorDataset(torch.from_numpy(image_premise), torch.from_numpy(image_hypothesis))\n",
        "  text_data = TensorDataset(torch.from_numpy(premise_sequence), torch.from_numpy(hypothesis_sequence))\n",
        "  \n",
        "  text_loader = DataLoader(text_data, batch_size=1)\n",
        "  img_loader = DataLoader(img_data, batch_size=1)\n",
        "\n",
        "  for lstm, cnn in zip(text_loader, img_loader):\n",
        "    lstm_inp1, lstm_inp2 = lstm\n",
        "    cnn_inp1, cnn_inp2 = cnn\n",
        "    lstm_inp1, lstm_inp2 = lstm_inp1.float().to(DEVICE), lstm_inp2.float().to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2 = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    print(output)\n",
        "\n",
        "  score = output.squeeze().cpu().detach().numpy().tolist()\n",
        "  if score > threshold:\n",
        "    return \"Positive\", score\n",
        "  else:\n",
        "    return \"Negative\", 1 - score\n"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft7vTrP2EEGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c7f438-b490-471d-a680-71de3498e4b3"
      },
      "source": [
        "for i in range(10):\n",
        "  random_index = random.randint(0, len(train_pairs))\n",
        "  image_id_1 = train_pairs[random_index][0]\n",
        "  image_id_2 = train_pairs[random_index][1]\n",
        "  text_1 = train_premise_texts[random_index]\n",
        "  text_2 = train_hypothesis_texts[random_index]\n",
        "  prediction, confidence = single_pair_inference(premise_image_path=IMAGES_DIR + image_id_1 + \".jpg\",\n",
        "            hypothesis_image_path=IMAGES_DIR + image_id_2 + \".jpg\",\n",
        "            premise_text=text_1,\n",
        "            hypothesis_text=text_2,\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            threshold=0.4,\n",
        "            do_plot=False)\n",
        "  print(\"Actual Label for this pair is\", \"Positive\" if y_train[random_index] == 1 else \"Negative\")\n",
        "  print(\"The prediction for this pair is\", prediction, \"with confidence\", confidence)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Negative with confidence 0.9990631838445552\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Negative with confidence 0.9990632036351599\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Positive\n",
            "The prediction for this pair is Negative with confidence 0.999063209281303\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Positive\n",
            "The prediction for this pair is Negative with confidence 0.9990631905966438\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Negative with confidence 0.9990631949622184\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Negative with confidence 0.9990631781402044\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Negative with confidence 0.999063198803924\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Positive\n",
            "The prediction for this pair is Negative with confidence 0.9990631814580411\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Positive\n",
            "The prediction for this pair is Negative with confidence 0.9990631903638132\n",
            "tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Negative with confidence 0.9990631856489927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0qmvz0xpmci"
      },
      "source": [
        "def ranker(input_image_id, input_description, item_id_pool, model, tokenizer, threshold=0.3, top_n=5, do_plot=False):\n",
        "  input_image_path = IMAGES_DIR + input_image_id + \".jpg\"\n",
        "  item_id_to_score = {}\n",
        "\n",
        "  for item_id in item_id_pool:\n",
        "    candidate_image_path = IMAGES_DIR + item_id + \".jpg\"\n",
        "    candidate_description = item_to_info[item_id]\n",
        "    output_prediction, output_confidence = single_pair_inference(premise_image_path=input_image_path,\n",
        "          hypothesis_image_path=candidate_image_path,\n",
        "          premise_text=input_description,\n",
        "          hypothesis_text=candidate_description,\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          threshold=threshold,\n",
        "          do_plot=False)\n",
        "    if output_prediction == \"Positive\":\n",
        "      item_id_to_score[item_id] = output_confidence\n",
        "    else:\n",
        "      continue\n",
        "  return item_id_to_score"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnuz8ilHN0Fv"
      },
      "source": [
        "random_index = random.randint(0, len(train_pairs))\n",
        "image_id = train_pairs[random_index][0]\n",
        "input_description = item_to_info[image_id]\n",
        "all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "\n",
        "ranker(image_id, input_description, all_item_ids, model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eda32c3-3691-42c2-b11c-ace2bbc0ccc4"
      },
      "source": [
        "# Ranking Evaluation\n",
        "\n",
        "Build a dataset where a random sample of \"premise\" products is compared against all \"hypothesis\" products in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29981c99-e216-4f07-a0ea-cbf962ed9315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0843ddcc-5df4-4ee2-c815-23e29db9125b"
      },
      "source": [
        "# Build pairs to rank\n",
        "\n",
        "NUM_QUERIES = 50\n",
        "\n",
        "val_products = sorted(list(set(x for pair in val_pairs for x in pair)))\n",
        "\n",
        "np.random.seed(1234)\n",
        "premise_products = np.random.choice(val_products, size=min(NUM_QUERIES, len(val_products)), replace=False)\n",
        "hypothesis_products = val_products\n",
        "\n",
        "ranking_pairs = list(itertools.product(premise_products, hypothesis_products))\n",
        "print(len(ranking_pairs), \"pairs,\", len(hypothesis_products), \"products per query\")"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35900 pairs, 718 products per query\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43a41485-1311-4cca-8315-7aa40fbd3eaf"
      },
      "source": [
        "# Get the ground-truth\n",
        "\n",
        "ground_truth_map = {}\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "    item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if item_id not in premise_products: continue\n",
        "\n",
        "    for room_url in room_url_list:\n",
        "        room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "        ground_truth_map[item_id] = ground_truth_map.get(item_id, set()) | set(room_to_items[room_id])\n",
        "ground_truth_lists = [ground_truth_map[item_id] for item_id in premise_products]\n",
        "\n",
        "# plt.hist([len(x) for x in ground_truth_lists], bins=np.arange(0, max(len(x) for x in ground_truth_lists), 5));"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c514d836-2840-456b-a8c6-3f045e188d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d7db85ef-592e-46dd-ff1f-36417dbebc36"
      },
      "source": [
        "X_rank_image = FurnitureImagePairsDataset(IMAGES_DIR, ranking_pairs, np.zeros(len(ranking_pairs)))\n",
        "X_rank_text_premise = ranking_pairs\n",
        "X_rank_text_hypothesis = ranking_pairs\n",
        "X_rank_text_premise.size()\n",
        "\n",
        "def tokenize(text):\n",
        "  try:\n",
        "      return clip.tokenize(text)\n",
        "  except:\n",
        "      return clip.tokenize(' '.join(text.split()[:50]))\n",
        "\n",
        "# X_rank_text_premise = torch.cat([tokenize(item_to_info[id]) for id, _ in ranking_pairs], 0)\n",
        "# X_rank_text_hypothesis = torch.cat([tokenize(item_to_info[id]) for _, id in ranking_pairs], 0)\n",
        "# X_rank_text_premise.size()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 718/718 [00:03<00:00, 189.50it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-233-13d73428036a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_rank_text_premise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranking_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_rank_text_hypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranking_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_rank_text_premise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67aaf16f-a89c-4a82-9cef-b568ab08c1bb"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "img_ranking_data = X_rank_image # TensorDataset(torch.from_numpy(X_val_image_premise), torch.from_numpy(X_val_image_hypothesis), torch.from_numpy(y_val))\n",
        "text_ranking_data = TensorDataset(X_rank_text_premise, X_rank_text_hypothesis, torch.zeros(len(ranking_pairs)))\n",
        "\n",
        "text_ranking_loader = DataLoader(text_ranking_data, batch_size=BATCH_SIZE)\n",
        "img_ranking_loader = DataLoader(img_ranking_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_ranking_loader), len(img_ranking_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3278310c-3fde-4aee-a8d7-26d677f83cf1"
      },
      "source": [
        "checkpoint_path = CHECKPOINT_DIR + \"aws_image_embedding_one_layer_dual_lr_epoch_2.p\"\n",
        "state = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "full_model.load_state_dict(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58f659d1-b8df-4175-8cbc-cc2428ea35fd"
      },
      "source": [
        "model.eval()\n",
        "ranking_results = []\n",
        "with torch.no_grad():\n",
        "    for lstm, cnn in tqdm.tqdm(zip(text_ranking_loader, img_ranking_loader), total=len(text_ranking_loader)):\n",
        "        lstm_inp1, lstm_inp2, _ = lstm\n",
        "        cnn_inp1, cnn_inp2, _ = cnn\n",
        "        lstm_inp1, lstm_inp2 = lstm_inp1.to(device), lstm_inp2.to(device)\n",
        "        cnn_inp1, cnn_inp2 = cnn_inp1.to(device), cnn_inp2.to(device)\n",
        "        model.zero_grad()\n",
        "        output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "        ranking_results.append(output.cpu().numpy())\n",
        "ranking_results = np.concatenate(ranking_results).reshape(len(premise_products), len(hypothesis_products))\n",
        "print(ranking_results.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "130811f3-c839-4bad-b715-b92627ad8287"
      },
      "source": [
        "import math\n",
        "class Evaluator:\n",
        "    def __init__(self, GroundTruth):\n",
        "      self.GroundTruth = GroundTruth\n",
        "\n",
        "    def NDCG_Eval(self, rankresult, topk):\n",
        "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
        "      DCGScore = 0\n",
        "      result = []\n",
        "      for i, item in enumerate(sortedRankResult[:topk]):\n",
        "        if item[0] in self.GroundTruth:\n",
        "          result.append((item, i))\n",
        "      DCGScore = sum([item[0][1]/math.log(item[1]+2, 2) for item in result])\n",
        "      IDCGScore = sum([1/math.log(i+2,2) for i in range(topk)])\n",
        "      NDCG = DCGScore / IDCGScore\n",
        "\n",
        "      return NDCG\n",
        "    \n",
        "    def Score_Eval(self, rankresult, topk):\n",
        "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
        "      return sum(i[1] for i in sortedRankResult[:topk] if i[0] in self.GroundTruth) / topk\n",
        "    \n",
        "    def Precision(self, rankresult, topk):\n",
        "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
        "      topkresult = sortedRankResult[:topk]\n",
        "      return len([i for i in sortedRankResult[:topk] if i[0] in self.GroundTruth]) / len(topkresult)\n",
        "\n",
        "    def Recall(self, rankresult, topk):\n",
        "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
        "      topkresult = sortedRankResult[:topk]\n",
        "      return len([i for i in sortedRankResult[:topk] if i[0] in self.GroundTruth]) / len(self.GroundTruth)\n",
        "    \n",
        "    def FValue(self, rankresult, topk):\n",
        "      precision = self.Precision(rankresult, topk)\n",
        "      recall = self.Recall(rankresult, topk)\n",
        "      return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "752e0478-0a4b-4cde-8c8c-2785152f1786"
      },
      "source": [
        "# Now use the evaluator\n",
        "TOP_K = 10\n",
        "\n",
        "ndcg = np.zeros(len(premise_products))\n",
        "score = np.zeros(len(premise_products))\n",
        "precision = np.zeros(len(premise_products))\n",
        "recall = np.zeros(len(premise_products))\n",
        "fvalue = np.zeros(len(premise_products))\n",
        "for i, (ground_truth, rankings) in enumerate(zip(ground_truth_lists, ranking_results)):\n",
        "    evaluator = Evaluator(ground_truth)\n",
        "    rankings = {product: output for product, output in zip(hypothesis_products, rankings)}\n",
        "    ndcg[i] = evaluator.NDCG_Eval(rankings, TOP_K)\n",
        "    score[i] = evaluator.Score_Eval(rankings, TOP_K)\n",
        "    precision[i] = evaluator.Precision(rankings, TOP_K)\n",
        "    recall[i] = evaluator.Recall(rankings, TOP_K)\n",
        "    fvalue[i] = evaluator.FValue(rankings, TOP_K)\n",
        "print(\"NDCG: {:.4f} (95% CI {:.3f}-{:.3f})\".format(ndcg.mean(), ndcg.mean() - 1.96 * ndcg.std(), ndcg.mean() + 1.96 * ndcg.std()))\n",
        "print(\"Score: {:.4f} (95% CI {:.3f}-{:.3f})\".format(score.mean(), score.mean() - 1.96 * score.std(), score.mean() + 1.96 * score.std()))\n",
        "print(\"Precision: {:.4f} (95% CI {:.3f}-{:.3f})\".format(precision.mean(), precision.mean() - 1.96 * precision.std(), precision.mean() + 1.96 * precision.std()))\n",
        "print(\"Recall: {:.4f} (95% CI {:.3f}-{:.3f})\".format(recall.mean(), recall.mean() - 1.96 * recall.std(), recall.mean() + 1.96 * recall.std()))\n",
        "print(\"FValue: {:.4f} (95% CI {:.3f}-{:.3f})\".format(fvalue.mean(), fvalue.mean() - 1.96 * fvalue.std(), fvalue.mean() + 1.96 * fvalue.std()))\n",
        "\n",
        "with open(os.path.splitext(checkpoint_path)[0] + \"_rankings_{}_queries.pkl\".format(len(premise_products)), \"wb\") as file:\n",
        "    pickle.dump({\n",
        "        \"ndcg\": ndcg,\n",
        "        \"score\": score,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"fvalue\": fvalue\n",
        "    }, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnZxCsoFIiyM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}