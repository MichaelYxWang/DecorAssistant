{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0b710-7998-4421-9d48-421bc92fce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import sys; sys.path.insert(0, \"intermediate_fusion\")\n",
    "from text_preprocessing import Tokenizer, pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b46207-01cb-4d95-9aa4-78af4e586e60",
   "metadata": {
    "id": "7DW6oxGPmC9-"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"dataset/\"\n",
    "DATASET_DIR = BASE_DIR + \"text_data/\"\n",
    "IMAGES_DIR = BASE_DIR + \"images/all_items/\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76e25e-45f4-441f-836b-b3e1cd89ba64",
   "metadata": {
    "id": "qmCx_kVEmF-s"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(path):\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.resize(img, (256, 256))\n",
    "  img = img.astype(np.float32) / 255\n",
    "  return np.moveaxis(img, 2, 0)\n",
    "\n",
    "def read_pickle(fn):\n",
    "\twith open(fn, \"rb\") as f:\n",
    "\t\treturn pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bf5f6-2c06-4c55-995e-06719aad22a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abwrNXYP8ibS",
    "outputId": "87f87e2d-b55a-490f-c05e-0a9d376f90f6"
   },
   "outputs": [],
   "source": [
    "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
    "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
    "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
    "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
    "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
    "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
    "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
    "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
    "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
    "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
    "item_to_rooms_map = {item_url.split(\"/\")[-1].split(\".jpg\")[0] : val for item_url, val in item_to_rooms_map.items()}\n",
    "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
    "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
    "\n",
    "room_to_items = {}\n",
    "\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "  if not os.path.exists(IMAGES_DIR + item_id + \".jpg\"):\n",
    "      print(item_url + \" does not exist\")\n",
    "      continue\n",
    "\n",
    "  for room_url in room_url_list:\n",
    "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if room_id not in room_to_items:\n",
    "      room_to_items[room_id] = [item_id]\n",
    "    else:\n",
    "      room_to_items[room_id].append(item_id)\n",
    "    \n",
    "with open(BASE_DIR + \"train_sets_reweighted.pkl\", \"rb\") as file:\n",
    "    train_sets = pickle.load(file)\n",
    "with open(BASE_DIR + \"val_data_reweighted.pkl\", \"rb\") as file:\n",
    "    val_pairs, y_val = pickle.load(file)\n",
    "with open(BASE_DIR + \"preprocessed_text.pkl\", \"rb\") as file:\n",
    "    item_to_info = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f8487-4770-4473-8843-e2c90c12511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/val_data_reweighted.pkl\", \"rb\") as file:\n",
    "    val_pairs, y_val = pickle.load(file)\n",
    "    \n",
    "val_products = set([x for pair in val_pairs for x in pair])\n",
    "# val_id_indexes = [i for i, id in enumerate(image_ids) if id in val_products]\n",
    "val_image_ids = sorted(val_products)\n",
    "# val_image_ids = [image_ids[i] for i in val_id_indexes]\n",
    "# val_embeddings = [emb[val_id_indexes] for emb in embs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb42be-8196-40aa-b41d-6b505c8419b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53d99a-28ba-446c-be38-5e2bf1477645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Evaluator:\n",
    "    def __init__(self, GroundTruth):\n",
    "      self.GroundTruth = GroundTruth\n",
    "\n",
    "    def NDCG_Eval(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      DCGScore = 0\n",
    "      result = []\n",
    "      for i, item in enumerate(sortedRankResult[:topk]):\n",
    "        if item[0] in self.GroundTruth:\n",
    "          result.append((item, i))\n",
    "      DCGScore = sum([item[0][1]/math.log(item[1]+2, 2) for item in result])\n",
    "      IDCGScore = sum([1/math.log(i+2,2) for i in range(topk)])\n",
    "      NDCG = DCGScore / IDCGScore\n",
    "\n",
    "      return NDCG\n",
    "    \n",
    "    def Score_Eval(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      return sum(i[1] for i in sortedRankResult[:topk] if i[0] in self.GroundTruth) / topk\n",
    "    \n",
    "    def Precision(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      topkresult = sortedRankResult[:topk]\n",
    "      return len([i for i in sortedRankResult[:topk] if i[0] in self.GroundTruth]) / len(topkresult)\n",
    "\n",
    "    def Recall(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      topkresult = sortedRankResult[:topk]\n",
    "      return len([i for i in sortedRankResult[:topk] if i[0] in self.GroundTruth]) / len(self.GroundTruth)\n",
    "    \n",
    "    def FValue(self, rankresult, topk):\n",
    "      precision = self.Precision(rankresult, topk)\n",
    "      recall = self.Recall(rankresult, topk)\n",
    "      return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035352b-e9dd-4926-9453-8b4071d9f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(arr):\n",
    "    sem = arr.std() / np.sqrt(len(arr))\n",
    "    return arr.mean() - 1.96 * sem, arr.mean() + 1.96 * sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d68bb7-0fd7-4cbd-80be-315b557949b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_pairs = []\n",
    "triplet_ground_truths = []\n",
    "for pair in (val_pairs[i] for i in np.argwhere(y_val).flatten()):\n",
    "    triplet_pairs.append(pair)\n",
    "    gt = set()\n",
    "    for room_url in item_to_rooms_map[pair[0]]:\n",
    "        room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        if pair[1] in room_to_items[room_id]:\n",
    "            gt |= set(room_to_items[room_id])\n",
    "    triplet_ground_truths.append(gt)\n",
    "\n",
    "plt.hist([len(x) for x in triplet_ground_truths])\n",
    "plt.title(len(triplet_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9dce7-4c90-4ee7-b0f9-040008b2fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_categories = [set(simple_preprocess(item_categories[o])) for o in val_image_ids]\n",
    "def eligible_product_indexes(product_idx, cutoff=0.5):\n",
    "    cat = preprocessed_categories[product_idx]\n",
    "    cat_sims = [len(x & cat) / max(1, len(x | cat)) for x in preprocessed_categories]\n",
    "    return [i for i, sim in enumerate(cat_sims) if sim > cutoff]\n",
    "\n",
    "query_products = {val_image_ids[i]: [val_image_ids[j] for j in eligible_product_indexes(i)]\n",
    "                  for i in range(len(val_image_ids)) if len(eligible_product_indexes(i)) > 10}\n",
    "print(len(query_products))\n",
    "query, candidates = list(query_products.items())[np.random.randint(0, len(query_products))]\n",
    "print(item_categories[query])\n",
    "print(', '.join([item_categories[cand] for cand in candidates]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b8929-3c84-432c-9edd-57f33b60107b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Data Option 1: Embeddings\n",
    "\n",
    "This should be used if the model uses a simple similarity metric between embeddings to predict compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715da82-1149-469e-a306-d22ea80048e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_image_ids, embs = zip(*pickle.load(open(\"vae/multimodal_vae_embeddings.pickle\", \"rb\")))\n",
    "val_embeddings = np.stack([embs[emb_image_ids.index(id)] for id in val_image_ids], axis=0)\n",
    "results = 1 / (1 + np.exp(-np.dot(val_embeddings, val_embeddings.T)))\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30282db4-97d8-494f-9a3a-21d9e2a0cb0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loading Data Option 2: Binary Classifier\n",
    "\n",
    "This should be used if the model has a binary classifier that takes two embeddings and predicts compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77502b26-2016-4ba1-8b8d-28deece6002a",
   "metadata": {},
   "source": [
    "## Generating classifications\n",
    "\n",
    "Use these cells if you have a set of embeddings and a classifier model you want to run on all pairs of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ab7aa-dca4-4c10-8412-d95c41413c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OutputClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, first_embs, second_embs):\n",
    "        output = torch.cat([first_embs, second_embs], 1) # (emb_1 * emb_2).sum(1)\n",
    "        output = self.fc1(output)\n",
    "        output = self.dropout1(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693bc90-ceb6-46cd-86a6-cfb630a7de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = []\n",
    "emb_product_ids = None\n",
    "emb_paths = [\"intermediate_fusion_reweighted/embeddings_0.p\",\n",
    "            \"intermediate_fusion_reweighted/embeddings_1.p\",\n",
    "            \"intermediate_fusion_reweighted/embeddings_2.p\"]\n",
    "checkpoint_paths = [\"intermediate_fusion_reweighted/base_0_checkpoint_classifier.p\",\n",
    "                   \"intermediate_fusion_reweighted/base_1_checkpoint_classifier.p\",\n",
    "                   \"intermediate_fusion_reweighted/base_2_checkpoint_classifier.p\"]\n",
    "# # emb_paths = [\"intermediate_fusion_reweighted/text_only_embeddings.p\"]\n",
    "# # checkpoint_paths = [\"intermediate_fusion_reweighted/text_only_checkpoint_classifier.p\"]\n",
    "\n",
    "for path in emb_paths:\n",
    "    with open(path, \"rb\") as file:\n",
    "        new_product_ids, embs = pickle.load(file)\n",
    "    if emb_product_ids is not None: assert new_product_ids == emb_product_ids\n",
    "    else: emb_product_ids = new_product_ids\n",
    "    all_embs.append(embs)\n",
    "\n",
    "# For now just take the first one\n",
    "embs = all_embs[0]\n",
    "path = checkpoint_paths[0]\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c8315-19c3-49b0-a976-8cf95da08748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs = ... # matrix of embeddings for all products\n",
    "model = OutputClassifier(256, 1) # a classification model that takes two arguments: a batch of \"first\" embeddings and a batch of \"second\" embeddings\n",
    "state = torch.load(path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state)\n",
    "\n",
    "emb_product_idx_mapping = {id: i for i, id in enumerate(emb_product_ids)}\n",
    "\n",
    "combs = list(itertools.product(range(len(val_image_ids)), range(len(val_image_ids))))\n",
    "print(len(combs), \"pairs for\", len(val_image_ids), \"products\")\n",
    "\n",
    "def make_batches(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "        \n",
    "results_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(make_batches(combs, 32), total=len(combs) // 32 + 1):\n",
    "        first_embs = torch.from_numpy(np.stack([embs[emb_product_idx_mapping[val_image_ids[x[0]]]] for x in batch]))\n",
    "        second_embs = torch.from_numpy(np.stack([embs[emb_product_idx_mapping[val_image_ids[x[1]]]] for x in batch]))\n",
    "        results_list.append(model(first_embs, second_embs).cpu().numpy())\n",
    "results = np.concatenate(results_list).reshape(len(val_image_ids), len(val_image_ids))\n",
    "results = 1 / (1 + np.exp(-results)) # optional sigmoid transformation (if not applied by model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71219809-dd24-44c1-8bfe-a9da1fa49f08",
   "metadata": {},
   "source": [
    "## Reading classifications from file \n",
    "\n",
    "Use this if you *already* have the `results` variable stored in a file, and you just want to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6dc19-a2c3-4352-a555-c79ceb553419",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.load(\"naive_clip/linear_layer_unreweighted_CLIP_predictions.npy\")\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5580d-9b1e-4b82-9f4b-6d5c18801395",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7909c73-84d6-4740-ae3b-bdf7a1a6ecd5",
   "metadata": {},
   "source": [
    "## Single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe39d7-1f63-4329-8538-6d81e90f545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metrics\n",
    "ground_truth_map = {}\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "    item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if item_id not in val_products: continue\n",
    "\n",
    "    for room_url in room_url_list:\n",
    "        room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        ground_truth_map[item_id] = ground_truth_map.get(item_id, set()) | set(room_to_items[room_id])\n",
    "ground_truth_lists = [ground_truth_map[item_id] - set([item_id]) for item_id in val_image_ids]\n",
    "\n",
    "ndcg = np.zeros(len(val_products))\n",
    "score = np.zeros(len(val_products))\n",
    "precision = np.zeros(len(val_products))\n",
    "recall = np.zeros(len(val_products))\n",
    "fvalue = np.zeros(len(val_products))\n",
    "for i, (ground_truth, sim_row) in enumerate(zip(ground_truth_lists, results)):\n",
    "    evaluator = Evaluator(ground_truth)\n",
    "    rankings = {product: output for product, output in zip(val_image_ids, sim_row) if product != val_image_ids[i]}\n",
    "    ndcg[i] = evaluator.NDCG_Eval(rankings, TOP_K)\n",
    "    score[i] = evaluator.Score_Eval(rankings, TOP_K)\n",
    "    precision[i] = evaluator.Precision(rankings, TOP_K)\n",
    "    recall[i] = evaluator.Recall(rankings, TOP_K)\n",
    "    fvalue[i] = evaluator.FValue(rankings, TOP_K)\n",
    "print(\"Single query results:\")\n",
    "print(\"NDCG: {:.4f} (95% CI {:.3f}-{:.3f})\".format(ndcg.mean(), *confidence_interval(ndcg)))\n",
    "print(\"Score: {:.4f} (95% CI {:.3f}-{:.3f})\".format(score.mean(), *confidence_interval(score)))\n",
    "print(\"Precision: {:.4f} (95% CI {:.3f}-{:.3f})\".format(precision.mean(), *confidence_interval(precision)))\n",
    "print(\"Recall: {:.4f} (95% CI {:.3f}-{:.3f})\".format(recall.mean(), *confidence_interval(recall)))\n",
    "print(\"FValue: {:.4f} (95% CI {:.3f}-{:.3f})\".format(fvalue.mean(), *confidence_interval(fvalue)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919293fc-7ed1-4dfd-8e8f-aab6d15a9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_map = {}\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "    item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if item_id not in val_products: continue\n",
    "\n",
    "    for room_url in room_url_list:\n",
    "        room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        ground_truth_map[item_id] = ground_truth_map.get(item_id, set()) | set(room_to_items[room_id])\n",
    "ground_truth_lists = [ground_truth_map[item_id] - set([item_id]) for item_id in val_image_ids]\n",
    "\n",
    "scores = []\n",
    "ground_truth_objects = []\n",
    "score_query_idxs = []\n",
    "print(results.shape, len(ground_truth_lists))\n",
    "for i, (ground_truth, sim_row) in enumerate(zip(ground_truth_lists, results)):\n",
    "    for gt_item in ground_truth:\n",
    "        if gt_item not in query_products: continue\n",
    "        ground_truth_objects.append(gt_item)\n",
    "        score_query_idxs.append(i)\n",
    "        rankings = {product: output for product, output in zip(val_image_ids, sim_row)\n",
    "                    if product != val_image_ids[i] and product in query_products[gt_item]}\n",
    "        top_items = set(sorted(rankings, key=rankings.get, reverse=True)[:TOP_K])\n",
    "        scores.append(gt_item in top_items)\n",
    "print(len(scores), np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec3be1-adfb-4304-a465-c616e4c4b53d",
   "metadata": {},
   "source": [
    "## Double query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7ba62-2839-4363-ae61-79f153d11359",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_index = {id: i for i, id in enumerate(val_image_ids)}\n",
    "\n",
    "ndcg = np.zeros(len(triplet_pairs))\n",
    "score = np.zeros(len(triplet_pairs))\n",
    "precision = np.zeros(len(triplet_pairs))\n",
    "recall = np.zeros(len(triplet_pairs))\n",
    "fvalue = np.zeros(len(triplet_pairs))\n",
    "\n",
    "for i, (pair, ground_truth) in enumerate(zip(triplet_pairs, triplet_ground_truths)):\n",
    "    sim_row = np.mean(np.vstack([results[val_image_index[pair[0]]],\n",
    "                                 results[val_image_index[pair[1]]]]), axis=0)\n",
    "\n",
    "    evaluator = Evaluator(ground_truth)\n",
    "    rankings = {product: output for product, output in zip(val_image_ids, sim_row) if product not in pair}\n",
    "    ndcg[i] = evaluator.NDCG_Eval(rankings, TOP_K)\n",
    "    score[i] = evaluator.Score_Eval(rankings, TOP_K)\n",
    "    precision[i] = evaluator.Precision(rankings, TOP_K)\n",
    "    recall[i] = evaluator.Recall(rankings, TOP_K)\n",
    "    fvalue[i] = evaluator.FValue(rankings, TOP_K)\n",
    "\n",
    "print(\"Double query results:\")\n",
    "print(\"NDCG: {:.4f} (95% CI {:.3f}-{:.3f})\".format(ndcg.mean(), ndcg.mean() - 1.96 * ndcg.std(), ndcg.mean() + 1.96 * ndcg.std()))\n",
    "print(\"Score: {:.4f} (95% CI {:.3f}-{:.3f})\".format(score.mean(), score.mean() - 1.96 * score.std(), score.mean() + 1.96 * score.std()))\n",
    "print(\"Precision: {:.4f} (95% CI {:.3f}-{:.3f})\".format(precision.mean(), precision.mean() - 1.96 * precision.std(), precision.mean() + 1.96 * precision.std()))\n",
    "print(\"Recall: {:.4f} (95% CI {:.3f}-{:.3f})\".format(recall.mean(), recall.mean() - 1.96 * recall.std(), recall.mean() + 1.96 * recall.std()))\n",
    "print(\"FValue: {:.4f} (95% CI {:.3f}-{:.3f})\".format(fvalue.mean(), fvalue.mean() - 1.96 * fvalue.std(), fvalue.mean() + 1.96 * fvalue.std()))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d28b18-754d-4f5e-86b7-1cf929560464",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_map = {}\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "    item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if item_id not in val_products: continue\n",
    "\n",
    "    for room_url in room_url_list:\n",
    "        room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        ground_truth_map[item_id] = ground_truth_map.get(item_id, set()) | set(room_to_items[room_id])\n",
    "ground_truth_lists = [ground_truth_map[item_id] - set([item_id]) for item_id in val_image_ids]\n",
    "\n",
    "scores = []\n",
    "ground_truth_objects = []\n",
    "score_query_idxs = []\n",
    "print(results.shape, len(ground_truth_lists))\n",
    "for i, (pair, ground_truth) in enumerate(zip(triplet_pairs, triplet_ground_truths)):\n",
    "    sim_row = np.amin(np.vstack([results[val_image_index[pair[0]]],\n",
    "                                 results[val_image_index[pair[1]]]]), axis=0)\n",
    "\n",
    "    for gt_item in ground_truth:\n",
    "        if gt_item in pair or gt_item not in query_products: continue\n",
    "        ground_truth_objects.append(gt_item)\n",
    "        score_query_idxs.append(i)\n",
    "        rankings = {product: output for product, output in zip(val_image_ids, sim_row)\n",
    "                    if product not in pair and product in query_products[gt_item]}\n",
    "        top_items = set(sorted(rankings, key=rankings.get, reverse=True)[:TOP_K])\n",
    "        scores.append(gt_item in top_items)\n",
    "print(len(scores), np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed88a4-3d9c-451b-b261-9703e7771b57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Accuracy and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83480ed-ba56-4f49-a879-da1e2a140872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([results[val_image_ids.index(pair[0]), val_image_ids.index(pair[1])] for pair in val_pairs])\n",
    "print(\"Validation accuracy: {:.3f}\".format(np.mean(np.round(pred) == y_val)))\n",
    "print(\"AUC: {:.3f}\".format(roc_auc_score(y_val, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41557c91-9eb6-4d37-b9a9-8a424b05ec5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
