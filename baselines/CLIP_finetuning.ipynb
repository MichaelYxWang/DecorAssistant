{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"CLIP_finetuning.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2DkS5ep8xfs","executionInfo":{"status":"ok","timestamp":1635087369457,"user_tz":240,"elapsed":24011,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"85e1f6c1-7e04-44aa-def0-1e520e4a2982"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"D2DkS5ep8xfs","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"94a03101-8cdb-4706-bdc4-92ead9aaa9a0","executionInfo":{"status":"ok","timestamp":1635087380909,"user_tz":240,"elapsed":11458,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"af06caf5-ada4-4e3c-bfd7-317705d58b07"},"source":["! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"],"id":"94a03101-8cdb-4706-bdc4-92ead9aaa9a0","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[?25l\r\u001b[K     |█████                           | 10 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=9743b4c389378a42e208f9e8fc9f8a384b37f9e81d4cc40abb87a14a6cd8a121\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","Successfully built ftfy\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.0.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-vdgs25cb\n","  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-vdgs25cb\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.0.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.9.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.10.0+cu111)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.7.4.3)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369090 sha256=c96ea5c560c0a3a5063815df37e497d79d7e6a06c06ee69ba21feff0d84e8f72\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-c6cwmudc/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d596d512-a70e-4a4f-99ec-d9eb0393a482","executionInfo":{"status":"ok","timestamp":1635087406352,"user_tz":240,"elapsed":25449,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"25b1e572-c785-478d-be11-a5eb0dd3ec68"},"source":["import numpy as np\n","import torch\n","import pickle\n","import itertools\n","import os\n","import cv2\n","from PIL import Image\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.cuda.amp import GradScaler, autocast\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","print(\"Torch version:\", torch.__version__)\n","\n","assert torch.__version__.split(\".\") >= [\"1\", \"7\", \"1\"], \"PyTorch 1.7.1 or later is required\""],"id":"d596d512-a70e-4a4f-99ec-d9eb0393a482","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 1.9.0+cu111\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eccf1d7e-469c-4876-936d-0c799f8ac78b","executionInfo":{"status":"ok","timestamp":1635087407678,"user_tz":240,"elapsed":1342,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"54b622be-93a0-48ec-954c-968b4cade7e4"},"source":["import clip\n","\n","clip.available_models()"],"id":"eccf1d7e-469c-4876-936d-0c799f8ac78b","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f78b6ff5-181a-44a6-8c68-0ff48b3f56a7","executionInfo":{"status":"ok","timestamp":1635088650918,"user_tz":240,"elapsed":4812,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"16ee2470-b181-4ef6-a7c7-bc455482ee05"},"source":["model, preprocess = clip.load(\"ViT-B/32\")\n","model.cuda().eval()\n","input_resolution = model.visual.input_resolution\n","context_length = model.context_length\n","vocab_size = model.vocab_size\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"],"id":"f78b6ff5-181a-44a6-8c68-0ff48b3f56a7","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Model parameters: 151,277,313\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]}]},{"cell_type":"code","metadata":{"id":"W38-m4DL31v4","executionInfo":{"status":"ok","timestamp":1635088655967,"user_tz":240,"elapsed":1188,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}}},"source":["# CLIP has some layers explicitly parameterized using fp16 values. We need to\n","# convert them back to fp32 in order to use automatic mixed-precision training\n","def convert_weights(model: nn.Module):\n","    \"\"\"Convert applicable model parameters to fp32\"\"\"\n","\n","    def _convert_weights_to_fp32(l):\n","        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n","            l.weight.data = l.weight.data.float()\n","            if l.bias is not None:\n","                l.bias.data = l.bias.data.float()\n","\n","        if isinstance(l, nn.MultiheadAttention):\n","            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n","                tensor = getattr(l, attr)\n","                if tensor is not None:\n","                    tensor.data = tensor.data.float()\n","\n","        for name in [\"text_projection\", \"proj\"]:\n","            if hasattr(l, name):\n","                attr = getattr(l, name)\n","                if attr is not None:\n","                    attr.data = attr.data.float()\n","\n","    model.apply(_convert_weights_to_fp32)\n","\n","convert_weights(model)"],"id":"W38-m4DL31v4","execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"abwrNXYP8ibS","executionInfo":{"status":"ok","timestamp":1635087446708,"user_tz":240,"elapsed":7318,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}}},"source":["def preprocess_img(path):\n","  img = cv2.imread(path)\n","  img = cv2.resize(img, (256, 256))\n","  img = img.astype(np.float32) / 255\n","  return img\n","\n","def read_pickle(fn):\n","\twith open(fn, \"rb\") as f:\n","\t\treturn pickle.load(f)\n","  \n","DATASET_DIR = \"/content/drive/MyDrive/coursework/mmml/DecorAssistant/dataset/text_data/\"\n","IMAGES_DIR = \"/content/drive/MyDrive/coursework/mmml/DecorAssistant/dataset/images/all_items/\"\n","  \n","# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n","room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n","# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n","item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n","# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n","item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n","# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n","item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n","# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n","item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n","# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n","room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n","\n","# Some simple preprossing\n","item_to_info = {key : value[\"type\"] + \" \" +\n","                             value[\"desc\"]\n","                       for key, value in item_property.items()}\n","\n","room_to_items = {}\n","\n","for item_url, room_url_list in item_to_rooms_map.items():\n","  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n","\n","  for room_url in room_url_list:\n","    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n","    if room_id not in room_to_items:\n","      room_to_items[room_id] = []\n","    else:\n","      room_to_items[room_id].append(item_id)\n","\n","all_positive_pairs = []\n","for room, item_id_list in room_to_items.items():\n","  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2))\n","  all_positive_pairs += pairs_for_current_room\n","\n","\n","train_pairs = all_positive_pairs[500:650]\n","val_pairs = train_pairs"],"id":"abwrNXYP8ibS","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"tj3VG6AU8pu9","executionInfo":{"status":"ok","timestamp":1635087478354,"user_tz":240,"elapsed":31648,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}}},"source":["image_premise_id_list = [x[0] for x in train_pairs]\n","image_hypothesis_id_list = [x[1] for x in train_pairs]\n","X_image_premise = torch.stack([preprocess(Image.open(IMAGES_DIR + image_id + \".jpg\")) for image_id in image_premise_id_list])\n","X_image_hypothesis = torch.stack([preprocess(Image.open(IMAGES_DIR + image_id + \".jpg\")) for image_id in image_hypothesis_id_list])\n","\n","y = np.array([np.array([0, 1]) for _ in range(len(train_pairs))])\n","\n","premise_texts = [item_to_info[id] for id in image_premise_id_list]\n","hypothesis_texts = [item_to_info[id] for id in image_hypothesis_id_list]\n","\n","X_text_premise = clip.tokenize(premise_texts)\n","X_text_hypothesis = clip.tokenize(hypothesis_texts)"],"id":"tj3VG6AU8pu9","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4AB9yI99bIR","executionInfo":{"status":"ok","timestamp":1635087478356,"user_tz":240,"elapsed":16,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"39604ede-d8c4-45c9-c1fc-e52676dea649"},"source":["BATCH_SIZE = 32\n","\n","img_train_data = TensorDataset(X_image_premise, X_image_hypothesis, torch.from_numpy(y))\n","text_train_data = TensorDataset(X_text_premise, X_text_hypothesis, torch.from_numpy(y))\n","\n","img_val_data = img_train_data\n","text_val_data = text_train_data\n","\n","text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n","img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n","\n","text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n","img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n","\n","print(len(text_train_loader), len(img_train_loader))\n","print(len(text_val_loader), len(img_val_loader))"],"id":"T4AB9yI99bIR","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["5 5\n","5 5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0JB4-tFMYbZ","executionInfo":{"status":"ok","timestamp":1635089866388,"user_tz":240,"elapsed":817,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"ea889155-2135-4098-d7e2-647c4c3ad20b"},"source":["# Find out embedding shapes\n","print(model.encode_image(X_image_premise[0:1].cuda()).size())\n","print(model.encode_text(X_text_premise[0:1].cuda()).size())"],"id":"F0JB4-tFMYbZ","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 512])\n","torch.Size([1, 512])\n"]}]},{"cell_type":"code","metadata":{"id":"vvJNMnx-9pVe","executionInfo":{"status":"ok","timestamp":1635088662567,"user_tz":240,"elapsed":440,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}}},"source":["class CLIPIKEA(nn.Module):\n","    def __init__(self, clip_model, embedding_dim, n_out):\n","        super(CLIPIKEA, self).__init__()\n","\n","        self.clip_model = clip_model\n","        self.combined_fc1 = nn.Linear(embedding_dim * 4, 256)\n","        self.output_fc = nn.Linear(256, n_out)\n","\n","    def forward(self, txt_1, txt_2, img_1, img_2):\n","        batch_size = txt_1.size(0)\n","\n","        with autocast(enabled=False):\n","            txt_emb_1 = self.clip_model.encode_text(txt_1)\n","            txt_emb_2 = self.clip_model.encode_text(txt_2)\n","            img_emb_1 = self.clip_model.encode_image(img_1)\n","            img_emb_2 = self.clip_model.encode_image(img_2)\n","\n","        all_emb = torch.cat((txt_emb_1, txt_emb_2, img_emb_1, img_emb_2), 1)\n","        x_comb = F.relu(self.combined_fc1(all_emb))\n","        out = self.output_fc(x_comb)\n","\n","        return out"],"id":"vvJNMnx-9pVe","execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45R7g0x8NihA","executionInfo":{"status":"ok","timestamp":1635088667176,"user_tz":240,"elapsed":896,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"140c8302-4c3f-4dfc-ea3f-de8157d133c1"},"source":["output_size = y.shape[1]\n","print(output_size)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","full_model = CLIPIKEA(model, 512, output_size)\n","full_model.to(device)\n","\n","lr=0.001\n","# criterion = nn.MultiLabelSoftMarginLoss()\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(full_model.parameters(), lr=lr, weight_decay=1e-5)"],"id":"45R7g0x8NihA","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","cuda\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68imyr2CN0G1","executionInfo":{"status":"ok","timestamp":1635089677623,"user_tz":240,"elapsed":13645,"user":{"displayName":"Venkatesh Sivaraman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14174966050689348145"}},"outputId":"504746cb-18bc-4b62-d4ee-09def6b0b4aa"},"source":["epochs = 1\n","grad_clip = 5\n","\n","# Scale gradients to use fp16 training\n","scaler = GradScaler()\n","\n","full_model.train()\n","for i in range(epochs):\n","    total_acc_train = 0\n","    total_loss_train = 0\n","\n","    for lstm, cnn in zip(text_train_loader, img_train_loader):\n","        lstm_inp1, lstm_inp2, lstm_labels = lstm\n","        cnn_inp1, cnn_inp2, cnn_labels = cnn\n","        \n","        with autocast():\n","            lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n","            cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n","            full_model.zero_grad()\n","            output = full_model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n","            loss = criterion(output.squeeze(), lstm_labels.half())\n","        \n","        scaler.scale(loss).backward()\n","        \n","        scaler.unscale_(optimizer)\n","        nn.utils.clip_grad_norm_(full_model.parameters(), grad_clip)\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        with torch.no_grad():\n","            acc = torch.abs(torch.sigmoid(output.squeeze()) - lstm_labels.float()).view(-1)\n","            acc = (1. - acc.sum() / acc.size()[0])\n","            total_acc_train += acc\n","            total_loss_train += loss.item()\n","  \n","    train_acc = total_acc_train/len(text_train_loader)\n","    train_loss = total_loss_train/len(text_train_loader)\n","    full_model.eval()\n","    total_acc_val = 0\n","    total_loss_val = 0\n","    with torch.no_grad():\n","        for lstm, cnn in zip(text_val_loader, img_val_loader):\n","            lstm_inp1, lstm_inp2, lstm_labels = lstm\n","            cnn_inp1, cnn_inp2, cnn_labels = cnn\n","            lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n","            cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n","            full_model.zero_grad()\n","            output = full_model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n","            val_loss = criterion(output.squeeze(), lstm_labels.float())\n","            acc = torch.abs(torch.sigmoid(output.squeeze()) - lstm_labels.float()).view(-1)\n","            acc = (1. - acc.sum() / acc.size()[0])\n","            total_acc_val += acc\n","            total_loss_val += val_loss.item()\n","    val_acc = total_acc_val/len(text_val_loader)\n","    val_loss = total_loss_val/len(text_val_loader)\n","    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n","    full_model.train()\n","    torch.cuda.empty_cache()"],"id":"68imyr2CN0G1","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: train_loss: 0.0000 train_acc: 1.0000 | val_loss: 0.0000 val_acc: 1.0000\n"]}]},{"cell_type":"code","metadata":{"id":"tLROalwnTgPv"},"source":[""],"id":"tLROalwnTgPv","execution_count":null,"outputs":[]}]}