{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe1FTawUffC5"
      },
      "source": [
        "# Hyperparams and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JvNGzXEy7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761d5df8-4c2d-4622-b73f-bd23339f53f4"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 17:19:48--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.204.104\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.204.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.3MB/s    in 99s     \n",
            "\n",
            "2021-10-26 17:21:28 (15.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLocJQsNqoT"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "random.seed(517)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwYoWs8Ns-S"
      },
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/DecorAssist/\"\n",
        "DATASET_DIR = ROOT_DIR + \"IKEA/text_data/\"\n",
        "IMAGES_DIR = ROOT_DIR + \"IKEA/images/all_items/\"\n",
        "\n",
        "# Global Parameter Variables\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "NUM_WORDS_TOKENIZER = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 16\n",
        "POSITIVE_SIZE = 300 # We might only use a subset of the positive pairs\n",
        "TRAIN_TEST_RATIO = 0.33\n",
        "\n",
        "# Model Hyperparameters\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LEARNING_RATE = 2e-4 # 0.001\n",
        "HIDDEN_DIM = 128 # 64\n",
        "N_LAYERS = 4 # 2\n",
        "EPOCHS = 10\n",
        "CLIP = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYbhsNmNgzV"
      },
      "source": [
        "def preprocess_img(path):\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.resize(img, (75, 115))\n",
        "  img = img.astype(np.float32) / 255\n",
        "  return img\n",
        "\n",
        "\n",
        "def read_pickle(fn):\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)\n",
        "  \n",
        "\n",
        "def random_negative_sampling(all_positive_pairs):\n",
        "  num_examples = len(all_positive_pairs) * 2\n",
        "  all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "  negative_count = 0\n",
        "  selected_negative_pairs = []\n",
        "  while negative_count < num_examples / 2:\n",
        "    random_pair = tuple(random.sample(all_item_ids, 2))\n",
        "    if random_pair in all_positive_pairs:\n",
        "      continue\n",
        "    else:\n",
        "      selected_negative_pairs.append(random_pair)\n",
        "      negative_count += 1\n",
        "  return selected_negative_pairs\n",
        "\n",
        "\n",
        "def get_embedding_matrix(word_index, weights_path=\"/content/GoogleNews-vectors-negative300.bin\"):\n",
        "  word2vecDict = KeyedVectors.load_word2vec_format(weights_path, binary=True)\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "  for word in word2vecDict.wv.vocab:\n",
        "    embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "  print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
        "        \n",
        "  embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
        "\n",
        "  embeddedCount = 0\n",
        "  for word, i in word_index.items():\n",
        "    i-=1\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "  print(\"total embedded:\", embeddedCount, \"common words\")\n",
        "  del(embeddings_index)\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRFmhR8RGJJ"
      },
      "source": [
        "# Build Train and Eval Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJPk6ZK5mSw-"
      },
      "source": [
        "#### Load raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaq9zR8RQRIp"
      },
      "source": [
        "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
        "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
        "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
        "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
        "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
        "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
        "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
        "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
        "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
        "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
        "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
        "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
        "\n",
        "# Some simple preprossing\n",
        "item_to_info = {key : value[\"type\"] + \" \" +\n",
        "                             value[\"desc\"]\n",
        "                       for key, value in item_property.items()} # remove view more info\n",
        "\n",
        "room_to_items = {}\n",
        "\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "\n",
        "  for room_url in room_url_list:\n",
        "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if room_id not in room_to_items:\n",
        "      room_to_items[room_id] = []\n",
        "    else:\n",
        "      room_to_items[room_id].append(item_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDaGXCr4mV0I"
      },
      "source": [
        "#### Construct positive and negative pairs\n",
        "\n",
        "For IR-style problem, seen and unseen can be tricky. We need to discuss whether unseen means \"unseen pairs\" or \"unseen image or text\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0MZqyUWmN87"
      },
      "source": [
        "all_positive_pairs = []\n",
        "for room, item_id_list in room_to_items.items():\n",
        "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
        "  all_positive_pairs += pairs_for_current_room\n",
        "\n",
        "all_positive_pairs = all_positive_pairs[:POSITIVE_SIZE]\n",
        "all_pairs = all_positive_pairs + random_negative_sampling(all_positive_pairs)\n",
        "y = np.array([np.array([1, 0]) for _ in range(len(all_positive_pairs))] + \n",
        "             [np.array([0, 1]) for _ in range(len(all_positive_pairs))])\n",
        "train_pairs, val_paris, y_train, y_val = train_test_split(all_pairs, y, test_size=TRAIN_TEST_RATIO, random_state=517)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRaA0zbGmeyt"
      },
      "source": [
        "#### Build PyTorch dataloader for train/val image/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckQwq5lIN82"
      },
      "source": [
        "train_image_premise_id_list = [x[0] for x in train_pairs]\n",
        "train_image_hypothesis_id_list = [x[1] for x in train_pairs]\n",
        "X_train_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_premise_id_list)))\n",
        "X_train_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_hypothesis_id_list)))\n",
        "X_train_image_premise = np.reshape(X_train_image_premise, (X_train_image_premise.shape[0], 3, 75, 115))\n",
        "X_train_image_hypothesis = np.reshape(X_train_image_hypothesis, (X_train_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai_e-I4NiFyk"
      },
      "source": [
        "val_image_premise_id_list = [x[0] for x in val_paris]\n",
        "val_image_hypothesis_id_list = [x[1] for x in val_paris]\n",
        "X_val_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_premise_id_list)))\n",
        "X_val_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_hypothesis_id_list)))\n",
        "X_val_image_premise = np.reshape(X_val_image_premise, (X_val_image_premise.shape[0], 3, 75, 115))\n",
        "X_val_image_hypothesis = np.reshape(X_val_image_hypothesis, (X_val_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO1ZkgcUK1m7",
        "outputId": "d087a8c0-d1df-4dc1-d28c-0a6d9ca13080"
      },
      "source": [
        "train_premise_texts = [item_to_info[id] for id in train_image_premise_id_list]\n",
        "train_hypothesis_texts = [item_to_info[id] for id in train_image_hypothesis_id_list]\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
        "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
        "WORD_INDEX = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
        "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
        "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
        "\n",
        "X_train_text_premise = tokenizer.texts_to_sequences(train_premise_texts)\n",
        "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_train_text_hypothesis = tokenizer.texts_to_sequences(train_hypothesis_texts)\n",
        "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 288 unique tokens.\n",
            "Max len: 100\n",
            "Loaded 3000000 word vectors.\n",
            "total embedded: 272 common words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAakls6kMkX"
      },
      "source": [
        "val_premise_texts = [item_to_info[id] for id in val_image_premise_id_list]\n",
        "val_hypothesis_texts = [item_to_info[id] for id in val_image_hypothesis_id_list]\n",
        "\n",
        "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
        "X_val_text_premise = tokenizer.texts_to_sequences(val_premise_texts)\n",
        "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_val_text_hypothesis = tokenizer.texts_to_sequences(val_hypothesis_texts)\n",
        "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vm9RcV-MWk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692c346f-b0ed-49ef-9e96-c984cc06f065"
      },
      "source": [
        "img_train_data = TensorDataset(torch.from_numpy(X_train_image_premise), torch.from_numpy(X_train_image_hypothesis), torch.from_numpy(y_train))\n",
        "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise), torch.from_numpy(X_train_text_hypothesis), torch.from_numpy(y_train))\n",
        "\n",
        "img_val_data = TensorDataset(torch.from_numpy(X_val_image_premise), torch.from_numpy(X_val_image_hypothesis), torch.from_numpy(y_val))\n",
        "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise), torch.from_numpy(X_val_text_hypothesis), torch.from_numpy(y_val))\n",
        "\n",
        "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
        "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
        "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_train_loader), len(img_train_loader))\n",
        "print(len(text_val_loader), len(img_val_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 26\n",
            "13 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW73L3zZNe1e"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7w-DGAxyiZ"
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "\n",
        "    # LSTM for the text overview\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # CNN for the posters\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.max_pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "    self.max_pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "    self.max_pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
        "    self.max_pool4 = nn.MaxPool2d(2)\n",
        "    self.cnn_dropout = nn.Dropout(0.1)\n",
        "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
        "\n",
        "    # Concat layer for the combined feature space\n",
        "    # self.combined_fc1 = nn.Linear(640, 256)\n",
        "    self.combined_fc1 = nn.Linear(640*2, 256)\n",
        "    self.combined_fc2 = nn.Linear(256, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def lstm_encoder(self, lstm_inp):\n",
        "    batch_size = lstm_inp.size(0)\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    lstm_inp = lstm_inp.long()\n",
        "    embeds = self.emb(lstm_inp)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = self.dropout(lstm_out[:, -1])\n",
        "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
        "    return lstm_out\n",
        "\n",
        "  def cnn_encoder(self, cnn_inp):\n",
        "    x = F.relu(self.conv1(cnn_inp))\n",
        "    x = self.max_pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.max_pool2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.max_pool3(x)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = self.max_pool4(x)\n",
        "    x = x.view(-1, 5 * 2 * 128)\n",
        "    x = self.cnn_dropout(x)\n",
        "    cnn_out = F.relu(self.cnn_fc(x))\n",
        "    return cnn_out\n",
        "\n",
        "  def forward(self, lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2):\n",
        "    cnn_out1, cnn_out2 = self.cnn_encoder(cnn_inp1), self.cnn_encoder(cnn_inp2)\n",
        "    lstm_out1, lstm_out2 = self.lstm_encoder(lstm_inp1), self.lstm_encoder(lstm_inp2)\n",
        "    combined_inp = torch.cat((cnn_out1, cnn_out2, lstm_out1, lstm_out2), 1)\n",
        "    x_comb = F.relu(self.combined_fc1(combined_inp))\n",
        "    x_comb = F.relu(self.combined_fc2(x_comb))\n",
        "    # out = torch.sigmoid(self.output_fc(x_comb))\n",
        "    out = self.softmax(self.output_fc(x_comb))\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    return hidden"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2rumtEQYv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bb45ee-4835-4a44-96d4-dafc6108407a"
      },
      "source": [
        "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
        "\n",
        "model = CNN_LSTM(len(WORD_INDEX)+1, WORD2VEC_EMBEDDING_MATRIX, HIDDEN_DIM, N_LAYERS, y.shape[1])\n",
        "model.to(DEVICE)\n",
        "print(\"Model Architecture {}\\n\".format(model))\n",
        "\n",
        "lr= LEARNING_RATE\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "print(\"Training Started...\")\n",
        "model.train()\n",
        "for i in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "    \n",
        "  for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE), lstm_labels.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc\n",
        "      total_loss_train += loss.item()\n",
        "  \n",
        "  train_acc = total_acc_train/len(text_train_loader)\n",
        "  train_loss = total_loss_train/len(text_train_loader)\n",
        "  model.eval()\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  with torch.no_grad():\n",
        "    for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE), lstm_labels.to(DEVICE)\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "      model.zero_grad()\n",
        "      output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_val += acc\n",
        "      total_loss_val += val_loss.item()\n",
        "  val_acc = total_acc_val/len(text_val_loader)\n",
        "  val_loss = total_loss_val/len(text_val_loader)\n",
        "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "  model.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device: cuda\n",
            "\n",
            "Model Architecture CNN_LSTM(\n",
            "  (emb): Embedding(289, 300)\n",
            "  (lstm): LSTM(300, 128, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (lstm_fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (cnn_fc): Linear(in_features=1280, out_features=512, bias=True)\n",
            "  (combined_fc1): Linear(in_features=1280, out_features=256, bias=True)\n",
            "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (output_fc): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "Training Started...\n",
            "Epoch 1: train_loss: 0.6941 train_acc: 0.4995 | val_loss: 0.6931 val_acc: 0.5000\n",
            "Epoch 2: train_loss: 0.6933 train_acc: 0.4999 | val_loss: 0.6932 val_acc: 0.5000\n",
            "Epoch 3: train_loss: 0.6931 train_acc: 0.5000 | val_loss: 0.6931 val_acc: 0.5000\n",
            "Epoch 4: train_loss: 0.6916 train_acc: 0.5008 | val_loss: 0.6920 val_acc: 0.5008\n",
            "Epoch 5: train_loss: 0.6846 train_acc: 0.5053 | val_loss: 0.6903 val_acc: 0.5040\n",
            "Epoch 6: train_loss: 0.6715 train_acc: 0.5172 | val_loss: 0.6974 val_acc: 0.5108\n",
            "Epoch 7: train_loss: 0.6506 train_acc: 0.5368 | val_loss: 0.6955 val_acc: 0.5169\n",
            "Epoch 8: train_loss: 0.6292 train_acc: 0.5577 | val_loss: 0.6967 val_acc: 0.5268\n",
            "Epoch 9: train_loss: 0.6132 train_acc: 0.5780 | val_loss: 0.7088 val_acc: 0.5283\n",
            "Epoch 10: train_loss: 0.5928 train_acc: 0.5869 | val_loss: 0.6933 val_acc: 0.5547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRkG-JWjfCj"
      },
      "source": [
        "# Ranker\n",
        "- For image 1 image 2 text 1 text 2 as input, we generate a score ~(0,1).\n",
        "- Search engine: only given one image and one text, compute the score for this item with all the other items in the pool. E.g, if we have num_items, the score matrix will be (num_items, ). Find predicted top 5 or top 10.\n",
        "- Haocheng's ranking evaluation: for a specific item, we know all the ground truth items that are in the same room as this specific item. \n",
        "\n",
        "The output from yuanxin: {\"802.903.9234.923\" : score, xxx}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOlqTLXknYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d34e073-d484-4a52-eb64-0b9f716ddd01"
      },
      "source": [
        "def single_pair_inference(premise_image_path, hypothesis_image_path, premise_text, hypothesis_text, model, tokenizer):\n",
        "  image_premise, image_hypothesis = preprocess_img(premise_image_path), preprocess_img(hypothesis_image_path)\n",
        "  image_premise = np.reshape(image_premise, (1, 3, 75, 115))\n",
        "  image_hypothesis = np.reshape(image_hypothesis, (1, 3, 75, 115))\n",
        "  premise_sequence = tokenizer.texts_to_sequences([premise_text])\n",
        "  premise_sequence = pad_sequences(premise_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  hypothesis_sequence = tokenizer.texts_to_sequences([hypothesis_text])\n",
        "  hypothesis_sequence = pad_sequences(hypothesis_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "  img_data = TensorDataset(torch.from_numpy(image_premise), torch.from_numpy(image_hypothesis))\n",
        "  text_data = TensorDataset(torch.from_numpy(premise_sequence), torch.from_numpy(hypothesis_sequence))\n",
        "  \n",
        "  text_loader = DataLoader(text_data, batch_size=1)\n",
        "  img_loader = DataLoader(img_data, batch_size=1)\n",
        "\n",
        "  for lstm, cnn in zip(text_loader, img_loader):\n",
        "    lstm_inp1, lstm_inp2 = lstm\n",
        "    cnn_inp1, cnn_inp2 = cnn\n",
        "    lstm_inp1, lstm_inp2 = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2 = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "  return output.squeeze().cpu().detach().numpy().tolist()\n",
        "  \n",
        "\n",
        "\n",
        "image_id_1 = train_image_premise_id_list[0]\n",
        "image_id_2 = train_image_hypothesis_id_list[0]\n",
        "text_1 = train_premise_texts[0]\n",
        "text_2 = train_hypothesis_texts[0]\n",
        "single_pair_inference(premise_image_path=IMAGES_DIR + image_id_1 + \".jpg\",\n",
        "          hypothesis_image_path=IMAGES_DIR + image_id_2 + \".jpg\",\n",
        "          premise_text=text_1,\n",
        "          hypothesis_text=text_2,\n",
        "          model=model,\n",
        "          tokenizer=tokenizer)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7792053818702698, 0.22079461812973022]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0qmvz0xpmci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7357c58-972c-4207-8ec8-b26d7d3b30bc"
      },
      "source": [
        "len(all_pairs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck31vLsy5Mth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}