{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe1FTawUffC5"
      },
      "source": [
        "# Hyperparams and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JvNGzXEy7H2"
      },
      "source": [
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "# !gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLocJQsNqoT"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "random.seed(517)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwYoWs8Ns-S"
      },
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/DecorAssist/\"\n",
        "DATASET_DIR = ROOT_DIR + \"IKEA/text_data/\"\n",
        "IMAGES_DIR = ROOT_DIR + \"IKEA/images/all_items/\"\n",
        "\n",
        "# Global Parameter Variables\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "NUM_WORDS_TOKENIZER = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 16\n",
        "POSITIVE_SIZE = 2000 # We might only use a subset of the positive pairs\n",
        "\n",
        "# Model Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_DIM = 64\n",
        "N_LAYERS = 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYbhsNmNgzV"
      },
      "source": [
        "def preprocess_img(path):\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.resize(img, (75, 115))\n",
        "  img = img.astype(np.float32) / 255\n",
        "  return img\n",
        "\n",
        "\n",
        "def read_pickle(fn):\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)\n",
        "  \n",
        "\n",
        "def random_negative_sampling(all_positive_pairs):\n",
        "  num_examples = len(all_positive_pairs) * 2\n",
        "  all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "  negative_count = 0\n",
        "  selected_negative_pairs = []\n",
        "  while negative_count < num_examples / 2:\n",
        "    random_pair = tuple(random.sample(all_item_ids, 2))\n",
        "    if random_pair in all_positive_pairs:\n",
        "      continue\n",
        "    else:\n",
        "      selected_negative_pairs.append(random_pair)\n",
        "      negative_count += 1\n",
        "  return selected_negative_pairs\n",
        "\n",
        "\n",
        "def get_embedding_matrix(word_index, weights_path=\"/content/GoogleNews-vectors-negative300.bin\"):\n",
        "  word2vecDict = KeyedVectors.load_word2vec_format(weights_path, binary=True)\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "  for word in word2vecDict.wv.vocab:\n",
        "    embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "  print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
        "        \n",
        "  embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
        "\n",
        "  embeddedCount = 0\n",
        "  for word, i in word_index.items():\n",
        "    i-=1\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "  print(\"total embedded:\", embeddedCount, \"common words\")\n",
        "  del(embeddings_index)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRFmhR8RGJJ"
      },
      "source": [
        "# Build Train and Eval Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJPk6ZK5mSw-"
      },
      "source": [
        "#### Load raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaq9zR8RQRIp"
      },
      "source": [
        "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
        "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
        "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
        "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
        "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
        "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
        "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
        "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
        "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
        "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
        "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
        "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
        "\n",
        "# Some simple preprossing\n",
        "item_to_info = {key : value[\"type\"] + \" \" +\n",
        "                             value[\"desc\"]\n",
        "                       for key, value in item_property.items()} # remove view more info\n",
        "\n",
        "room_to_items = {}\n",
        "\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "\n",
        "  for room_url in room_url_list:\n",
        "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if room_id not in room_to_items:\n",
        "      room_to_items[room_id] = []\n",
        "    else:\n",
        "      room_to_items[room_id].append(item_id)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDaGXCr4mV0I"
      },
      "source": [
        "#### Construct positive and negative pairs\n",
        "\n",
        "For IR-style problem, seen and unseen can be tricky. We need to discuss whether unseen means \"unseen pairs\" or \"unseen image or text\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0MZqyUWmN87"
      },
      "source": [
        "all_positive_pairs = []\n",
        "for room, item_id_list in room_to_items.items():\n",
        "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
        "  all_positive_pairs += pairs_for_current_room\n",
        "\n",
        "all_positive_pairs = all_positive_pairs[:POSITIVE_SIZE]\n",
        "all_pairs = all_positive_pairs + random_negative_sampling(all_positive_pairs)\n",
        "y = np.array([np.array([1, 0]) for _ in range(len(all_positive_pairs))] + \n",
        "             [np.array([0, 1]) for _ in range(len(all_positive_pairs))])\n",
        "train_pairs, val_paris, y_train, y_val = train_test_split(all_pairs, y, test_size=0.5, random_state=517)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRaA0zbGmeyt"
      },
      "source": [
        "#### Build PyTorch dataloader for train/val image/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckQwq5lIN82"
      },
      "source": [
        "train_image_premise_id_list = [x[0] for x in train_pairs]\n",
        "train_image_hypothesis_id_list = [x[1] for x in train_pairs]\n",
        "X_train_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_premise_id_list)))\n",
        "X_train_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_hypothesis_id_list)))\n",
        "X_train_image_premise = np.reshape(X_train_image_premise, (X_train_image_premise.shape[0], 3, 75, 115))\n",
        "X_train_image_hypothesis = np.reshape(X_train_image_hypothesis, (X_train_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai_e-I4NiFyk"
      },
      "source": [
        "val_image_premise_id_list = [x[0] for x in val_paris]\n",
        "val_image_hypothesis_id_list = [x[1] for x in val_paris]\n",
        "X_val_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_premise_id_list)))\n",
        "X_val_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_hypothesis_id_list)))\n",
        "X_val_image_premise = np.reshape(X_val_image_premise, (X_val_image_premise.shape[0], 3, 75, 115))\n",
        "X_val_image_hypothesis = np.reshape(X_val_image_hypothesis, (X_val_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO1ZkgcUK1m7",
        "outputId": "805c3c88-aec9-4a10-a45b-194aab550de0"
      },
      "source": [
        "train_premise_texts = [item_to_info[id] for id in train_image_premise_id_list]\n",
        "train_hypothesis_texts = [item_to_info[id] for id in train_image_hypothesis_id_list]\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
        "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
        "WORD_INDEX = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
        "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
        "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
        "\n",
        "X_train_text_premise = tokenizer.texts_to_sequences(train_premise_texts)\n",
        "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_train_text_hypothesis = tokenizer.texts_to_sequences(train_hypothesis_texts)\n",
        "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 667 unique tokens.\n",
            "Max len: 100\n",
            "Loaded 3000000 word vectors.\n",
            "total embedded: 632 common words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAakls6kMkX"
      },
      "source": [
        "val_premise_texts = [item_to_info[id] for id in val_image_premise_id_list]\n",
        "val_hypothesis_texts = [item_to_info[id] for id in val_image_hypothesis_id_list]\n",
        "\n",
        "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
        "X_val_text_premise = tokenizer.texts_to_sequences(val_premise_texts)\n",
        "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_val_text_hypothesis = tokenizer.texts_to_sequences(val_hypothesis_texts)\n",
        "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vm9RcV-MWk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f7d431-79ef-4082-ac59-9f970e76f7a8"
      },
      "source": [
        "img_train_data = TensorDataset(torch.from_numpy(X_train_image_premise), torch.from_numpy(X_train_image_hypothesis), torch.from_numpy(y_train))\n",
        "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise), torch.from_numpy(X_train_text_hypothesis), torch.from_numpy(y_train))\n",
        "\n",
        "img_val_data = TensorDataset(torch.from_numpy(X_val_image_premise), torch.from_numpy(X_val_image_hypothesis), torch.from_numpy(y_val))\n",
        "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise), torch.from_numpy(X_val_text_hypothesis), torch.from_numpy(y_val))\n",
        "\n",
        "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
        "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
        "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_train_loader), len(img_train_loader))\n",
        "print(len(text_val_loader), len(img_val_loader))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125 125\n",
            "125 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW73L3zZNe1e"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7w-DGAxyiZ"
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "\n",
        "    # LSTM for the text overview\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # CNN for the posters\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.max_pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "    self.max_pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "    self.max_pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
        "    self.max_pool4 = nn.MaxPool2d(2)\n",
        "    self.cnn_dropout = nn.Dropout(0.1)\n",
        "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
        "\n",
        "    # Concat layer for the combined feature space\n",
        "    # self.combined_fc1 = nn.Linear(640, 256)\n",
        "    self.combined_fc1 = nn.Linear(640*2, 256)\n",
        "    self.combined_fc2 = nn.Linear(256, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "\n",
        "  def forward(self, lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2):\n",
        "      # LSTM Forward\n",
        "      batch_size = lstm_inp1.size(0)\n",
        "      hidden = self.init_hidden(batch_size)\n",
        "\n",
        "      lstm_inp1 = lstm_inp1.long()\n",
        "      lstm_inp2 = lstm_inp2.long()\n",
        "      embeds1 = self.emb(lstm_inp1)\n",
        "      embeds2 = self.emb(lstm_inp2)\n",
        "      lstm_out1, hidden = self.lstm(embeds1, hidden)\n",
        "      lstm_out1 = self.dropout(lstm_out1[:, -1])\n",
        "      lstm_out1 = F.relu(self.lstm_fc(lstm_out1))\n",
        "\n",
        "      lstm_out2, hidden = self.lstm(embeds2, hidden)\n",
        "      lstm_out2 = self.dropout(lstm_out2[:, -1])\n",
        "      lstm_out2 = F.relu(self.lstm_fc(lstm_out2))\n",
        "\n",
        "      # CNN Forward\n",
        "      x1 = F.relu(self.conv1(cnn_inp1))\n",
        "      x1 = self.max_pool1(x1)\n",
        "      x1 = F.relu(self.conv2(x1))\n",
        "      x1 = self.max_pool2(x1)\n",
        "      x1 = F.relu(self.conv3(x1))\n",
        "      x1 = self.max_pool3(x1)\n",
        "      x1 = F.relu(self.conv4(x1))\n",
        "      x1 = self.max_pool4(x1)\n",
        "      x1 = x1.view(-1, 5*2*128)\n",
        "      x1 = self.cnn_dropout(x1)\n",
        "      cnn_out1 = F.relu(self.cnn_fc(x1))\n",
        "\n",
        "      x2 = F.relu(self.conv1(cnn_inp2))\n",
        "      x2 = self.max_pool1(x2)\n",
        "      x2 = F.relu(self.conv2(x2))\n",
        "      x2 = self.max_pool2(x2)\n",
        "      x2 = F.relu(self.conv3(x2))\n",
        "      x2 = self.max_pool3(x2)\n",
        "      x2 = F.relu(self.conv4(x2))\n",
        "      x2 = self.max_pool4(x2)\n",
        "      x2 = x2.view(-1, 5*2*128)\n",
        "      x2 = self.cnn_dropout(x2)\n",
        "      cnn_out2 = F.relu(self.cnn_fc(x2))\n",
        "\n",
        "      combined_inp = torch.cat((cnn_out1, cnn_out2, lstm_out1, lstm_out2), 1)\n",
        "      x_comb = F.relu(self.combined_fc1(combined_inp))\n",
        "      x_comb = F.relu(self.combined_fc2(x_comb))\n",
        "      out = torch.sigmoid(self.output_fc(x_comb))\n",
        "\n",
        "      return out\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    return hidden"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPz2qovGyBjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f5abd5-3548-4d09-958e-cbae4c188d30"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = CNN_LSTM(len(WORD_INDEX)+1, WORD2VEC_EMBEDDING_MATRIX, HIDDEN_DIM, N_LAYERS, y.shape[1])\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr= LEARNING_RATE\n",
        "# criterion = nn.MultiLabelSoftMarginLoss()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2rumtEQYv8"
      },
      "source": [
        "epochs = 10\n",
        "clip = 5\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "    \n",
        "  for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc\n",
        "      total_loss_train += loss.item()\n",
        "  \n",
        "  train_acc = total_acc_train/len(text_train_loader)\n",
        "  train_loss = total_loss_train/len(text_train_loader)\n",
        "  model.eval()\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  with torch.no_grad():\n",
        "    for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n",
        "      model.zero_grad()\n",
        "      output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_val += acc\n",
        "      total_loss_val += val_loss.item()\n",
        "  val_acc = total_acc_val/len(text_val_loader)\n",
        "  val_loss = total_loss_val/len(text_val_loader)\n",
        "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "  model.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TEh8Rj8ScoF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRkG-JWjfCj"
      },
      "source": [
        "# Inference\n",
        "- For image 1 image 2 text 1 text 2 as input, we generate a score ~(0,1).\n",
        "- Search engine: only given one image and one text, compute the score for this item with all the other items in the pool. E.g, if we have num_items, the score matrix will be (num_items, ). Find predicted top 5 or top 10.\n",
        "- Haocheng's ranking evaluation: for a specific item, we know all the ground truth items that are in the same room as this specific item. \n",
        "\n",
        "The output from yuanxin: {\"802.903.9234.923\" : score, xxx}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOlqTLXknYN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}