{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe1FTawUffC5"
      },
      "source": [
        "# Hyperparams and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JvNGzXEy7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8888e4-5e98-44e0-89e7-995e1fbaf13c"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-28 17:54:51--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.64.22\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  46.8MB/s    in 34s     \n",
            "\n",
            "2021-10-28 17:55:25 (46.1 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLocJQsNqoT"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import itertools\n",
        "import tqdm\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "random.seed(517)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwYoWs8Ns-S"
      },
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/DecorAssist/\"\n",
        "DATASET_DIR = ROOT_DIR + \"IKEA/text_data/\"\n",
        "IMAGES_DIR = ROOT_DIR + \"IKEA/images/all_items/\"\n",
        "\n",
        "# Global Parameter Variables\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "NUM_WORDS_TOKENIZER = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 16\n",
        "POSITIVE_SIZE = 300 # We might only use a subset of the positive pairs\n",
        "TRAIN_TEST_RATIO = 0.33\n",
        "\n",
        "# Model Hyperparameters\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LEARNING_RATE = 2e-4 # 0.001\n",
        "HIDDEN_DIM = 128 # 64\n",
        "N_LAYERS = 4 # 2\n",
        "EPOCHS = 10\n",
        "CLIP = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYbhsNmNgzV"
      },
      "source": [
        "def preprocess_img(path):\n",
        "  img = cv2.imread(path)\n",
        "  try:\n",
        "    img = cv2.resize(img, (75, 115))\n",
        "  except:\n",
        "    print(path)\n",
        "  img = img.astype(np.float32) / 255\n",
        "  img = np.reshape(img, (3, 75 ,115))\n",
        "  return img\n",
        "\n",
        "\n",
        "def read_pickle(fn):\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)\n",
        "  \n",
        "\n",
        "# def random_negative_sampling(total_positive_pairs):\n",
        "#   all_positive_pairs = total_positive_pairs[:POSITIVE_SIZE]\n",
        "#   num_examples = len(all_positive_pairs) * 2\n",
        "#   all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "#   negative_count = 0\n",
        "#   selected_negative_pairs = []\n",
        "#   while negative_count < num_examples / 2:\n",
        "#     random_pair = tuple(random.sample(all_item_ids, 2))\n",
        "#     if random_pair in total_positive_pairs:\n",
        "#       continue\n",
        "#     else:\n",
        "#       selected_negative_pairs.append(random_pair)\n",
        "#       negative_count += 1\n",
        "#   return selected_negative_pairs\n",
        "\n",
        "\n",
        "# Train-val split that does not share products between training and validation sets.\n",
        "def generate_product_limited_samples(products, all_positive_pairs, random_state=None):\n",
        "    \"\"\"\n",
        "    Generates positive and negative examples for the given products using shared\n",
        "    occurence in rooms to indicate whether two products are compatible.\n",
        "\n",
        "    products: A sequence of product IDs; ALL positive and negative pairs must\n",
        "        contain only these product IDs.\n",
        "    all_positive_pairs: A set of product ID pairs that are positive.\n",
        "    \n",
        "    Returns: A tuple (x, y), where x is a sequence of product ID pairs and y is\n",
        "        the array of [0, 1] labels indicating presence in all_positive_pairs.\n",
        "    \"\"\"\n",
        "    product_set = set(products)\n",
        "    within_positive_pairs = [p for p in sorted(all_positive_pairs) if p[0] in product_set and p[1] in product_set]\n",
        "    negative_pairs = random_negative_sampling(products, all_positive_pairs, count=len(within_positive_pairs), random_state=random_state)\n",
        "    x = within_positive_pairs + negative_pairs\n",
        "    y = np.array([1] * len(within_positive_pairs) + [0] * len(negative_pairs))\n",
        "    if random_state is not None: np.random.seed(random_state)\n",
        "    indices = np.random.permutation(np.arange(len(x)))\n",
        "    return [x[i] for i in indices], y[indices]\n",
        "\n",
        "def random_negative_sampling(products, all_positive_pairs, count=None, random_state=None):\n",
        "  selected_negative_pairs = []\n",
        "  if random_state is not None: random.seed(random_state)\n",
        "  while len(selected_negative_pairs) < (count or len(all_positive_pairs)):\n",
        "    random_pair = tuple(random.sample(products, 2))\n",
        "    if random_pair in all_positive_pairs:\n",
        "      continue\n",
        "    else:\n",
        "      selected_negative_pairs.append(random_pair)\n",
        "  return selected_negative_pairs\n",
        "\n",
        "\n",
        "def get_embedding_matrix(word_index, weights_path=\"/content/GoogleNews-vectors-negative300.bin\"):\n",
        "  word2vecDict = KeyedVectors.load_word2vec_format(weights_path, binary=True)\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "  for word in word2vecDict.wv.vocab:\n",
        "    embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "  print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
        "        \n",
        "  embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
        "\n",
        "  embeddedCount = 0\n",
        "  for word, i in word_index.items():\n",
        "    i-=1\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "  print(\"total embedded:\", embeddedCount, \"common words\")\n",
        "  del(embeddings_index)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRFmhR8RGJJ"
      },
      "source": [
        "# Build Train and Eval Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJPk6ZK5mSw-"
      },
      "source": [
        "#### Load raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaq9zR8RQRIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa24ea83-4d6b-4bbe-cc36-21f38cff6ce5"
      },
      "source": [
        "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
        "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
        "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
        "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
        "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
        "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
        "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
        "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
        "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
        "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
        "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
        "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
        "\n",
        "# Some simple preprossing\n",
        "item_to_info = {key : value[\"type\"] + \" \" +\n",
        "                             value[\"desc\"]\n",
        "                       for key, value in item_property.items()} # remove view more info\n",
        "\n",
        "room_to_items = {}\n",
        "\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "  if not os.path.exists(IMAGES_DIR + item_id + \".jpg\"):\n",
        "      print(item_url + \" does not exist\")\n",
        "      continue\n",
        "\n",
        "  for room_url in room_url_list:\n",
        "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if room_id not in room_to_items:\n",
        "      room_to_items[room_id] = [item_id]\n",
        "    else:\n",
        "      room_to_items[room_id].append(item_id)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images/890.333.75.jpg does not exist\n",
            "images/991.333.98.jpg does not exist\n",
            "images/990.612.97.jpg does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDaGXCr4mV0I"
      },
      "source": [
        "#### Construct positive and negative pairs\n",
        "\n",
        "For IR-style problem, seen and unseen can be tricky. We need to discuss whether unseen means \"unseen pairs\" or \"unseen image or text\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0MZqyUWmN87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e89385-867c-48a7-db91-acd57948d409"
      },
      "source": [
        "# total_positive_pairs = []\n",
        "# for room, item_id_list in room_to_items.items():\n",
        "#   pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
        "#   total_positive_pairs += pairs_for_current_room\n",
        "\n",
        "all_positive_pairs = set()\n",
        "for room, item_id_list in room_to_items.items():\n",
        "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
        "  all_positive_pairs |= set(pairs_for_current_room)\n",
        "\n",
        "all_products = sorted(set([x for pair in all_positive_pairs for x in pair]))\n",
        "train_products, val_products = train_test_split(all_products, test_size=TRAIN_TEST_RATIO, random_state=517)\n",
        "\n",
        "# all_positive_pairs = total_positive_pairs[:POSITIVE_SIZE]\n",
        "# all_pairs = all_positive_pairs + random_negative_sampling(total_positive_pairs)\n",
        "# y = np.array([np.array([1, 0]) for _ in range(len(all_positive_pairs))] + \n",
        "#              [np.array([0, 1]) for _ in range(len(all_positive_pairs))])\n",
        "# train_pairs, val_pairs, y_train, y_val = train_test_split(all_pairs, y, test_size=TRAIN_TEST_RATIO, random_state=517)\n",
        "\n",
        "train_pairs, y_train = generate_product_limited_samples(train_products, all_positive_pairs, random_state=517)\n",
        "val_pairs, y_val = generate_product_limited_samples(val_products, all_positive_pairs, random_state=517)\n",
        "print(len(train_pairs), len(val_pairs))\n",
        "\n",
        "with open(ROOT_DIR + \"train_data.pkl\", \"wb\") as file:\n",
        "    pickle.dump((train_pairs, y_train), file)\n",
        "with open(ROOT_DIR + \"val_data.pkl\", \"wb\") as file:\n",
        "    pickle.dump((val_pairs, y_val), file)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22226 5384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tB_dsGrE-h1"
      },
      "source": [
        "# To read the training and validation sets\n",
        "with open(ROOT_DIR + \"train_data.pkl\", \"rb\") as file:\n",
        "    train_pairs, y_train = pickle.load(file)\n",
        "with open(ROOT_DIR + \"val_data.pkl\", \"rb\") as file:\n",
        "    val_pairs, y_val = pickle.load(file)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRaA0zbGmeyt"
      },
      "source": [
        "#### Build PyTorch dataloader for train/val image/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcOsiU0wFNnw"
      },
      "source": [
        "class FurnitureImagePairsDataset(Dataset):\n",
        "    \"\"\"Dataset containing pairs of furniture items.\"\"\"\n",
        "\n",
        "    def __init__(self, image_path, pairs, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_path (string): Path to the directory containing images.\n",
        "            pairs (list of tuples of strings): Pairs of image IDs to be used as training samples.\n",
        "            labels (array of integers): Labels for the training samples.\n",
        "        \"\"\"\n",
        "        super(FurnitureImagePairsDataset, self).__init__()\n",
        "        self.image_ids = list(set(x for pair in pairs for x in pair))\n",
        "        self.index_mapping = {image_id: i for i, image_id in enumerate(self.image_ids)}\n",
        "        self.images = [preprocess_img(image_path + image_id + \".jpg\") for image_id in tqdm.tqdm(self.image_ids)]\n",
        "        self.pairs = pairs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if isinstance(idx, (list, tuple)):\n",
        "            x1, x2, y = zip(*[self[i] for i in idx])\n",
        "            return torch.stack(x1), torch.stack(x2), torch.from_numpy(np.array(y))\n",
        "        pair = self.pairs[idx]\n",
        "        return self.images[self.index_mapping[pair[0]]], self.images[self.index_mapping[pair[1]]], self.labels[idx]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGMo8N_wFP0_",
        "outputId": "a8c7c0ce-d4a0-4631-d6df-ca5198dbfa8b"
      },
      "source": [
        "X_train_image = FurnitureImagePairsDataset(IMAGES_DIR, train_pairs, y_train)\n",
        "X_val_image = FurnitureImagePairsDataset(IMAGES_DIR, val_pairs, y_val)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1456/1456 [00:14<00:00, 101.35it/s]\n",
            "100%|██████████| 718/718 [00:03<00:00, 210.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gnBKL5GP61y",
        "outputId": "bf1a1cfa-6f40-42b6-a866-5e8e8dd93457"
      },
      "source": [
        "X_train_image[0][0].shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 75, 115)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckQwq5lIN82"
      },
      "source": [
        "# train_image_premise_id_list = [x[0] for x in train_pairs]\n",
        "# train_image_hypothesis_id_list = [x[1] for x in train_pairs]\n",
        "# X_train_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_premise_id_list)))\n",
        "# X_train_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_hypothesis_id_list)))\n",
        "# X_train_image_premise = np.reshape(X_train_image_premise, (X_train_image_premise.shape[0], 3, 75, 115))\n",
        "# X_train_image_hypothesis = np.reshape(X_train_image_hypothesis, (X_train_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai_e-I4NiFyk"
      },
      "source": [
        "# val_image_premise_id_list = [x[0] for x in val_pairs]\n",
        "# val_image_hypothesis_id_list = [x[1] for x in val_pairs]\n",
        "# X_val_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_premise_id_list)))\n",
        "# X_val_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_hypothesis_id_list)))\n",
        "# X_val_image_premise = np.reshape(X_val_image_premise, (X_val_image_premise.shape[0], 3, 75, 115))\n",
        "# X_val_image_hypothesis = np.reshape(X_val_image_hypothesis, (X_val_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO1ZkgcUK1m7",
        "outputId": "12b9d312-dd76-453b-f1c4-27b7f67bef85"
      },
      "source": [
        "train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
        "train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
        "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
        "WORD_INDEX = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
        "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
        "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
        "\n",
        "X_train_text_premise = tokenizer.texts_to_sequences(train_premise_texts)\n",
        "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_train_text_hypothesis = tokenizer.texts_to_sequences(train_hypothesis_texts)\n",
        "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2077 unique tokens.\n",
            "Max len: 100\n",
            "Loaded 3000000 word vectors.\n",
            "total embedded: 1922 common words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAakls6kMkX"
      },
      "source": [
        "val_premise_texts = [item_to_info[id] for id, _ in val_pairs]\n",
        "val_hypothesis_texts = [item_to_info[id] for _, id in val_pairs]\n",
        "\n",
        "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
        "X_val_text_premise = tokenizer.texts_to_sequences(val_premise_texts)\n",
        "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_val_text_hypothesis = tokenizer.texts_to_sequences(val_hypothesis_texts)\n",
        "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vm9RcV-MWk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fcb58a3-5f3d-4c33-a7d6-f2e60a4bcdf6"
      },
      "source": [
        "# img_train_data = TensorDataset(torch.from_numpy(X_train_image_premise), torch.from_numpy(X_train_image_hypothesis), torch.from_numpy(y_train))\n",
        "img_train_data = X_train_image\n",
        "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise), torch.from_numpy(X_train_text_hypothesis), torch.from_numpy(y_train))\n",
        "\n",
        "# img_val_data = TensorDataset(torch.from_numpy(X_val_image_premise), torch.from_numpy(X_val_image_hypothesis), torch.from_numpy(y_val))\n",
        "img_val_data = X_val_image\n",
        "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise), torch.from_numpy(X_val_text_hypothesis), torch.from_numpy(y_val))\n",
        "\n",
        "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
        "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
        "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_train_loader), len(img_train_loader))\n",
        "print(len(text_val_loader), len(img_val_loader))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1390 1390\n",
            "337 337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW73L3zZNe1e"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7w-DGAxyiZ"
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "\n",
        "    # LSTM for the text overview\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # CNN for the posters\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.max_pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "    self.max_pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "    self.max_pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
        "    self.max_pool4 = nn.MaxPool2d(2)\n",
        "    self.cnn_dropout = nn.Dropout(0.1)\n",
        "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
        "\n",
        "    # Concat layer for the combined feature space\n",
        "    # self.combined_fc1 = nn.Linear(640, 256)\n",
        "    self.combined_fc1 = nn.Linear(640*2, 256)\n",
        "    self.combined_fc2 = nn.Linear(256, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "    # self.softmax = nn.Softmax()\n",
        "\n",
        "  def lstm_encoder(self, lstm_inp):\n",
        "    batch_size = lstm_inp.size(0)\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    lstm_inp = lstm_inp.long()\n",
        "    embeds = self.emb(lstm_inp)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = self.dropout(lstm_out[:, -1])\n",
        "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
        "    return lstm_out\n",
        "\n",
        "  def cnn_encoder(self, cnn_inp):\n",
        "    x = F.relu(self.conv1(cnn_inp))\n",
        "    x = self.max_pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.max_pool2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.max_pool3(x)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = self.max_pool4(x)\n",
        "    x = x.view(-1, 5 * 2 * 128)\n",
        "    x = self.cnn_dropout(x)\n",
        "    cnn_out = F.relu(self.cnn_fc(x))\n",
        "    return cnn_out\n",
        "\n",
        "  def forward(self, lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2):\n",
        "    cnn_out1, cnn_out2 = self.cnn_encoder(cnn_inp1), self.cnn_encoder(cnn_inp2)\n",
        "    lstm_out1, lstm_out2 = self.lstm_encoder(lstm_inp1), self.lstm_encoder(lstm_inp2)\n",
        "    combined_inp = torch.cat((cnn_out1, cnn_out2, lstm_out1, lstm_out2), 1)\n",
        "    x_comb = F.relu(self.combined_fc1(combined_inp))\n",
        "    x_comb = F.relu(self.combined_fc2(x_comb))\n",
        "    # out = torch.sigmoid(self.output_fc(x_comb))\n",
        "    # out = self.softmax(self.output_fc(x_comb))\n",
        "    out = self.output_fc(x_comb)\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(DEVICE),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(DEVICE))\n",
        "    return hidden"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2rumtEQYv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aac9079-1043-4dd1-861b-71b5b51ff9d2"
      },
      "source": [
        "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
        "\n",
        "model = CNN_LSTM(len(WORD_INDEX)+1, WORD2VEC_EMBEDDING_MATRIX, HIDDEN_DIM, N_LAYERS, 1)\n",
        "model.to(DEVICE)\n",
        "print(\"Model Architecture {}\\n\".format(model))\n",
        "\n",
        "lr= LEARNING_RATE\n",
        "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "print(\"Training Started...\")\n",
        "model.train()\n",
        "for i in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "    \n",
        "  for lstm, cnn in tqdm.tqdm(zip(text_train_loader, img_train_loader), total=len(text_train_loader)):\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE), lstm_labels.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    loss = criterion(torch.round(torch.sigmoid(output.squeeze())), lstm_labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc\n",
        "      total_loss_train += loss.item()\n",
        "  \n",
        "  train_acc = total_acc_train/len(text_train_loader)\n",
        "  train_loss = total_loss_train/len(text_train_loader)\n",
        "  model.eval()\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  with torch.no_grad():\n",
        "    for lstm, cnn in tqdm.tqdm(zip(text_val_loader, img_val_loader), total=len(text_val_loader)):\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE), lstm_labels.to(DEVICE)\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "      model.zero_grad()\n",
        "      output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_val += acc\n",
        "      total_loss_val += val_loss.item()\n",
        "  val_acc = total_acc_val/len(text_val_loader)\n",
        "  val_loss = total_loss_val/len(text_val_loader)\n",
        "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "  model.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Currently using device: cuda\n",
            "\n",
            "Model Architecture CNN_LSTM(\n",
            "  (emb): Embedding(2078, 300)\n",
            "  (lstm): LSTM(300, 128, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (lstm_fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (cnn_fc): Linear(in_features=1280, out_features=512, bias=True)\n",
            "  (combined_fc1): Linear(in_features=1280, out_features=256, bias=True)\n",
            "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (output_fc): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Training Started...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1390/1390 [01:48<00:00, 12.86it/s]\n",
            "100%|██████████| 337/337 [00:05<00:00, 65.67it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss: 0.6931 train_acc: 0.5003 | val_loss: 0.6931 val_acc: 0.5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1390/1390 [01:48<00:00, 12.86it/s]\n",
            "100%|██████████| 337/337 [00:05<00:00, 64.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train_loss: 0.6931 train_acc: 0.5003 | val_loss: 0.6931 val_acc: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1390/1390 [01:48<00:00, 12.83it/s]\n",
            "100%|██████████| 337/337 [00:05<00:00, 64.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss: 0.6931 train_acc: 0.5003 | val_loss: 0.6931 val_acc: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 1278/1390 [01:39<00:08, 12.84it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRkG-JWjfCj"
      },
      "source": [
        "# Ranker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOlqTLXknYN"
      },
      "source": [
        "def single_pair_inference(premise_image_path, hypothesis_image_path, premise_text, hypothesis_text, model, tokenizer, threshold, do_plot=False):\n",
        "  premise_sequence = tokenizer.texts_to_sequences([premise_text])\n",
        "  premise_sequence = pad_sequences(premise_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  hypothesis_sequence = tokenizer.texts_to_sequences([hypothesis_text])\n",
        "  hypothesis_sequence = pad_sequences(hypothesis_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  image_premise, image_hypothesis = preprocess_img(premise_image_path), preprocess_img(hypothesis_image_path)\n",
        "\n",
        "  if do_plot:\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    ax1 = fig.add_subplot(2,2,1)\n",
        "    ax1.imshow(image_premise)\n",
        "    ax2 = fig.add_subplot(2,2,2)\n",
        "    ax2.imshow(image_hypothesis)\n",
        "    print(\"Left item description ------ {}\".format(premise_text))\n",
        "    print(\"Right item description ------  {}\".format(hypothesis_text))\n",
        "\n",
        "\n",
        "  image_premise = np.reshape(image_premise, (1, 3, 75, 115))\n",
        "  image_hypothesis = np.reshape(image_hypothesis, (1, 3, 75, 115))\n",
        "\n",
        "  img_data = TensorDataset(torch.from_numpy(image_premise), torch.from_numpy(image_hypothesis))\n",
        "  text_data = TensorDataset(torch.from_numpy(premise_sequence), torch.from_numpy(hypothesis_sequence))\n",
        "  \n",
        "  text_loader = DataLoader(text_data, batch_size=1)\n",
        "  img_loader = DataLoader(img_data, batch_size=1)\n",
        "\n",
        "  for lstm, cnn in zip(text_loader, img_loader):\n",
        "    lstm_inp1, lstm_inp2 = lstm\n",
        "    cnn_inp1, cnn_inp2 = cnn\n",
        "    lstm_inp1, lstm_inp2 = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2 = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "\n",
        "  score = output.squeeze().cpu().detach().numpy().tolist()[0]\n",
        "  if score > threshold:\n",
        "    return \"Positive\", score\n",
        "  else:\n",
        "    return \"Negative\", 1 - score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "ft7vTrP2EEGa",
        "outputId": "4a808514-0841-4b43-abfe-1c9ab56e318b"
      },
      "source": [
        "random_index = random.randint(0, len(train_image_premise_id_list))\n",
        "image_id_1 = train_image_premise_id_list[random_index]\n",
        "image_id_2 = train_image_hypothesis_id_list[random_index]\n",
        "text_1 = train_premise_texts[random_index]\n",
        "text_2 = train_hypothesis_texts[random_index]\n",
        "prediction, confidence = single_pair_inference(premise_image_path=IMAGES_DIR + image_id_1 + \".jpg\",\n",
        "          hypothesis_image_path=IMAGES_DIR + image_id_2 + \".jpg\",\n",
        "          premise_text=text_1,\n",
        "          hypothesis_text=text_2,\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          threshold=0.4,\n",
        "          do_plot=True)\n",
        "print(\"Actual Label for this pair is\", \"Positive\" if y_train[random_index].tolist() == [1, 0] else \"Negative\")\n",
        "print(\"The prediction for this pair is\", prediction, \"with confidence\", confidence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left item description ------ Mirror Provided with safety film - reduces damage if glass is broken.\n",
            "Right item description ------  Children's picnic table View more product information For added durability and so you can enjoy the natural expression of the wood, the furniture has been pre-treated with a layer of semi-transparent wood stain.\n",
            "Actual Label for this pair is Negative\n",
            "The prediction for this pair is Positive with confidence 0.43064793944358826\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAGUCAYAAABwcEd+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxkZXn3/+uurffZe4YZZmAGBAQ1Ko7irpFocEnML1FjogkxGLJIEqPPSzE+avJEoyY+KJr8THCPMYooCoKKyOKCig4CyiKLCMwAsy/d093Vtd3PH93c13Wfqeqp6VpO9fTn/XrNi6uqzn3OqaqmzulT375u570XAAAAAN2VSXsHAAAAgMWIE3EAAAAgBZyIAwAAACngRBwAAABIASfiAAAAQAo4EQcAAABS0JETcefcWc65u5xz9zrnzu/ENgAAWIg4RgJ4lGt3H3HnXFZE7haRF4rINhH5iYj8gff+jrZuCACABYZjJAAr14F1Pk1E7vXe3yci4pz7goi8XEQafsisWrXKb9y4sQO7AqBbbrrppt3e+9G09wPocRwjgUXm/vvvl927d7t6j3XiRPxYEdlqbm8TkTPmGrBx40bZsmVLB3YFQLc45x5Iex+ABYBjJLDIbN68ueFjqf2xpnPuXOfcFufcll27dqW1GwAA9ByOkcDi0IkT8YdEZIO5vX72voj3/iLv/Wbv/ebRUb7NBgAsChwjAQSdOBH/iYic5Jzb5JwriMirReTyDmwHAICFhmMkgKDtGXHvfcU5d56IXCUiWRH5pPf+9nZvBwCAhYZjJACrE3+sKd77r4vI1zuxbgAAFjKOkQAexcyaAAAAQAo4EQcAAABSwIk4AAAAkAJOxAEAAIAUcCIOAAAApIATcQAAACAFnIgDAAAAKeBEHAAAAEgBJ+IAAABACjoysyZa472ve79zrtkV2EGNF2twvx1Ri5aK1+XMduy+NR7R/L7FQ1p8PQAAAHoQV8QBAACAFHAiDgAAAKSAaEoXNYpYiIg4E+KIYh5mjG8QBZl50JT2MTsmERTx9qbdTrRjc0RbGmxnzvHziJM0jKDMI+YCAADQK7giDgAAAKSAE3EAAAAgBZyIAwAAACkgI95FNuu8Yd2q6LFcVn8nqkkt1F5LyWTmyEH7rNlQ1T6gZS1+u23mvOZ1jMvouhrl1ZO3c7lc3THJfHfG1V8ul9VtZk0tIpLN6JhCX17rQiHU+bze39/fH423twcGBg5bJ2/b8cPDw6Hu6+truM1CQR87cGB/qP/ir87TbQwPCQAAWLy4Ig4AAACkgBNxAAAAIAVEU7rIRjlGR4+JHisWJ0NdrVbMIC3jaEoy8mHjJBlT63KZTBz5iPbNrM5GQ2o2G3PIILM3GbvNxkOciaZ4byM4urJkNMU5E6Ex909NT4d6slgM9YGD4/FuNugaOVc7yWq1Wvf+KM5jl0k86Vr0fPS1efFv/XaoH3vqqQ23DwAAjn5cEQcAAABSwIk4AAAAkAKiKV1kYyI/vfW26LF/euc7Qv2Zz34q1BkzxiZWkikTb0IbznQZseGLai0RM3Hmtomz+KrpepJpHN/ImMhF1iQzGs6EKSK1WlnH2/tN/KRSKUvExkFqVbOc1rl8ztxvXiiJYyY2WlI1yx3y2tjxFTvexGl84zhN2YxZvXp1qE8+5ZSG2wEAAIsLV8QBAACAFHAiDgAAAKSAaEqPeNu73hnqb3zzylA/+OADoS6VS6F2ia4pdkIdK2O7mWSS3Ui0zprxuazWdqKc5IRC3kwi5HI6gU3WbDObi3/Xs7EVu24nui67/dkHAxtNsZ1J7PO0kZmZ/fT2RiirVY2ZJJ+bjbPUTGylVLH3a10ux3GaiYMToR4b1444dj8BAMDixlkBAAAAkAJOxAEAAIAUEE3pEbmsiYCYyXmGRwZDXasNhdpJ3OUjkzExDzNpTjyhT+Pfu6LHGiyX7IaStxP/xEuGarqY6IBi1l2uaEzErjuX2L43j801CU+j/ZwPux1vOr3YfRZvuq4kOr3YCY7uf3Bry/sDAACOPlwRBwAAAFLAiTgAAACQAk7EAQAAgBSQEe9xUcZbCvpAJp490mVsxrx+LjyZnY7WbWbWrJnF5hrf6DG7/UKhEI1xJlfeKL+e/O3QN/l86i0jEue97fP00jhvbtc9XdRWhLmCyd+btobFTDEaXyrrGNv+kPaFAADgUZwVAAAAACngRBwAAABIAdGUHmRjJnbGScnZ+EYc+WgU37C1T0ZLbFtAU2dt3WBdh2zTrtuOz8azeWYbxUlsZGWO+Ea2QbRlrpaFjZazkZHkaLsPpWl9dGBgINTl6elQ56v6nokk4jDEUQAAQB2cIQAAAAAp4EQcAAAASAHRlB6UjeIkGnnI5UzMo5bo+GEjJA1iKsloSrbBY3ZM1tloSjzeN4h8uIyNtiSiKSYEksmaDigmcpJJ7Kd9plHMw6ZhzO+U2cR+2uUyif0J25hjxs7o9bD72dcX6kol0cWmDbN7AgCAoxtXxAEAAIAUcCIOAAAApIBoSo+IohENUg02FnHvfXdFj61dt6HumEaT5oiI5HK5uo/ZSXhc1pnl484g/f19dR+z683n4zF9Zt15s1yyu0r8HOrHYxo9N9sNRSR+bRtFUMrVanS7VC7r+qpa9w8MhbpYnAp18nnOFXUBAAAQ4Yo4AAAAkApOxAEAAIAUcCIOAAAApICMeI+weedsrv7skTY8/obz3hiNP+mUU3R8g9knkxlxu5zNhdvlbN7b1snxNiPdZ9r69ff3R2PyZjuNlkvmraMse4Msuc2Fl0ul6LFisRjqySnNdU8VJ0O9d9/+aMy+fXu13r0n1N+97vpQDw4OhrqS2KZ93W1enLaGAADgUVwRBwAAAFLAiTgAAACQAqIpPSiXNa0AM/V/Vzo4ORndfuUrXxnqRjNBztVSzz5mYx72/mqixZ9drlGd3KZ9zDWamTM5s6Zdd4OYh50lNGsiLyIigwMDoV61cmVT24zWbV7D73/nO+Z+fZ9stOdw6wYAABDhijgAAACQCk7EAQAAgBQQTelBNloSRxx0mVqlEo1p1KWjkljuSNn1JrumdEq7Z6VstL5GcZyksbGxUNuYSvQ+Jbq5EEYBAACHwxVxAAAAIAWciAMAAAApIJrSg7INoik28DCV6Jqy0M0njtJqhKXZ8fY9GBoaCnW5XA51smtK46ALAADADK6IAwAAACmY94m4c26Dc+4659wdzrnbnXN/O3v/Cufc1c65e2b/u7x9uwsAQO/jGAmgGa1cEa+IyJu996eJyNNF5A3OudNE5HwRucZ7f5KIXDN7GwCAxYRjJIDDmndG3Hv/iIg8MluPO+fuFJFjReTlIvL82cU+IyLXi8hbW9rLRSZjWuFFGfGs1sXpYjd3ad7akf1u1JqxW/tg2xQOD4+EulicMnX8fthkv82LkwUDFgeOkQCa0ZbzAufcRhF5sojcKCJrZj+ARES2i8iaBmPOdc5tcc5t2bVrVzt2AwCAnsMxEkAjLZ+IO+eGReTLIvJG7/2YfczPXFaseznSe3+R936z937z6Ohoq7sBAEDP4RgJYC4ttS90zuVl5gPmc977S2fv3uGcW+u9f8Q5t1ZEdra6k4tNvsEMljYiMd3D0ZQ0ZsZs53rnMjg4GGrbvjCfz0fLuQwhFGCx4xgJ4HBa6ZriROQTInKn9/4C89DlInL2bH22iFw2/90DAGDh4RgJoBmtXBF/loj8kYj83Dl3y+x9fy8i7xORLzrnzhGRB0TkVa3tIgAACw7HSACH1UrXlO9L3BzCOnO+64VI1nRN8aZjSFb0/unpUlf3SaT93UzmWnery7VzjH2eWRNB6evv17rYF42P5kO123eN/pcBcDThGAmgGQRZAQAAgBRwIg4AAACkoKWuKeiMbK7BhD5RxKK9nUnmo53dTHot2tIomtJv4ii+Wg31/n17m1ovAADAo7giDgAAAKSAE3EAAAAgBURTelAup5057G9KdpKYUrXSse23e0Kedm6nU2OaXW+uwWRLttONSKKjjKnpmQIAAB7FFXEAAAAgBZyIAwAAACngRBwAAABIARnxHtTXp7M0ugYzMU5PT7e8nW60DJxrNs5W1nsky7W67lqtFupGrQyT2fFu5ewBAMDCxRVxAAAAIAWciAMAAAApIJrSg3KmFV7U+s7UlUpVmtFrs1y2OhtnO5/PvJ5ntf7rns8XjnxdAABgUeOKOAAAAJACTsQBAACAFBBN6UGZTP3fj2yUolIqtXWb3eigMtdy9v5kZ5VuxFmajdPY2U3t/cn3zD4WrbuJrjEAAGBx4Io4AAAAkAJOxAEAAIAUEE3pQXZyGNega0q1VI7GtBq/aHVMOyewsRPoiMTPu53Pcz7dXays6W6TjKbY21niKADQEdWajQjq/U743MXCwBVxAAAAIAWciAMAAAAp4EQcAAAASAEZ8R6Uy9d/W5rNSncqC96OHHga+9bWmTVNHtFm2bPZ+Hda+17ZxDu/+QKAqjW4P5OcxNhVQvn5r90Q6lvvfSjUo0uGQn3qyZui4Wee8bhQZ/vM3/CYPXAuPibUvB6LM06Xq5lP8ozYMfFOV8wpVs6b8U7Hu8RxqNFxPtnWF0cPzgsAAACAFHAiDgAAAKSAaEoPyuXyh12mWom/Autka8JurLedM3O2e7yNoNjlqlV9DyqVigAAjoyNdlTNx/A/fvLKaLnipH7GuorOLF2taWRjx4GpUO//+b3R+O/efHuox/cfDHXNm2NponXu9LRuc6CvL9Sbn3RKqF/wrCeHesXSgWj8YM4cO8zZVjQrcyJxQhxl8eGKOAAAAJACTsQBAACAFBBN6UE5M2NjxP7VtW/0t+aNtdpppRdm5uxUB5W5lrN1yXwl6ux7UGv8fvDbLoDFoGo/B23EInFIe3iXRkg+fdn3Q71994FQT01NRWNcRtedt7Nmev1MXrViVJcvxKc3fVnTwWS51gcPakylUo2PD8N9ZvbkvEZG7966O9T3f+naUE9MTETja+XpUE9Ojun9FdNxKxlFzejtY9etCfXpTzgx1Js2rImGbFi9Uvd5QPe56vV1yojt+pKIvJibjY5XyZlKa/YtsMs1OMQeErNpcCxejHEczhEAAACAFHAiDgAAAKSAaEoPypmvwKKvaUxZLsddOhZLtKSdyzUbYWlmfLXWuIsNfwUPYDHImhYgV9+kXUuuvOon0XLR52NNrwcODmq9dtW6aExfvx4Xx0wEZGJiPNQHTZylNpmYKMfETvbu3Rvqcrkc6qHB/mhMJq8xj8HcEl2XuYQ5MDKszyURKx3bpxEWl9GuK4P9eurlsoVojJjJhnbt1TjL1Tdo15dC5s5oSLWi8ZqiidoML1mm+2m2OZ3oujZZ0fdtqXk+a0aXhvrUTRuiMSefvDHUxw6beI15bbyZBCmZps1k9LXqVAe3hYIr4gAAAEAKOBEHAAAAUkA0pQfl8/q22N+UMvYvoOeIOLQ72jHf5TuxL52a0GcudkIf12BCn+RflNsICnEUAEery6+7KdR3bdNYxIG9GhnxtfgzsFQphrpa0WhItabHvlo1zjLYriX7TLTERes2k6wlPusHBnWynSc84fGhzhc0GjJdLkVjxsY0GmInbXOmDcyEiYLs2aVRFBGRPTt3hdpOHLR0QJ9nReJoysDQUKhXjq4KdaFPIyPjkweiMZWyicRk9axhemrC1LpI1WlMRiSO4EwX9XV7YNtOre9/KBrztW99L9R50yolZ6I2Y9P63k5NTkbjc1l9P5YMaSQot0Tf501rVkdjNh2ncaXjN6wN9egSfT0HCjq+r5C81tygI13KuCIOAAAApIATcQAAACAFnIgDAAAAKSAj3oNyufrtC72Zv8pL59r9dDJv3amZMdu9L43aD9pWV820OASAo5ltdzc9rZnoHWP7Qj08OByNGR3WtnpZc4ybLmqQOZeI81ZLmrFeNqzrK5nsdsm05cslZju2q3vooa2htsfYYnFarGyjFnuu/t9r9Q9qvltE5OTHasa7b0Bz0Fmn+zk+HbcSrJR0H6bL+tjByf2hrtbivHW2Zmfq1NO6YiLzHuTiNo0DWT3nqNr2g+Yp+3L8etZMO8iy1/egWtF1ZTO6L0NLl0fjnTN/C5czmfWyrveX2+LM/f0P6PtWnNbXqVrSn5uRIc2ef+c7347Gf/fKi6UXcUUcAAAASAEn4gAAAEAKiKb0oGz28F+HJXUqJpFGy8JW978d+2xv2/aF1Uo8o2m4v1auez8AHM2et/mUuvVcMwoff/rv6I3pR0K5fPUxoV6z7oRozIrR40JdMi0La+aztzylrQRXjeq6RERGl+kskTbZOWTaBQ570+NPRKYO6G37fCbMzJ4up9czsz4+PhT69BTL1zRKcWBSx5eKcTRlelrjJNMlbfM4YNoXDgzFUZ++fm0ZmO/TerCgkY9KVfdt/7i2ZRQRqWYG9TmIvp5ZrxGWiovjMMWiPgfJ6Danprbrvpj4yZJVo9H4nbs1xjRoWktmzf4XXCJaY2IrE+Y1XD6i50w/vuVnob7+8s/LQsAVcQAAACAFnIgDAAAAKSCa0oMKZgaxRmz3jvnqRgSkHR1YjnQ7yeWjzjPziMPE69aYSq1Wf5bN5O25vqIFgKPRXJ91m07WCMuBbRr/6DMzNO7Zel80Zvc27Zjxg+u0+0VfRrfz1LP+NNQP3HlLNH5geCTU57/tLaH+1re/o9vcFccvpssaDak5/ewfHtIohzPHhJUrtUuKiEgmo51OBgc0AuPMNdCq2YaISNl0Osmb7ib5gsYvMi7uYJI3M2NWTLRl+wGNoNhjV6YWt6SZKOt+mkk/ZcrMejo8HHeEOWWTRocOTOp7WDbpnJLpQnNgv86GKiKy7rj1obbxTzEzkFYm46hPLa+xldt/ph1R3vgX54T6Pef/RahdZmEcb7kiDgAAAKSAE3EAAAAgBURTelCuQTTFfskyV5ePZu5vdrl2T+7TqQl9ml3ffPazUTSlWq3fQSWJOAqAxWauz9rpce2YUatqTGLaRCZ+ct2l0Zhc9vCfo4953FNC/ctbbogfrGhM4gknrAz17/zTG0KdycTXJmu2e5apMyZC88BD+lzOe+u/RuNLRY269C1bE+qhQY1YZHMD0Zg+c/zP5vQ5jyzV5fKJaMrk2Lhu03b5qunzsZMAViWOtvabM8FqTY9r9jWvVuPXf/uunaF25lSyOKmvc9m8ZjkXn26O7dU4TDarcZR8nz5/7+MJlvbs3RHq66/Ujii5zMK+pryw9x4AAABYoDgRBwAAAFLAiTgAAACQAjLiPahQKBx2majdj3QnO93uGSvnu4753j/f7Vv2dbd1O54nABwt7N/G2JkoRURyNuM8qG3xqqbdXS7usNeUkml9l8nG1xkrZlbGc970nlB/50sfNkslrk2a55A1GWtn/mJr49oVof7iJ94bDT8wrtntrb/aFuoznvp4s43EscJktCte89L7JrRF4HduuDkacvHXfxjqsnlply9fHupi0bRJTPx901C/bmftGs3P17w+z6lynNcuFfWx4YLJnJuZRjMVXcbl4oz58uVLdIjom3399VeEev++/dGY4TUnhrrsdTv2RHYhtgvmijgAAACQAk7EAQAAgBQQTelBNpoSfbViWvRk5mjl1K1Wgs0s1wsza7a6TVtXa/rVp4t+jY3XZdtgLcSvygCgFbbd37s+8sXosSmvUYSs0+PdmrUjZqkj/6x8/ctfEOr33fPz6LHStM4yOTGmkQfvdF+SW4w+4l3965Yuq/s/lIjTDPVrzGPd6EppSkZPy+wJ2uiS4VC/4sXPiYbY2yVzKMo7nWWzUrMzc8bPZWpKl/vil78V6qt/sCXU05X4yfWPLA31jkl9Pfv7dDbQkWFtuegzcVvm3QceCvWvHnow1K/6/T8M9aXX/iQaUz5wINS3/0JnXt38OJ3lcyEeY1u+Iu6cyzrnbnbOXTF7e5Nz7kbn3L3OuYudc4cPPAMAcBTiGAlgLu2IpvytiNxpbr9fRD7ovX+MiOwTkXPasA0AABYijpEAGmopmuKcWy8iLxWR94jIm9zMdwIvEJFHv1v4jIj8g4h8tJXtLAY2vlBoMLOm/a2pUon/6rnd0YwjXb6THUw6NRvnvPbNzKxZqWpMpWYiKwAgwjHyUfc9Mh7dto1CKmXtuHHB+9/X0nZe9OzTQv2+98XHyHxBj6uTUxpTqZl9yfRwqmGuiGN0/mAeqomZwTNjZgZNhHCGB/W1+dPX/pbWf/Qys424U9sZL3xtqB/61S9CvfJY7Wxy3p+/OtSfvfR70fg/f/ULQ33h+94S6gnTgOU7W+6Oxjy8d0+oP/2FK0O9+Z/+WhayVq+If0hE3iIij75DK0Vkv/f+0f8DtonIsS1uAwCAhYhjJIA5zftE3Dn3MhHZ6b2/aZ7jz3XObXHObdm1a9d8dwMAgJ7DMRJAM1qJpjxLRH7bOfcSEekXkSUicqGILHPO5WZ/418vIg/VG+y9v0hELhIR2bx586Kf/cR+1dSoa4qtk18TNaMXOqC0OqZT+9Z0B5UGE/q4xFd9dm0L8a+4AbRsUR8ja+ZjL5uP/x41m9UoRM1MYHPKCetb26j5rD6YeMWyYj6vs9rNoyY2Vti7jeTmOo40eiy+0jrXccgsGS3m6i8jIoMj2uGmarrgrFg9GuqvXvX9UL/xXI2piIj83m8+I9T2WDps3oId27dHY3Lm5+jOux+Qo8W8r4h779/mvV/vvd8oIq8WkWu9968RketE5BWzi50tIpe1vJcAACwgHCMBNKMTE/q8VWb+KOVemcnDfaID2wAAYCHiGAkgaMv3MN7760Xk+tn6PhF5WjvWu5hEf/VcqN9Wtmq6dBwShWhjZ5Bejol0anyzY+xXaPb9sB1UREQyxFEAzFqMx0jbGWX5YHzNb6up4wl1bOQyMTtOE7I5jbwcs2ZT9NjOh7SDpDfRmG0P7w71CcceE42pNTgsOBM+jCOjjffNmxfEzbHcQoky7h8vhnpwaHmoCyY+UivoJEZn/fpTG66r0QR4A9n4Z2C8OBnqbN/gEe5x72KKewAAACAFnIgDAAAAKeBEHAAAAEhB7/bqWWRsLszOANZomVoijNbOmTXTnvGy2eVSyYib5WxG3Neaez8WSv4PAFrhTFvAicnp6LGqmU3TziSd/NunVhyYPBDdtpMf16YPhvoPz/n7UH/lfy6MxnzqMp0NcjCveeWVSzSffNL61aE+5cTjo/EjI5qXzkQth227wIVxTEgeI2/+1idD/fcf/Eqof3SjvmYFE4bvzx955n/58cdFt6fuvifUGSknF1+wuCIOAAAApIATcQAAACAFRFN6hP3aJ29aMEUza5rl89kj/5pnPjNeNju+1ThI3AKqvS0XW55N09S1Wrnu/cn2hcysCWBRq+kxaveBHdFDmaxeA5wcL+kDrn3XBv/8NS+Pbl/44f8w2zf74nVmz/f/x6XRmEpJ4yRDI32h3rNvLNQ//+W2UO/44jei8bWyHhdGhnUmyqHhoVCPHdgfjdlvXo/RZToD6AufuznUZ71gczTGNjx2rmJu1T9PKNfiY1K+0ctu4zSZ5Dydevue+x8xy+k2h4c0wtPsOxud82TjU1R72uN8/ZaHC/F4yxVxAAAAIAWciAMAAAApIJrSI5rpmmL/ujr57Us7u6Z0Y71Hsp1Wu7M0u51Gom41Vf2qzs6ymVyXfXsW+tdmAHCkdu3VriWZal/0WNV8djoXx/ra5bW/89zo9kf/49Ohni5NhXrsoEZD9u3aGY3J5HTf9uzWyEgur5GRWk7zEslj91CfPu9CVp9zaXoi1MtGhqIxq9eMhvrA/vFQX/y1q7W+9GvRmEJOt9Pfp6d1RW/qKV1XxsWRlTOedFKon/8snfT1uHU6M+aSkSXRGO/1tekb0QhKbq8GZQ7ui2M3R2pq5/b4jopGQ4vRLKwLG1fEAQAAgBRwIg4AAACkgGhKD8pm6v+lc8bEGqrV+GuZNCa36dSEPq12Z+nWJEI2mlKrVRsuBwCLQdV87v3fT18Z6lJtIlrOm24ca9et6si+uMR1RpvGyGT01Gd6Wicbuv7qr0djMnntdJLPa/xjeFijKQUTX1m/fn00PrdMxxdNN7SJCd1mvi+OfGTze0M9OamvW3lat1MYjKM+/YMab8n362NLB3SbhYG1oe7LxhGavRMa+bjwY18O9di4RktGli2PxoyM6HOrVHV9tiNOcUr3/1vfvSka//xnPFn3LVc/ZvKWP3tVdPsd//yRUNshX//OllCf9byn6L64ZKeX3sQVcQAAACAFnIgDAAAAKeBEHAAAAEgBGfEelMvXb18Yt75rrsVfOzPR7R7fS9tsdrlGLQsrlUq0HG0KASw29srezfc+GOrqeDFarpDXFncfeNdbOrIvyY/wviHNOO/fp20Kczn9rH7MyY+Jxux6eGuoyzV9DsUpbX9YMjNO3nvvL6PxZdO2cWhQT7fyg0t1mckHozGZIT3+L12q+fHBnI7v94VozL7d2ubv4LjO+rlypebvh4aHQ71zt+bQRUSGzWPDS3TfTjhRc9yT0/F7ODGh+e+x3Tq76OCy1aEum9f545/TvxkQEbnymu+Fuq/QH+qlI7rPpWopGlMxM4VWvT7275/UXPuZz9FZRwuZI29RnAauiAMAAAAp4EQcAAAASAHRlB5hYw558xVU/JWJmeGx1p0Wfc2ut1Mxk2bX0a02idVq1dSmfWGl8SxfaX/tBQDd4M1n3bBpUXigELfkrRX1c/Qpp57QmX1JxDdHl5s2gUWdMXLpqLb1229mshQRGRjQCMhaM37fhEZTPvHB94T68aeemNgLPZZXzTE7aw4JlVp87Ni572Cor/r290P9jW9rlOPWW34WjRlavibUflr37ZFdv9JtmgjOimXLovFObGtkbWU4MX7ALBW/h/mcPp+caDSzXNXWjEvNrKHr1qyIxi9dqvuQK5hZw7P6mq0ciGcdrZq2k1LRaEq5pPty7pv/MdT9A3FryGXmeU+WzMygXvf/Kb92UjTm9Cc+LtSnHH+M7memfcd1rogDAAAAKeBEHAAAAEgB0ZQeYeMLedM1JWP+ItsuU6s1nlmzl6Mp85lxMu1oin3dK1X9CqsyRxylUrwYjJEAACAASURBVCk3fAwAjkrmI7U4PRnqWjX+rC30a5cM7zUi4Fo8JbGf2xde9F/RY3v37wr1yHKNGEyX9DPdJbp0DC7RCMvfnPf6UL/k+c/UhZwtE8cUc+zImDxKzcR2crn4euixqzQ+8ad/8LJQv+7VLzVLxdupel131um6N5/5ylAf3KtdT6YPamcVEZE9/RoBmarp/qwc0W4qpal98TZrGtsZPW6D7llJj339fbovY3vj8VNTGsHJZ7W7TMW8ByUTsxERGTMRlMGK/txMjOnzeeZTz9D19sfRFjvT5rB540pmXT+5Pe58c833fhTqP3vN74b66aefJu3CFXEAAAAgBZyIAwAAACkgmtIj7Fdq0YQ+UeREv+ZJduLoxuQ47eiAYve73et+VDK208q6DhlTq7/PNfP1qohIJqN/YW73ht98ARytqqb7hD1C5bLxBDS1momAZJo7Damaz9usOY48uGN3qN934adCfdvPb4/G9w0OhLpc1tjMnt0aWckmYiLfv+K/Q+1c1tSNOmY07qQRrdnNcSRosIpGHdRERExDlCimcuO1l9Zd1w0//Gl0+3lP14l77KqjTi+JLiE79+lreM75H6i7/dt/cmOoj9u0KRrf3zcY6tKUxmaq5mdlsBD/3KwY1cl+Dm7Tji5TJTPZkIkAFSds1xeR8rR2dMnkNA4zMaHPZf+B7dGYU0/RCMoZbYyjWJwXAAAAACngRBwAAABIASfiAAAAQArIiPegXP7wb0syRtaNjPh81j2f9oXdmiWz2THZrMl7m5x+paJ5yFo1kUu3q7bbYZZNAEepjPmoGzct8rzPR8tVTLu4+G+fzLXBxOzRB6d1zHlv+T+hvvM2nWVy5eixui+F+Dg6tndPqLNmMwN9JoeciTPJ4s2CC+Sj287a2egI9+ynnx7d9s606zX32/bJtcTx8v+/+Ft6I6fvb9a8Znf88Gu63kQu3pu/nooOl+b68NOe//9FYx7z9LNCPWF3NKs/G1/9whdCvWzFsWJlzQyvoys1b17r1z24+MPvjcZkzPE/egna+PPAFXEAAAAgBZyIAwAAACkgmtIjbGuibIOZNa3kV06diny0Or4XoinzWa6Rsomg2HVVk9EU87VVhjgKgEXgZ3c8EOpCQY9j06W4vWtxQmMr9uOxYj5Tz3/3hdGYH/5AW+FVyjp74/CSZWYpjQse2BW3obvkvz8a6te/6V9DXS3p7I0Z035RRKRijr+FBfMxbmbjbrBE9pAH6i+ZaJIc3dq2fULXZ9r1Zkw7Su/qzww+s27TDtLcb9tUXnHJx6Mx//vDnw314KDOmrl0qdaTYyOhXrJseTR+544dof6bP/+jUD/jKdqWMJNtfH26U4dyrogDAAAAKeBEHAAAAEgB0ZQe1JfL173ffrUz1+yRvdRBJY2YSSf3M2Nn06zVj6kkl7OPNZ6RDQC6L4rYmfuzdploRHzssR9pX7jqe6GulHUWw7HxCTtEluV1Zs0LPnZJqB+4/6FQ33HP3fFWzYyNdpbMjJnx8XEnrA31Ny/+j2i87dJRNs+uUDLdrxJNU17/5n8I9X9d8A+yqCWOXXv26IykOTPraCarr/PcsUzbOUevCdvYzLU3/Dga8fMffTfUq457YqhrZf35yg+sDPULnvWkaPxfv+6Voe6lYzFXxAEAAIAUcCIOAAAApIBoSo9oNIFMs1+f+AZRiHaOabXrSrPbTHsSormWq1T167S5oimtdmcBgK4wn1VfufamUP/o1vtDnatOhnrsYDEaPlXRx7ymUaTqNGKZq8VdU7JLND5w8SVfDfXAgGZDyqVyNCaT18fyfXoN8duXfEy3bztxJA+dNR1z/nl/Eup/ee//1f10cTZl5yN7BTNcoodKaXx3qAtLtDtJcTruPNOIjaNUzbH03z6mnVGu//Ed8TbLfTre/Hz15fV9HzMdef7yT14Rje+lOIrFFXEAAAAgBZyIAwAAACkgmtIj7FcmLm8a4jfouNFsFGKuKEgzX9P0QqeUI43NtGNyn0br86ZhgI2mVGvx13He/EV4r34dBuDoVTO9TmwXp2kffx792ZveH2r7dX/W6Wda3kxsUyzGHVD68jqZypiJkzztjDNCPTgwEo3ZP67xgcnJSfOIiWhKHGcpme1uPEbXd9/WPaE+fsOorqmWiLZ4jS+89HmPC/W/fEBfj6qPt+lsvxjz4e9NTGOxfL6Xa/HxcuqAvod9y1aEupAz77UdkniZiqZbzTeuvjbUP71rqw7P9kdjcn0aHdqz++FQr1w+bGqNyTi3MK41L4y9BAAAAI4ynIgDAAAAKeBEHAAAAEgBGfEelMvVf1syJqdna5HWs9ftzFgfbRlxq1HLQn/IRKfMrAkgPeWy5p3f8aEvhNpnstFyo2uPDfWBfftCPdg3GOq+gmbHy7I6Gj9lPt+WTk6FeusD28x6x+Mxk3q7kNNj2UCfHvsqic/U9RvXh7qY0RzwZ6/aEurdu/eHes8+zY6LiIzv00zzwd0PhHr1sceFevpgnH+fLOp+1uwMy5nFdw3TJeZX9abNsm25PNCvb1ytqjnwy666Phq/fdfOUH/31gd1XfbvFLLxD8G69fqzt2vMtD+s1v+brJwkj+O9efxdfD9NAAAAQA/gRBwAAABIAdGUHmS/5rHmal/YjHbGP5p9rN1jGi3XrefWqLaRFRGRrH2vbGvKw+4JALTOmfjE/rJ+8gz0xy3hJKOPrVx3vI532v5vsqhTZubK8WddvloKdSWv2yyY2Q5HlsTbHFqqtweHtPXcwIC2Qkzu53RRYy/33afRktKUtj8smmjM+EGNooiIDJiozYYTHh/q4084IdS5fHzstfGWc97x8VBPHjyo26/oNk/eqDEfEZGXPO/poX7m6SeHOo62xEeFjPP1HzH31xLXULtxRdUlWjsOmpaF9tzkhc/ZHOpz3/ruUE+X4ha/2aGlWts2g+Z5vvv8v4jGPHbjmlC/9K90RtTCpL5P5ap51VwyM1r/3CptXBEHAAAAUsCJOAAAAJACoik9qFE0xZprZs1e7WbSyW12coyt7V9n2zoZTTny4BAAtE/NfD7921tfE2qXiSMG9mhz90P6Ff8//uvHQr3ptCeGOtc3EI2vFDWakjHHrqEJ7Thy0EQ5RERKJupi4xfZjH5yTkzE0ZKSiZ3Y7irZjEZOyk63f9pjTo3GDwzqrIy7du4I9a033xzqfDY+JfIV3U8bzyn0aWePYzZp15X+1eui8Vf99K5Qf+GbN4R6YqIY6lpi9shMRV+rE9euDPXZv/+yUK9fuzweY17ETnXp2rU7fg8HhjVSZI9/P7/99lDvL+q13r4BXV5EpFrSn8N8vi/Un3rveaHOZBqfog7kzHE5b7r62MlQEwfiXm1axhVxAAAAIAWciAMAAAApIJrSgxpN6OPNXy3XEn/BbL8aatRdpdcm6unUursVbbGv+SHRlHl0tQGAduk3k/DEGkcfTzGRh0ceuCfUt/74u6F25fj7/e3bdTKW1ceeqNsf1MjIuuNOjMasPU7jHNn8kBmjMY1CLt7OwLjGOfpMNKRU1sjKypp+7h6cjCfnKU5rtKJgogzZVatCXSuVozHZ/LJQHzOk+zl2UDu1TJjExn13bI3Gl0u6zzaDk8/YrlrxNpct184guZHRUF90mUZb9u7cFW/HnA6UymZ9Xrf/4mdqB5cXPPux0fhjVoyYMWa95jj2n5dcHY2x72+ppLGd2267P9Rr1+kEPKUJfZ9ERAoFPc/5xHv/Wh+YI45i/d5vPCPUF1+u+5bJ6LH4l9t2R2Mes2GN9CKuiAMAAAApaOlE3Dm3zDn3JefcL5xzdzrnnuGcW+Gcu9o5d8/sf5cffk0AABxdOEYCOJxWr4hfKCLf9N4/VkSeKCJ3isj5InKN9/4kEblm9jYAAIsNx0gAc5p3Rtw5t1REnisifyIi4r0viUjJOfdyEXn+7GKfEZHrReStrezkYmAzxXbWrUZZ47m68DQzE2Sz+9LM/e0e0+q6O7mfjdoXJsfXqnHuD8DisiCPkSafO7zEtJuraRu94lTRjpDHP/Glod62W7PLB8c1R33XnTdHYy7/9LvNLT2a2VklfTX+u5v7t+8L9Y9v+UWob7j1vlBPlnX2xhUm+y0i4kU/rzNOn2eupu0XCwNxa8ZSRccUJ/T5DA/qMbpoAtp5F+fvs+ZYnslpLr1S0/vzieuh+w9qlvpA8WFdzrRWnJ4oRWN8TY83y1cfo/UKzbXft0fbQf70v78djZ8w71W5pu/HcvNyTNXi08X+gu63bSF50GT2D45rdvwv/1jbL4qIPOeJp4S6ZjLzzV4dfuVvPjXUl3z9ulDbvxn4zCVXRmP+6U1/2uTau6uVK+KbRGSXiHzKOXezc+7jzrkhEVnjvX9kdpntIlI3He+cO9c5t8U5t2XXrl31FgEAYKHiGAngsFo5Ec+JyOki8lHv/ZNFZEISX7H5mcuEdS81eu8v8t5v9t5vHh0drbcIAAALFcdIAIfVSvvCbSKyzXt/4+ztL8nMh8wO59xa7/0jzrm1IrKz1Z1cDGzLQTuzZqNWhJVK4/aFzTpaZ8acT+Sl2TGVin71aV/zQ8Y7+zuufaxHp/YC0G4L7hjpzcfTZz6k8RGX11OFFcNDdoi8+Oy3h7pmpjUcGFwS6n/9P3EMPusaXQPUHXDZeJkTjh2tW7/6pc+xzyBU1cQx8bSnnKU38tom8Umn66yhI6PHR2NWrdEvK/rsBJhm/FBWt7k08dqUzPEin9eZPQ/s11lHy+VEjDGjx/+MieeUyhoJmq7FY3J9GiMqV3V/9u/RmMbYlM6aOpCI4DgTR5nYry3/Nhyj8ZGTTctJEZH77tNIUEY0drN+46ZQv/4lTwj1pnXtbR2YM9GY7bs1trRiRN+bn/z0tmhMp2YdbdW8r4h777eLyFbn3KPv1JkicoeIXC4iZ8/ed7aIXNbSHgIAsMBwjATQjFYn9PlrEfmcc64gIveJyOtk5uT+i865c0TkARF5VYvbAABgIeIYCWBOLZ2Ie+9vEZHNdR46s5X1Lnb2K5NGX5+0I4qSRjSl1fHNjGn3bJ4NZ9CcY5uOCAqw6C20Y6Qzn2NLV+iskjnz8ZY88lTNLJX2YzCT6wv1czc/rm37ODfzuZuJv/D/xU3fCnXVLPaef/5QqC+95tp4bWXtJlKtaKeSW75/Rai//qM7Q33jTXdE4ytOYxJF00lraES7jFTLieOy6e4yMKJRl6lxncJz1eqV0ZhiWd+VqonDZGpa92fNzNzTceebfhMjWprTfXtkty63ffdd0ZglS3S5KTOz5o1XXBzq//3Hn5VOKXs9fS2ZTj7ZYX3Nc/2D0RjpoTiKxcyaAAAAQAo4EQcAAABS0GpGHClJfsNyNMdEjnSbnRxjoynVRpEVEfH1O5IBQM9ypptJdHJgEx+Jz8Z81XSSMo8VzAQ0ffnuRwKyyXiguexon9u73v53ob7yO38cDZmqmE5l5mm//d0XhPq97/xfoX7ZM+MITtW8HjkTlal5jYncdPuD0Zhrbron1Dv2ahxl2crloU4ELkQmdLKevqUmZjI8Empf1nUdrMTXYG0XG3vs27d7b6j3jE9FY27b8rNQjyzTCM0Pr/iULtSwO07rCubt3bBpY6gzFd3P4pRGi3oZV8QBAACAFHAiDgAAAKSAaEoPshP6NJLsptLODijtjpN0Y5udjKbEHVTMGDP7Ra0WT7DEJD4AjkY1F39u7t/7sLml1/ZcxnRTcfGpRq9+Ir74Rc+Nbn/pK18N9Steqo1u/ukd/0uakWvQpSPj9Bj/1Mdvih6zt6NX2hyHTtr88mjMKtPhZvNv/GaoR5ZoTGX3tHaxydUSkQ0zidDkgQOhfszG9Xr/rT+JhixfqXGU0qBGYMQVpBtss5lK/4pQF8buD/Uh51J2IsTc4c+zuoUr4gAAAEAKOBEHAAAAUsCJOAAAAJACMuI9zraDyojNJHduxsv5aGcOu93baXWbjWbWtO/BfGYtBYCFplSJb1fLOmNkJqs55OgQ1cE2du30zr/7k+j2O978p6F2KVy3jBLmJm/+i598NV7OvL7//KGLQn3H968K9cPbd4S6lJjNc2xCM+PPee7zQr116/2h3rn7gB0ipbzmz19y+ikNnkHn5Mwcr/ni7lBPTWkOPJs4w/WmhWQv/Z3Cwvi/AwAAADjKcCIOAAAApIBoSg+yrQnt1yc+apeXmMmxC7Ncdmv2yvmsu1vPzXt93SumFdIhw4mmADgK3Xn/rui2N61bq6bh3rQ5RDmJj1e9eg0wm8wy9Cg7S2fSO/7u3FBv3a7v1atf96ZQbzzpMdGYvb+4M9T//GadXfR7N94c6tvv3prYBz0fOXPzac3sdltlzGtwwZv+KNR//p6Phro6NRGNuW7LbaE+82lP6ODeHZne/L8BAAAAOMpxIg4AAACkYGF8DwMRaT0aQjSl9f2s+eRXrLP3J6NCTe0NACwsF33xm9HtstfrecODOttiLjutCzWYYRLtZ49X61aPhrpS1XY3p54Uz+b5uQ+9U8ebt+rzX73WLBVft7XRkBPWr5A0bdq0NtTVku5XrVyMlvuXj3wq1Gd+9oLO71iTuCIOAAAApIATcQAAACAFRFN6nP2ayXZTaXYCmVajIa3GP1qNjMz1WCpRmwaT+MzZNcXWfEULYAGrmEl7RET6B5aG2vtCqN/+16/S+xPTp/Ap2Dn2PCFjQpI/uup/6i5zCHO82nLHfaHesPGEaLGcWa5q4km5FN5cZ5KhxapGorKV6Wi58fGy9CKuiAMAAAAp4EQcAAAASAEn4gAAAEAKyIgvUDUzm5lI+hnxXmqN2O5cum1N2Oy6bWsncuEAFjL7uTe2f2/0WM6Zz7qMZnBf8Oynhdol2746rgF2w5xZ8CbGbDjxpFBnE8vVzM9Exoa0U7i+67N6PpStaJvGUqkULZcpFKQX8X8DAAAAkAJOxAEAAIAUEE1ZQKJYxBwJiU7OcnmkOjkbZzsiKEe6zcb7EkeFiKMAOFrYuEK2OBY9lnHm8zHT4NoeUZQFo2Rir0MFfd+mpuNjXKWks1b6tE8lq7r9XH4w1CWfjxa7/suf6NouHQn+7wAAAABSwIk4AAAAkAKiKT2u2Zk1bWePZtbV7GPt7obSqZhIJ7fZ6LWNxyRmjiOZAuAoYT8D9+98JH7QfNj1FeJZN7HwZE2MqFa1XUfivikF04Ek2/hQ2BUuqz+fz/q11aF+5//6h2i5rKTb3aWR3tkTAAAAYBHhRBwAAABIAdGUBcRGITKJv05vJsKyEGMmze5PJycUms82vbffz6X8vR0AtGBqWr/SL9Xizz2XNVGGnMYX5jOZDNKXMfGNyYlpvb9/KF4ur9GUqvmRyKbytuvP4Lvfel4aO9ASrogDAAAAKeBEHAAAAEgB0ZQe1yhykmzL0c5JfOYzuU+7ox3NrLuT4xvtZzM1ABxNrr/xllBPF6eix2o1jaNk8uWu7RM6RY9lgyPDoZ4qxe/tYx97Yqizmd7sRrJQ8IoBAAAAKeBEHAAAAEgBJ+IAAABACsiI97iGmeTEbI/zaVl4pNnpVjPZza477ez4XPtTrdVfJjn7phM7PnoAABaUa269L9TZXCF6zJnPvpOPX9O1fUJn3H7fjlBPTGn7wuRl23N+6xnS8EEcEV49AAAAIAWciAMAAAApIJqygCTjD1Yz0Y5uzYzZyZaJrbYMnM/27eve6C3wEo93LmNq8igAFq6qN7NnVqvRY6Wyxhde/btndW2f0BmXfOP6UGdy+VBXavF123XL45k2MX9cEQcAAABSwIk4AAAAkAKiKT3ORiYymcZxh/lEU5p5rBfGHOly7d5+o0hQ1DUlGfuRI4/NAEAv2rP9wVBPjY9Fj01PFUP93Kec1rV9QmcUZSDU+bxGU/LZfLSc90Qu24Ur4gAAAEAKOBEHAAAAUkA0pcfZgIONSJTL8V+udyqa0ur4Vrc5n3W3e5txXa17f/JLOsfMPQAWMPv5VpnW+i/Ofnm03Dmv/f2u7RM6z+cGQ53L6CliWSrxghmOce3CFXEAAAAgBZyIAwAAACngRBwAAABIARnxBcRm9rLZTMPHyIi33grRPlat1s+Fz2dmTwBYCGyL3M9f8HehLlUaz/CMhW/N8pFQ329+BsqT09FyWXv4Iy7eEq6IAwAAACngRBwAAABIAdGUHtdoVsdqg/uT2hknaXfko53r7uT4xq0MvV0oHuQaPMRXeAAWGOf0ml1fnut3R5uS6PlEcWoi1HEctq+r+7SYtPR/lHPu75xztzvnbnPOfd451++c2+Scu9E5d69z7mLnXKFdOwsAwELBMRLA4cz7RNw5d6yI/I2IbPbeP15EsiLyahF5v4h80Hv/GBHZJyLntGNHAQBYKDhGAmhGq9GUnIgMOOfKIjIoIo+IyAtE5A9nH/+MiPyDiHy0xe1A2hCfaHJ9vRxNaTQm7WhKcjwJFADCMRILQNbMGL1/fKzuMrnh/uh21ekxL8cRryXzviLuvX9IRD4gIg/KzIfLARG5SUT2e+8fnQt1m4gc2+pOAgCwkHCMBNCMVqIpy0Xk5SKySUTWiciQiJx1BOPPdc5tcc5t2bVr13x3AwCAnsMxEkAzWomm/IaI/Mp7v0tExDl3qYg8S0SWOedys7/xrxeRh+oN9t5fJCIXiYhs3ryZmVEaqNbKoXaSDXUySTGfCX1anZBmPpMAdSqaMp91zSeOUvN2ch9dPtndhh9oYNHjGIkFwXu9Jjs1pRP32MnsNo4OR2OyZgzJlNa00jXlQRF5unNu0M1MwXWmiNwhIteJyCtmlzlbRC5rbRcBAFhwOEYCOKxWMuI3isiXROSnIvLz2XVdJCJvFZE3OefuFZGVIvKJNuwnAAALBsdIAM1oqWuK9/5dIvKuxN33icjTWlkvAAALHcdIAIfDzJo9buYbzRm1WsXcHy83n4x4s481WmY+Y5rRjoz3kS4313Oz+e/4dW48nsgcAGAh2HtgKtQTEzqzZjarf5f2N3/4W9GYSkbPR/KcSraEuWoBAACAFHAiDgAAAKSA7xN6nK9qLMLGHXwtDj+0Gk1pZrlWoy3zXfeRLtfumT0bv7aN2zfZ5ZIxIgAAesUNP7071N7rAatYLoZ645pVXd2nxYQr4gAAAEAKOBEHAAAAUkA0pefV79JxyFJNRFPmGtPM/e0Y342ZNdu9zUbL1bz+1bivJbquNLVmAADSdduuA6GemNQ4ymf/+Q1p7M6iwxVxAAAAIAWciAMAAAApIJrS46pVre3kPnNNQNPLHUia0e4JfZoZM9e6quZNqFXNRD++Vrc+dN3mBh1UAAA95MnHjYb67b9/XqgztPzqCq6IAwAAACngRBwAAABIAdGUnmfiDyb94BK/QvVSB5R2bn+u5boVYamZF95L/e3XqnE0xTWoAQDoJS99xmmhdhyxuo4r4gAAAEAKOBEHAAAAUsCJOAAAAJACMuI9zuaTbfvCTCbOcXUqI97sMt3aTjs1+3yayaUn2xdmCIkDABYAl/yjM3QVrz4AAACQAk7EAQAAgBQQTelxjaIQ1UrnZrzsZMvDdo5vdkw7Z/r0JoJi73eJGcjsTWbWBAAA9XBFHAAAAEgBJ+IAAABACoim9LgoFmFm2czmGndNWSjRkjQ6rTSKk8w1PppZs8GMm4duqKndAQAAixhXxAEAAIAUcCIOAAAApIBoSo+r1UzGwWkUolqNsw99fX11x9soRbVajR4rlUpmO7ruTEZ/P+vlmEmz5hOhaRRB8WJfQ1d3GRER8xKKo1MKAACogyviAAAAQAo4EQcAAABSwIk4AAAAkAIy4j3PtCU0efH+vny01N+//W2hfsUrXxXqJzzhCTqmvz8ak8/rOhq19bPZ52TGfHp6+rDjkznsds+a2an1NmoHGddHvEkAAICAK+IAAABACjgRBwAAAFJANKXH1aIpGrUeHBqMltv2q1+G+iMf+JdQ53L6Fk+XS9GYgwcnQn3MsRtC/RtnnRXqF77oRaEeGR6OxjvToy9r4iiZbFbrTPy7no26TE1NhbpYLIY6GRlpdgbMI1nmSMbYSE6z0RTn+B0XAADMjbMFAAAAIAWciAMAAAApIJrS82wswtyb6GBSqZRDXcvoY6Vy/c4mIiIjwxpvObh/d6i/9qXPhfrrX74k1FkTORER2bVHx2RMBOZ3X/XqUP/e770y3uagbnPp0qWhXrJkSahdk1NR7tu3L9TtjLYkNdM1JTmzps8ynSYAAJgbV8QBAACAFHAiDgAAAKSAaEqPq9XqT+gjcTJFXJyT0PvNMslQRqlUlnrK5u5Mxi4Txy2GBgd0OaexlW9dflmor7vqG9GYnNMfufHx8VD3mS4wz3vhi6IxL3nxi0N93LHrQ7161WioqzV9QQ4JhZhoStW8NhMT2jXG7ouISLlc/7WJJjsyr3nNx2+IkzjGAwAAkMQVcQAAACAFnIgDAAAAKSCa0uOmpjQ+USpWtE5EJ2zMIpPV368yJqiRyeejMcnJdnSMqtXmCrcYWROH8bqG0nTcTaRqVlcomJiHmWzo+m9eGY353tXf1M1kdEyxbOIgZr2nP/M50fg//H3t4nLi8ceH2j7/4eRkRSaCsnbtWt3/aHIfs9HES9Ns5xcAALB4cUUcAAAASAEn4gAAAEAKOBEHAAAAUkBGvIsqZc14f/wTH48eu+EH3w+1zS5PTuiMkdNFzVEXy4nsdVXXXegrhDqf17c4k8iVO6fB5lyuIPXYfUlmyp2ZabPq7XI2MB239auZ7LTNUWcy+nx8/NTEmzsqZkzW7I+d8fKm710fjb/1h98NdSHXF+qSyZjvPXgwGvOcX//1UJ/7+j8Ldc4857JphWjbTM7sD7/jAgCAuXG2AAAAAKSAE3EAAAAgBURTOsDGJMTUjzvlhFC7fBwFWbJkWaiPWbcu1DUzY2S+oO0HS1WNrIiIDPTb1oSmLWBV4xPFUkmskonK5LK6Lacl1QAADiJJREFUvv5+3bdcTqMY2Ww8W2S2Wn/2SBfFWeJlbGtFG02x6864+PfDKOVhXk/foE4qmwhKpTypY5y+NsMD8TZvvuH6UL/2qitCPTTYb7bpTJ2MptTsDa1pawgAAGZxRRwAAABIASfiAAAAQAqIpnSAM9GQ9RuODfWenTtCbeMfIiK1isZG9u18ONQjIyOhtjGPY45ZE4230Yg9e/bU3a+B/r7o9qC5bRuiVM1smpWKrndqKo7DlMrTobbdWQYHB0OdSLOIq2qExoY04g4qie4s5rFcLlf3/kbLJ29727XFREucJNZldsGbaTOzmfpxnDo7YW80NwYAACwqXBEHAAAAUsCJOAAAAJACoikd4M1EL8ND+hL3rx8NdV9hKBqTNemFBx/ZHurJYjwJz6NGV6+Mbg8P6vqWLtUOLDXRyEshNxCNGRrSMQfHJ0I9fnAs1BkzIU++L47TDHm9bcMXVTET3SQmHpqcngr19LQ+t/4B7UYy0t8fjcmYzi2P7NoVatuMJJOpH18REekzHWX6+mwcR38PzSbGRGGSWv1JhKRBZGZm5+xERrajCjEVAAAw47BXxJ1zn3TO7XTO3WbuW+Gcu9o5d8/sf5fP3u+ccx92zt3rnPuZc+70Tu48AABp4hgJoBXNRFM+LSJnJe47X0Su8d6fJCLXzN4WEXmxiJw0++9cEfloe3YTAICe9GnhGAlgng57Iu69/66I7E3c/XIR+cxs/RkR+R1z/3/5GT8SkWXOubXt2lkAAHoJx0gArZhvRnyN9/6R2Xq7iDzaS+9YEdlqlts2e98jsojUTCb4W9+4JtQf+eAFob7s8q9EY8ZM+8IVo8tDvczkzfsKA6aOWxEePKgZ79179JgwNTVtxuSjMStWapZ8aGiwbl2t6nPpK8QZ8YEB3Z+poma/pyZ09sqyxLN5Dg2bNodDmgU/OKb7Pymxk044UW+YvPZ2mxc3y5dKNp8tMjExbW6Nh8p2GMzm4uy2ncVUTJvD/oLm1XMF3f+5WiYymyaw6HCMBNCUlrum+JkG1o3nF2/AOXeuc26Lc27LLnNCBQDA0YJjJIC5zPdEfMejX6fN/nfn7P0PicgGs9z62fsO4b2/yHu/2Xu/eXR0tN4iAAAsRBwjATRlvtGUy0XkbBF53+x/LzP3n+ec+4KInCEiB8zXc4tG1kwnedyJJ4X6X//to3VrkXhmzMlJDWf8/OabQv23b/ybUN977y+i8TYmsnSZRktWjWr8JJdLzKxpxtTKlVDv3KFXX0oljZYkZ7xcc8zquo/ZNoeFgbgVYV9e92FwQPdz37DGaXbt3B2NuW/r/aGemtb9OeEkjayccvLJoc65eD8nzOv505v09Rw7YGIqkk2M0daKGXMx62ET9VmyZDjU+Wwi2rJkqQBYtDhGAmjKYU/EnXOfF5Hni8gq59w2EXmXzHy4fNE5d46IPCAir5pd/Osi8hIRuVdmor6v68A+AwDQEzhGAmjFYU/Evfd/0OChM+ss60XkDa3uFAAACwHHSACtYGbNHmG7bNgZL894xrNCfeOWWxqvwNsZLHVd//6RD4f6Qx/8QDTk4Ye2hbpgOqL0D2rHkKFhjY8MD8ezgY6Y+MW+PftCvX37jlCXK5VojI1zrF6t0RY7s+Ux6xp38+ozs24uWzIS6rt+cWeo9+6Ooy1jk9qRJZvX5/akpz411OtMzEZE5Mc/+kGo95uOLlLTyIqNENn3TETkvgf0tX3/e94d6vP//u2hTnZaAQAAi0vLXVMAAAAAHDlOxAEAAIAUONutIy2bN2/2W7ZsSXs3MOveuzTm8W8f+VCoL/3ypdFypbJ2MMmbiW6yGVPn4kmEhoY0mtJvJiXas1e7ptiuLSIilarGW1auXBnqFStXhNr+HFcScRjbEcVOVmSbq4yP74/GTJpJiSYndbKiwcFBqWdwMI6m2JWvWb0m1Nd+9/uhTnahWeicczd57zenvR/A0YZjJLCwbd68WbZs2VI3j3p0nQkAAAAACwQn4gAAAEAKOBEHAAAAUkD7QhzihJNOCfUHP/Ifof7Qv/1ntFytpi0TvanvvOP2UP/VX50bjbnr5z/TG3nNbg/06yyfK1YticYM9Gv+emhYWxYWTP5869atod67b080XrxuZ9VqzZgvXaLbSebKxcymaXPhdgZT+/x37opz7QfHteXhxPh43TFHW0YcAAAcGc4EAAAAgBRwIg4AAACkgGgKDtFsZCJaztSP/7Unhvq737+xuY3WqqH81MfiCMyFF1wQ6h0PawQlW9BoSqFf4ycbNm6K99Prvi1ZojETOzPmvr1x+8KpKW1ZmM/rrKMPP6IRlKx5zhuOOzYav2bNMaG2E2hm0u8WCgAAegRXxAEAAIAUcCIOAAAApIBoCrrKzoDposyGRkted+5fRWPOPvcv664rIzr+nrvvDvWHL/iXaLn/+dxnQ/3Idu1aUiho5CSXj38nHcz063I57ZSyfsOGUFeq0tDylaOhvvLKK0Jdy+p2+C0YAIDFjXMBAAAAIAWciAMAAAApIJqCroriKA0Xim9mknfUcdIpJ4f6I//58egxezuahMhr/at77onGvPY1rwn17XfoJETZnMZZrvv2taE+8dTHxjvUzPMEAACLGlfEAQAAgBRwIg4AAACkgGgKjnq2U0s8WZHWJzz21GjMD7bcpEuZmEm1quvKZvV+uw2RQ9I1AAAAh+CKOAAAAJACTsQBAACAFHAiDgAAAKSAjDiOes20TDzkN9IGY2wu/Ei3AQAAYHFFHAAAAEgBJ+IAAABACjgRBwAAAFLAiTgAAACQAk7EAQAAgBRwIg4AAACkgBNxAAAAIAWciAMAAAAp4EQcAAAASAEn4gAAAEAKOBEHAAAAUsCJOAAAAJACTsQBAACAFHAiDgAAAKSAE3EAAAAgBZyIAwAAACngRBwAAABIASfiAAAAQAqc9z7tfRDn3C4ReUBEVonI7hR3he2nu/1e2Ae2P//tH++9H23nzgAIx8gJWbifDUfLPrB9fgbmu/2Gx8eeOBF/lHNui/d+M9tfnNvvhX1g++n/DAA4VNr/b6a9/V7YB7bPz0Antk80BQAAAEgBJ+IAAABACnrtRPwitr+oty+S/j6wfQC9KO3/N9Pevkj6+8D205f2PrR9+z2VEQcAAAAWi167Ig4AAAAsCj1xIu6cO8s5d5dz7l7n3Pld2uYnnXM7nXO3mftWOOeuds7dM/vf5R3c/gbn3HXOuTucc7c75/62m/vgnOt3zv3YOXfr7Pb/cfb+Tc65G2ffi4udc4VObN/sR9Y5d7Nz7opub985d79z7ufOuVucc1tm7+vmz8Ay59yXnHO/cM7d6Zx7Rpe3f8rsc3/035hz7o3d3AcAh9ftYyTHR46Ps9tbtMfIbh4fUz8Rd85lReTfReTFInKaiPyBc+60Lmz60yJyVuK+80XkGu/9SSJyzeztTqmIyJu996eJyNNF5A2zz7tb+zAtIi/w3j9RRJ4kImc5554uIu8XkQ967x8jIvtE5JwObf9Rfysid5rb3d7+r3vvn2TaEXXzZ+BCEfmm9/6xIvJEmXkdurZ97/1ds8/9SSLyFBGZFJGvdHMfAMwtpWPkp4XjI8fHGYvyGNnV46P3PtV/IvIMEbnK3H6biLytS9veKCK3mdt3icja2XqtiNzVxdfhMhF5YRr7ICKDIvJTETlDZhrV5+q9Nx3Y7vrZH+QXiMgVIuK6vP37RWRV4r6uvP4islREfiWzf6eR9s+giLxIRG5Icx/4xz/+HfovrWMkx8ew7UV5fJzdBsdI3/njY+pXxEXkWBHZam5vm70vDWu894/M1ttFZE03Nuqc2ygiTxaRG7u5D7Nfe90iIjtF5GoR+aWI7PfeV2YX6fR78SEReYuI1GZvr+zy9r2IfMs5d5Nz7tzZ+7r1+m8SkV0i8qnZrx4/7pwb6uL2k14tIp+frdPaBwCH6pVjJMfHxXV8FOEY+aiOHh974US8J/mZX3c63lLGOTcsIl8WkTd678e6uQ/e+6qf+dplvYg8TUQe26ltJTnnXiYiO733N3Vrm3U823t/usx85fsG59xz7YMdfv1zInK6iHzUe/9kmZm+OvqKq4s/gwUR+W0RuST5WLf2AcDCwfGxs3rk+CjCMbIrx8deOBF/SEQ2mNvrZ+9Lww7n3FoRkdn/7uzkxpxzeZn5kPmc9/7SNPZBRMR7v19ErpOZr7qWOedysw918r14loj8tnPufhH5gsx8/XZhF7cv3vuHZv+7U2ayX0+T7r3+20Rkm/f+xtnbX5KZD52uv/8y8yH7U+/9jtnbaewDgPp65RjJ8XERHR9FOEbO6vjxsRdOxH8iIifN/jVwQWa+Arg8pX25XETOnq3PlplcWkc455yIfEJE7vTeX9DtfXDOjTrnls3WAzKTv7tTZj5wXtHp7Xvv3+a9X++93ygz7/m13vvXdGv7zrkh59zIo7XMZMBuky69/t777SKy1Tl3yuxdZ4rIHd3afsIfiH7tJintA4D6euUYyfFxkRwfRThGGp0/PnY65N5kEP4lInK3zGSw3t6lbX5eRB4RkbLM/OZ1jsxksK4RkXtE5NsisqKD23+2zHyl8TMRuWX230u6tQ8i8msicvPs9m8TkXfO3n+CiPxYRO6Vma9i+rrwXjxfRK7o5vZnt3Pr7L/bH/256/LPwJNEZMvse/BVEVneze3P7sOQiOwRkaXmvq7uA//4x7+5/3X7GMnxcXEfH822FvUxslvHR2bWBAAAAFLQC9EUAAAAYNHhRBwAAABIASfiAAAAQAo4EQcAAABSwIk4AAAAkAJOxAEAAIAUcCIOAAAApIATcQAAACAF/w8yFIMtueXzHAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0qmvz0xpmci"
      },
      "source": [
        "def ranker(input_image_id, input_description, item_id_pool, model, tokenizer, threshold=0.3, top_n=5, do_plot=False):\n",
        "  input_image_path = IMAGES_DIR + input_image_id + \".jpg\"\n",
        "  item_id_to_score = {}\n",
        "\n",
        "  for item_id in item_id_pool:\n",
        "    candidate_image_path = IMAGES_DIR + item_id + \".jpg\"\n",
        "    candidate_description = item_to_info[item_id]\n",
        "    output_prediction, output_confidence = single_pair_inference(premise_image_path=input_image_path,\n",
        "          hypothesis_image_path=candidate_image_path,\n",
        "          premise_text=input_description,\n",
        "          hypothesis_text=candidate_description,\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          threshold=threshold,\n",
        "          do_plot=False)\n",
        "    if output_prediction == \"Positive\":\n",
        "      item_id_to_score[item_id] = output_confidence\n",
        "    else:\n",
        "      continue\n",
        "  return item_id_to_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnuz8ilHN0Fv",
        "outputId": "9849cbdb-cd93-427b-ae92-77c6e3434eb6"
      },
      "source": [
        "random_index = random.randint(0, len(train_image_premise_id_list))\n",
        "image_id = train_image_premise_id_list[random_index]\n",
        "input_description = item_to_info[image_id]\n",
        "all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "\n",
        "ranker(image_id, input_description, all_item_ids, model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'000.379.80': 0.7950586080551147,\n",
              " '000.783.34': 0.4862244129180908,\n",
              " '000.997.89': 0.6121296882629395,\n",
              " '001.050.64': 0.5520339608192444,\n",
              " '001.125.40': 0.9947899580001831,\n",
              " '001.194.19': 0.7242779731750488,\n",
              " '001.286.21': 0.7616478800773621,\n",
              " '001.301.67': 0.31785741448402405,\n",
              " '001.330.19': 0.963839590549469,\n",
              " '001.414.01': 0.9705495238304138,\n",
              " '001.592.26': 0.9896553754806519,\n",
              " '001.592.31': 0.9660294055938721,\n",
              " '001.658.02': 0.9394971132278442,\n",
              " '001.737.36': 0.952509880065918,\n",
              " '001.878.56': 0.4908461272716522,\n",
              " '002.030.93': 0.9679516553878784,\n",
              " '002.033.33': 0.9927250146865845,\n",
              " '002.051.72': 0.9583969712257385,\n",
              " '002.095.99': 0.9429596066474915,\n",
              " '002.134.88': 0.5146068930625916,\n",
              " '002.255.04': 0.6914235949516296,\n",
              " '002.261.79': 0.9529257416725159,\n",
              " '002.267.49': 0.9506180286407471,\n",
              " '002.286.06': 0.34026509523391724,\n",
              " '002.287.05': 0.49328428506851196,\n",
              " '002.365.50': 0.5843818187713623,\n",
              " '002.382.00': 0.4233534038066864,\n",
              " '002.387.66': 0.8849351406097412,\n",
              " '002.392.33': 0.3145957887172699,\n",
              " '002.423.58': 0.8302456736564636,\n",
              " '002.451.30': 0.3780365586280823,\n",
              " '002.459.17': 0.4750702977180481,\n",
              " '002.460.78': 0.6462698578834534,\n",
              " '002.482.18': 0.8473235964775085,\n",
              " '002.485.29': 0.7880481481552124,\n",
              " '002.485.67': 0.7859435081481934,\n",
              " '002.508.95': 0.9792227745056152,\n",
              " '002.518.52': 0.908477246761322,\n",
              " '002.579.53': 0.9345458745956421,\n",
              " '002.588.39': 0.9617053866386414,\n",
              " '002.621.05': 0.9067032337188721,\n",
              " '002.632.80': 0.6649417877197266,\n",
              " '002.638.12': 0.9812737703323364,\n",
              " '002.643.88': 0.9610219597816467,\n",
              " '002.645.81': 0.5473190546035767,\n",
              " '002.662.45': 0.6855719089508057,\n",
              " '002.665.75': 0.8166608810424805,\n",
              " '002.696.06': 0.48618796467781067,\n",
              " '002.785.78': 0.6987402439117432,\n",
              " '002.806.99': 0.6760379672050476,\n",
              " '002.813.02': 0.9785067439079285,\n",
              " '002.831.03': 0.7689374089241028,\n",
              " '002.851.83': 0.6296300292015076,\n",
              " '002.907.78': 0.8865047693252563,\n",
              " '002.914.76': 0.9710619449615479,\n",
              " '002.914.81': 0.9835188388824463,\n",
              " '002.948.75': 0.9257492423057556,\n",
              " '002.994.15': 0.9823019504547119,\n",
              " '003.101.06': 0.5154466032981873,\n",
              " '003.123.08': 0.9355744123458862,\n",
              " '003.146.56': 0.4339234232902527,\n",
              " '003.200.25': 0.3442792594432831,\n",
              " '003.218.69': 0.47660723328590393,\n",
              " '003.288.37': 0.49006137251853943,\n",
              " '091.195.61': 0.9242982864379883,\n",
              " '091.283.58': 0.9191305041313171,\n",
              " '098.744.98': 0.8381944298744202,\n",
              " '098.843.60': 0.9709309935569763,\n",
              " '099.031.08': 0.973321795463562,\n",
              " '099.034.53': 0.9472792744636536,\n",
              " '100.937.63': 0.9916280508041382,\n",
              " '100.989.87': 0.8627578020095825,\n",
              " '101.229.11': 0.9066197276115417,\n",
              " '101.311.33': 0.5247506499290466,\n",
              " '101.419.76': 0.8791581392288208,\n",
              " '101.484.64': 0.9735909104347229,\n",
              " '101.493.50': 0.32750922441482544,\n",
              " '101.665.75': 0.7727946043014526,\n",
              " '101.821.65': 0.981045126914978,\n",
              " '101.855.93': 0.8406063914299011,\n",
              " '101.920.13': 0.736142635345459,\n",
              " '102.121.29': 0.44387388229370117,\n",
              " '102.222.32': 0.3328578770160675,\n",
              " '102.257.87': 0.6339396834373474,\n",
              " '102.333.77': 0.9308109879493713,\n",
              " '102.348.62': 0.4653356075286865,\n",
              " '102.398.88': 0.3070748746395111,\n",
              " '102.414.19': 0.8156928420066833,\n",
              " '102.414.62': 0.889674186706543,\n",
              " '102.427.39': 0.9576063752174377,\n",
              " '102.460.25': 0.9735480546951294,\n",
              " '102.479.68': 0.3061961829662323,\n",
              " '102.485.19': 0.6700312495231628,\n",
              " '102.509.13': 0.9836612343788147,\n",
              " '102.519.98': 0.4282836318016052,\n",
              " '102.534.88': 0.5725728273391724,\n",
              " '102.566.13': 0.9261559844017029,\n",
              " '102.594.85': 0.38295698165893555,\n",
              " '102.604.36': 0.7966843247413635,\n",
              " '102.620.96': 0.8344048261642456,\n",
              " '102.628.31': 0.31824734807014465,\n",
              " '102.638.16': 0.9307649731636047,\n",
              " '102.640.00': 0.9399704933166504,\n",
              " '102.679.99': 0.9782758951187134,\n",
              " '102.696.96': 0.9528533220291138,\n",
              " '102.708.07': 0.5059435367584229,\n",
              " '102.727.88': 0.9886656999588013,\n",
              " '102.783.56': 0.6961860656738281,\n",
              " '102.802.03': 0.5856936573982239,\n",
              " '102.828.29': 0.995317816734314,\n",
              " '102.882.99': 0.9706754088401794,\n",
              " '102.883.03': 0.9898199439048767,\n",
              " '102.894.11': 0.856417179107666,\n",
              " '102.895.76': 0.5847588777542114,\n",
              " '102.901.84': 0.527858316898346,\n",
              " '102.911.31': 0.5068662762641907,\n",
              " '102.929.08': 0.9872264266014099,\n",
              " '102.947.71': 0.8118510842323303,\n",
              " '102.948.89': 0.9896873831748962,\n",
              " '102.953.70': 0.9798853993415833,\n",
              " '102.957.99': 0.8734533786773682,\n",
              " '102.976.56': 0.5317376852035522,\n",
              " '102.980.00': 0.43058261275291443,\n",
              " '102.984.20': 0.4246017336845398,\n",
              " '103.008.14': 0.9657266736030579,\n",
              " '103.022.38': 0.6048815250396729,\n",
              " '103.043.55': 0.9505003690719604,\n",
              " '103.067.26': 0.7216702103614807,\n",
              " '103.112.28': 0.3575722277164459,\n",
              " '103.123.84': 0.9527584910392761,\n",
              " '103.196.63': 0.9208467602729797,\n",
              " '103.197.19': 0.495382696390152,\n",
              " '103.211.47': 0.9477829337120056,\n",
              " '103.213.26': 0.826318621635437,\n",
              " '103.220.62': 0.3033103048801422,\n",
              " '191.195.27': 0.6531025171279907,\n",
              " '191.278.10': 0.6005686521530151,\n",
              " '191.289.61': 0.7394161820411682,\n",
              " '198.850.43': 0.5331985950469971,\n",
              " '198.944.72': 0.9262325167655945,\n",
              " '199.031.03': 0.9536206722259521,\n",
              " '200.114.13': 0.39547669887542725,\n",
              " '200.452.05': 0.41688427329063416,\n",
              " '200.474.50': 0.7720171809196472,\n",
              " '200.783.33': 0.9709648489952087,\n",
              " '201.125.39': 0.6128437519073486,\n",
              " '201.286.96': 0.6500534415245056,\n",
              " '201.514.08': 0.48809006810188293,\n",
              " '201.521.96': 0.3176373243331909,\n",
              " '201.822.83': 0.9803521633148193,\n",
              " '201.840.41': 0.8334120512008667,\n",
              " '201.878.60': 0.8172996044158936,\n",
              " '201.906.26': 0.9619527459144592,\n",
              " '201.906.74': 0.9534048438072205,\n",
              " '201.932.67': 0.49648869037628174,\n",
              " '202.253.34': 0.30324703454971313,\n",
              " '202.287.66': 0.7539507150650024,\n",
              " '202.290.11': 0.7557495832443237,\n",
              " '202.387.51': 0.9871149063110352,\n",
              " '202.410.08': 0.9512208104133606,\n",
              " '202.414.09': 0.8869768977165222,\n",
              " '202.438.37': 0.8617967367172241,\n",
              " '202.468.45': 0.7700445652008057,\n",
              " '202.469.06': 0.735824465751648,\n",
              " '202.480.00': 0.9528625011444092,\n",
              " '202.481.18': 0.8978665471076965,\n",
              " '202.508.61': 0.7546510696411133,\n",
              " '202.508.99': 0.9416216611862183,\n",
              " '202.527.42': 0.9598929286003113,\n",
              " '202.561.51': 0.5673824548721313,\n",
              " '202.578.10': 0.4691084623336792,\n",
              " '202.604.45': 0.42488351464271545,\n",
              " '202.636.32': 0.8567050099372864,\n",
              " '202.643.92': 0.9714757800102234,\n",
              " '202.645.99': 0.9450033903121948,\n",
              " '202.703.12': 0.5485875010490417,\n",
              " '202.731.36': 0.7245964407920837,\n",
              " '202.753.38': 0.3373793959617615,\n",
              " '202.806.84': 0.30597424507141113,\n",
              " '202.811.36': 0.30038559436798096,\n",
              " '202.848.04': 0.6992238759994507,\n",
              " '202.867.61': 0.5993765592575073,\n",
              " '202.873.17': 0.666621744632721,\n",
              " '202.882.70': 0.9308815002441406,\n",
              " '202.902.06': 0.7449544072151184,\n",
              " '202.902.11': 0.7266055941581726,\n",
              " '202.919.94': 0.31083282828330994,\n",
              " '202.937.14': 0.6510019302368164,\n",
              " '202.948.79': 0.8402160406112671,\n",
              " '202.948.84': 0.33299732208251953,\n",
              " '202.953.60': 0.9854578971862793,\n",
              " '202.959.11': 0.9891082048416138,\n",
              " '202.960.86': 0.9808245897293091,\n",
              " '202.962.65': 0.32192233204841614,\n",
              " '202.976.27': 0.8000880479812622,\n",
              " '203.002.86': 0.43992629647254944,\n",
              " '203.057.07': 0.9382162094116211,\n",
              " '203.063.87': 0.8967039585113525,\n",
              " '203.070.37': 0.5520222783088684,\n",
              " '203.125.19': 0.39312490820884705,\n",
              " '203.129.01': 0.8750418424606323,\n",
              " '203.139.67': 0.9134231805801392,\n",
              " '203.146.60': 0.6640481948852539,\n",
              " '203.195.92': 0.9627587199211121,\n",
              " '203.279.31': 0.9593861699104309,\n",
              " '203.323.53': 0.5621169805526733,\n",
              " '231.029.00': 0.858278751373291,\n",
              " '245.244.85': 0.9858614206314087,\n",
              " '290.047.76': 0.7817588448524475,\n",
              " '290.204.65': 0.9522644281387329,\n",
              " '290.428.77': 0.3901076316833496,\n",
              " '290.949.51': 0.5201782584190369,\n",
              " '290.960.02': 0.951557993888855,\n",
              " '291.026.30': 0.8782799243927002,\n",
              " '291.239.58': 0.7690433263778687,\n",
              " '291.805.24': 0.43393129110336304,\n",
              " '291.805.43': 0.9325597286224365,\n",
              " '298.280.90': 0.31915345788002014,\n",
              " '299.321.81': 0.9486048817634583,\n",
              " '300.492.79': 0.49498018622398376,\n",
              " '300.643.59': 0.7286224365234375,\n",
              " '301.165.51': 0.6204235553741455,\n",
              " '301.256.97': 0.8688926696777344,\n",
              " '301.286.91': 0.49195945262908936,\n",
              " '301.354.46': 0.8746119737625122,\n",
              " '301.377.75': 0.8464131951332092,\n",
              " '301.396.23': 0.4991012513637543,\n",
              " '301.486.94': 0.8331040740013123,\n",
              " '301.491.32': 0.9949520826339722,\n",
              " '301.510.35': 0.7505231499671936,\n",
              " '301.591.83': 0.9850009679794312,\n",
              " '301.784.26': 0.8284536600112915,\n",
              " '301.794.35': 0.48730212450027466,\n",
              " '301.805.04': 0.45629405975341797,\n",
              " '301.904.47': 0.9098114371299744,\n",
              " '301.931.39': 0.30009374022483826,\n",
              " '301.933.18': 0.4499536454677582,\n",
              " '301.933.23': 0.9561231732368469,\n",
              " '301.992.83': 0.5097871422767639,\n",
              " '302.002.53': 0.4408174753189087,\n",
              " '302.006.96': 0.9619256258010864,\n",
              " '302.054.39': 0.4879637658596039,\n",
              " '302.135.52': 0.8102444410324097,\n",
              " '302.176.54': 0.9501190781593323,\n",
              " '302.176.73': 0.32481786608695984,\n",
              " '302.225.23': 0.7118732929229736,\n",
              " '302.249.04': 0.9533873200416565,\n",
              " '302.275.92': 0.6740226149559021,\n",
              " '302.290.77': 0.8218305706977844,\n",
              " '302.313.01': 0.5457109212875366,\n",
              " '302.382.51': 0.6263279318809509,\n",
              " '302.495.70': 0.9649691581726074,\n",
              " '302.519.40': 0.3869222104549408,\n",
              " '302.527.46': 0.32117319107055664,\n",
              " '302.530.67': 0.7393928170204163,\n",
              " '302.530.72': 0.9847857356071472,\n",
              " '302.643.82': 0.90465247631073,\n",
              " '302.673.14': 0.7604023218154907,\n",
              " '302.679.79': 0.9852560758590698,\n",
              " '302.680.02': 0.982434093952179,\n",
              " '302.701.42': 0.7913464307785034,\n",
              " '302.703.16': 0.32235461473464966,\n",
              " '302.707.69': 0.931773841381073,\n",
              " '302.713.68': 0.983065128326416,\n",
              " '302.714.91': 0.9629228115081787,\n",
              " '302.810.94': 0.9726709723472595,\n",
              " '302.811.50': 0.8822556138038635,\n",
              " '302.816.97': 0.7793359756469727,\n",
              " '302.829.46': 0.7096133232116699,\n",
              " '302.835.78': 0.966947078704834,\n",
              " '302.837.81': 0.45391687750816345,\n",
              " '302.882.41': 0.35246482491493225,\n",
              " '302.902.01': 0.9117090702056885,\n",
              " '302.920.78': 0.7385810017585754,\n",
              " '302.994.85': 0.6790514588356018,\n",
              " '303.037.22': 0.5766990184783936,\n",
              " '303.047.26': 0.7163954973220825,\n",
              " '303.097.81': 0.5867915153503418,\n",
              " '303.197.23': 0.9215261936187744,\n",
              " '303.208.54': 0.9094188809394836,\n",
              " '303.323.43': 0.3119357228279114,\n",
              " '380.062.00': 0.8924457430839539,\n",
              " '390.117.81': 0.5332222580909729,\n",
              " '390.203.80': 0.9654511213302612,\n",
              " '390.256.41': 0.9193779230117798,\n",
              " '390.314.68': 0.8357541561126709,\n",
              " '391.250.99': 0.9524574279785156,\n",
              " '391.277.05': 0.7907573580741882,\n",
              " '391.278.09': 0.6346479058265686,\n",
              " '391.710.34': 0.962345540523529,\n",
              " '391.805.66': 0.8527953028678894,\n",
              " '398.766.60': 0.8892000317573547,\n",
              " '398.856.45': 0.8654241561889648,\n",
              " '400.337.63': 0.3655378222465515,\n",
              " '400.545.38': 0.8693903088569641,\n",
              " '401.125.43': 0.7505621314048767,\n",
              " '401.128.97': 0.832107424736023,\n",
              " '401.315.51': 0.8540880084037781,\n",
              " '401.499.33': 0.9303650856018066,\n",
              " '401.509.93': 0.9418119788169861,\n",
              " '401.522.80': 0.7921271324157715,\n",
              " '401.541.04': 0.6277782917022705,\n",
              " '401.592.05': 0.9792808890342712,\n",
              " '401.878.64': 0.8318609595298767,\n",
              " '401.906.11': 0.888787567615509,\n",
              " '401.928.51': 0.43474701046943665,\n",
              " '401.933.32': 0.8509330749511719,\n",
              " '401.980.23': 0.9323961734771729,\n",
              " '402.017.37': 0.31012973189353943,\n",
              " '402.112.13': 0.9650578498840332,\n",
              " '402.121.61': 0.9678905606269836,\n",
              " '402.160.84': 0.362019807100296,\n",
              " '402.176.77': 0.8617548942565918,\n",
              " '402.226.07': 0.9902730584144592,\n",
              " '402.238.57': 0.8806492686271667,\n",
              " '402.275.96': 0.9909729957580566,\n",
              " '402.288.12': 0.7207497954368591,\n",
              " '402.329.46': 0.9472450613975525,\n",
              " '402.331.49': 0.5138996839523315,\n",
              " '402.364.78': 0.8081225752830505,\n",
              " '402.414.08': 0.966640293598175,\n",
              " '402.414.51': 0.9776799082756042,\n",
              " '402.414.89': 0.6241844892501831,\n",
              " '402.467.74': 0.7589543461799622,\n",
              " '402.481.22': 0.8966835141181946,\n",
              " '402.484.43': 0.4084727466106415,\n",
              " '402.530.76': 0.7444582581520081,\n",
              " '402.572.20': 0.4321770966053009,\n",
              " '402.590.64': 0.40861570835113525,\n",
              " '402.595.11': 0.7305374145507812,\n",
              " '402.612.55': 0.5851110816001892,\n",
              " '402.629.57': 0.30428579449653625,\n",
              " '402.636.26': 0.5600405931472778,\n",
              " '402.637.87': 0.905985951423645,\n",
              " '402.662.34': 0.447104811668396,\n",
              " '402.675.68': 0.5480879545211792,\n",
              " '402.708.20': 0.30766773223876953,\n",
              " '402.712.59': 0.8102600574493408,\n",
              " '402.733.62': 0.6063975691795349,\n",
              " '402.745.21': 0.9644906520843506,\n",
              " '402.797.31': 0.8544566035270691,\n",
              " '402.804.90': 0.41585254669189453,\n",
              " '402.808.19': 0.821890652179718,\n",
              " '402.812.77': 0.3145886957645416,\n",
              " '402.886.79': 0.9931981563568115,\n",
              " '402.902.91': 0.7203657031059265,\n",
              " '402.913.42': 0.6445230841636658,\n",
              " '402.929.02': 0.9528955221176147,\n",
              " '402.954.01': 0.9858391880989075,\n",
              " '402.958.73': 0.6864321827888489,\n",
              " '402.989.23': 0.38490357995033264,\n",
              " '403.000.11': 0.9626307487487793,\n",
              " '403.023.26': 0.9449730515480042,\n",
              " '403.034.39': 0.7279489636421204,\n",
              " '403.070.17': 0.8620189428329468,\n",
              " '403.081.06': 0.3043532967567444,\n",
              " '403.112.84': 0.3253861367702484,\n",
              " '403.196.28': 0.6629544496536255,\n",
              " '403.202.12': 0.9201949238777161,\n",
              " '403.253.18': 0.3986101448535919,\n",
              " '403.320.74': 0.5661657452583313,\n",
              " '403.702.83': 0.9398530721664429,\n",
              " '490.022.72': 0.7575592398643494,\n",
              " '490.024.32': 0.3935910761356354,\n",
              " '490.460.68': 0.8527031540870667,\n",
              " '490.468.17': 0.8352214097976685,\n",
              " '490.473.41': 0.7699116468429565,\n",
              " '491.023.04': 0.5222008228302002,\n",
              " '491.586.97': 0.924800455570221,\n",
              " '498.759.62': 0.8738752603530884,\n",
              " '500.382.13': 0.9505370259284973,\n",
              " '500.643.58': 0.36187052726745605,\n",
              " '500.772.47': 0.9604727625846863,\n",
              " '500.774.69': 0.5395547747612,\n",
              " '501.139.38': 0.608279287815094,\n",
              " '501.158.62': 0.5326407551765442,\n",
              " '501.387.50': 0.5428047180175781,\n",
              " '501.591.77': 0.9584812521934509,\n",
              " '501.635.46': 0.991226851940155,\n",
              " '501.784.11': 0.338058739900589,\n",
              " '501.819.94': 0.9866885542869568,\n",
              " '502.015.34': 0.7196659445762634,\n",
              " '502.062.11': 0.9381409883499146,\n",
              " '502.094.41': 0.9762951135635376,\n",
              " '502.135.46': 0.9885739088058472,\n",
              " '502.145.60': 0.9255231022834778,\n",
              " '502.189.64': 0.9635102152824402,\n",
              " '502.191.43': 0.8628962635993958,\n",
              " '502.213.01': 0.9920098781585693,\n",
              " '502.216.88': 0.533268392086029,\n",
              " '502.285.76': 0.841036319732666,\n",
              " '502.290.19': 0.31065133213996887,\n",
              " '502.427.37': 0.7993305325508118,\n",
              " '502.432.99': 0.7560418844223022,\n",
              " '502.480.70': 0.9566020369529724,\n",
              " '502.493.81': 0.9746222496032715,\n",
              " '502.517.98': 0.9043101668357849,\n",
              " '502.523.59': 0.6797642111778259,\n",
              " '502.523.97': 0.872173547744751,\n",
              " '502.539.81': 0.9559706449508667,\n",
              " '502.579.84': 0.6507938504219055,\n",
              " '502.584.55': 0.39903324842453003,\n",
              " '502.629.33': 0.3894273638725281,\n",
              " '502.637.15': 0.6051697731018066,\n",
              " '502.642.63': 0.7502281069755554,\n",
              " '502.663.18': 0.6915045380592346,\n",
              " '502.675.58': 0.6373603940010071,\n",
              " '502.715.17': 0.7862780094146729,\n",
              " '502.773.93': 0.579521894454956,\n",
              " '502.795.99': 0.42921173572540283,\n",
              " '502.797.02': 0.9280871748924255,\n",
              " '502.797.21': 0.9503426551818848,\n",
              " '502.816.20': 0.4036393165588379,\n",
              " '502.818.56': 0.8973598480224609,\n",
              " '502.829.45': 0.7027062177658081,\n",
              " '502.831.05': 0.9749606251716614,\n",
              " '502.841.95': 0.9306418299674988,\n",
              " '502.847.94': 0.6337394118309021,\n",
              " '502.914.74': 0.9722168445587158,\n",
              " '502.929.06': 0.9813255071640015,\n",
              " '502.947.69': 0.8579361438751221,\n",
              " '502.953.68': 0.9787377715110779,\n",
              " '502.953.87': 0.9955006241798401,\n",
              " '502.970.08': 0.8972052335739136,\n",
              " '502.984.18': 0.6867589354515076,\n",
              " '502.994.08': 0.9687749743461609,\n",
              " '503.001.24': 0.8680698871612549,\n",
              " '503.002.99': 0.306918740272522,\n",
              " '503.045.94': 0.7619657516479492,\n",
              " '503.048.53': 0.7537298798561096,\n",
              " '503.080.02': 0.5902990102767944,\n",
              " '503.132.25': 0.9878731966018677,\n",
              " '503.222.39': 0.8004782199859619,\n",
              " '503.323.23': 0.7627896070480347,\n",
              " '503.334.12': 0.8452644944190979,\n",
              " '590.062.60': 0.9867037534713745,\n",
              " '590.066.46': 0.8746302723884583,\n",
              " '590.203.84': 0.9483986496925354,\n",
              " '590.235.04': 0.9712320566177368,\n",
              " '591.278.08': 0.944934606552124,\n",
              " '591.580.03': 0.5396483540534973,\n",
              " '600.478.96': 0.907948911190033,\n",
              " '600.937.65': 0.9708606600761414,\n",
              " '600.940.72': 0.9369191527366638,\n",
              " '601.136.12': 0.3214472234249115,\n",
              " '601.141.12': 0.5310270190238953,\n",
              " '601.315.93': 0.590242326259613,\n",
              " '601.392.35': 0.6303389072418213,\n",
              " '601.505.53': 0.49373719096183777,\n",
              " '601.591.72': 0.9357587099075317,\n",
              " '601.638.81': 0.472201406955719,\n",
              " '601.665.73': 0.919762372970581,\n",
              " '601.774.73': 0.9611347317695618,\n",
              " '601.794.34': 0.7679501175880432,\n",
              " '601.888.48': 0.9001314043998718,\n",
              " '602.022.17': 0.7429190874099731,\n",
              " '602.035.80': 0.6612802147865295,\n",
              " '602.054.33': 0.8784425258636475,\n",
              " '602.060.98': 0.961111843585968,\n",
              " '602.079.03': 0.6506657004356384,\n",
              " '602.176.76': 0.9403088688850403,\n",
              " '602.236.20': 0.9737128019332886,\n",
              " '602.261.38': 0.8921687602996826,\n",
              " '602.364.82': 0.3464198112487793,\n",
              " '602.400.21': 0.9802690148353577,\n",
              " '602.418.17': 0.9166253805160522,\n",
              " '602.425.29': 0.8825058341026306,\n",
              " '602.484.18': 0.9855526089668274,\n",
              " '602.510.38': 0.9500281810760498,\n",
              " '602.519.91': 0.5450840592384338,\n",
              " '602.566.63': 0.3402933180332184,\n",
              " '602.604.48': 0.6723129749298096,\n",
              " '602.607.64': 0.5088484287261963,\n",
              " '602.621.26': 0.8740901350975037,\n",
              " '602.633.43': 0.9658951163291931,\n",
              " '602.638.14': 0.9244378805160522,\n",
              " '602.643.90': 0.9921677708625793,\n",
              " '602.662.28': 0.49339377880096436,\n",
              " '602.802.67': 0.46376726031303406,\n",
              " '602.812.95': 0.9732357263565063,\n",
              " '602.813.75': 0.9691205024719238,\n",
              " '602.820.68': 0.9781991839408875,\n",
              " '602.824.45': 0.7311875820159912,\n",
              " '602.883.05': 0.9886161684989929,\n",
              " '602.886.83': 0.9703212976455688,\n",
              " '602.901.86': 0.9665841460227966,\n",
              " '602.909.83': 0.3898966908454895,\n",
              " '602.914.78': 0.8628871440887451,\n",
              " '602.915.10': 0.9804850816726685,\n",
              " '602.917.08': 0.6872010231018066,\n",
              " '602.964.28': 0.44901567697525024,\n",
              " '602.976.73': 0.38491740822792053,\n",
              " '602.994.22': 0.9803472757339478,\n",
              " '603.034.19': 0.9755284190177917,\n",
              " '603.043.53': 0.9653695821762085,\n",
              " '603.057.05': 0.7383919358253479,\n",
              " '603.083.08': 0.8209754228591919,\n",
              " '603.083.13': 0.7709381580352783,\n",
              " '603.122.92': 0.9333704113960266,\n",
              " '603.195.28': 0.8287431597709656,\n",
              " '603.221.73': 0.7135750651359558,\n",
              " '603.247.99': 0.9816192388534546,\n",
              " '603.279.34': 0.6445626616477966,\n",
              " '603.279.67': 0.43731218576431274,\n",
              " '603.303.47': 0.5049571394920349,\n",
              " '603.323.70': 0.4350055754184723,\n",
              " '603.491.15': 0.7316229939460754,\n",
              " '690.109.35': 0.33237457275390625,\n",
              " '690.208.59': 0.9816077947616577,\n",
              " '690.303.68': 0.34229332208633423,\n",
              " '690.481.70': 0.6207798719406128,\n",
              " '690.698.98': 0.4889765679836273,\n",
              " '691.249.46': 0.3780968487262726,\n",
              " '691.335.97': 0.7528275847434998,\n",
              " '699.030.68': 0.9420661330223083,\n",
              " '699.030.73': 0.9718502163887024,\n",
              " '699.031.05': 0.972288966178894,\n",
              " '700.108.35': 0.6315919756889343,\n",
              " '700.108.78': 0.9536044001579285,\n",
              " '700.113.97': 0.6753144860267639,\n",
              " '700.774.68': 0.858348548412323,\n",
              " '700.914.12': 0.4041234254837036,\n",
              " '700.989.89': 0.4802986681461334,\n",
              " '701.034.10': 0.798513650894165,\n",
              " '701.286.13': 0.6643803715705872,\n",
              " '701.527.02': 0.9569893479347229,\n",
              " '701.591.76': 0.9828975796699524,\n",
              " '701.592.04': 0.9850287437438965,\n",
              " '701.774.77': 0.9366541504859924,\n",
              " '701.866.03': 0.3315117359161377,\n",
              " '701.906.38': 0.9644435048103333,\n",
              " '701.970.84': 0.9053866267204285,\n",
              " '702.034.00': 0.9898068308830261,\n",
              " '702.135.45': 0.8613523244857788,\n",
              " '702.176.71': 0.92843097448349,\n",
              " '702.176.85': 0.9679113626480103,\n",
              " '702.189.63': 0.6736907958984375,\n",
              " '702.210.03': 0.9106588959693909,\n",
              " '702.231.44': 0.35838136076927185,\n",
              " '702.237.28': 0.807752251625061,\n",
              " '702.257.70': 0.9830108880996704,\n",
              " '702.275.90': 0.9045783281326294,\n",
              " '702.276.27': 0.9773191809654236,\n",
              " '702.283.25': 0.3182009756565094,\n",
              " '702.382.68': 0.6436077952384949,\n",
              " '702.385.41': 0.7528800964355469,\n",
              " '702.420.86': 0.8993189334869385,\n",
              " '702.433.16': 0.9798652529716492,\n",
              " '702.455.94': 0.9753343462944031,\n",
              " '702.458.53': 0.3679827153682709,\n",
              " '702.481.25': 0.8247472047805786,\n",
              " '702.495.06': 0.641520082950592,\n",
              " '702.508.54': 0.9659596681594849,\n",
              " '702.517.40': 0.7907914519309998,\n",
              " '702.553.33': 0.44086775183677673,\n",
              " '702.569.31': 0.9270576238632202,\n",
              " '702.583.17': 0.9417139291763306,\n",
              " '702.592.13': 0.53453528881073,\n",
              " '702.600.80': 0.8677265644073486,\n",
              " '702.640.02': 0.9826080203056335,\n",
              " '702.683.97': 0.40819650888442993,\n",
              " '702.686.46': 0.8677025437355042,\n",
              " '702.694.48': 0.3658626079559326,\n",
              " '702.714.89': 0.9557548761367798,\n",
              " '702.715.21': 0.8469434976577759,\n",
              " '702.720.64': 0.9566007256507874,\n",
              " '702.735.96': 0.7133826613426208,\n",
              " '702.782.97': 0.5905372500419617,\n",
              " '702.783.01': 0.32323068380355835,\n",
              " '702.805.11': 0.3195013701915741,\n",
              " '702.808.13': 0.44611769914627075,\n",
              " '702.828.31': 0.9750403165817261,\n",
              " '702.836.75': 0.863689661026001,\n",
              " '702.902.23': 0.9213846325874329,\n",
              " '702.914.68': 0.98700350522995,\n",
              " '702.914.87': 0.976787269115448,\n",
              " '702.922.98': 0.9461166262626648,\n",
              " '702.954.14': 0.9809783697128296,\n",
              " '702.976.58': 0.5709441900253296,\n",
              " '702.977.43': 0.9842512607574463,\n",
              " '703.037.20': 0.8733810782432556,\n",
              " '703.057.00': 0.5625011920928955,\n",
              " '703.084.16': 0.3099607527256012,\n",
              " '703.116.16': 0.9632866978645325,\n",
              " '703.191.27': 0.3140459954738617,\n",
              " '703.195.61': 0.9696176052093506,\n",
              " '703.200.17': 0.8232631087303162,\n",
              " '703.264.20': 0.6080442070960999,\n",
              " '703.363.82': 0.3066607415676117,\n",
              " '790.204.63': 0.9489060044288635,\n",
              " '790.462.60': 0.5347737669944763,\n",
              " '791.116.32': 0.3787423372268677,\n",
              " '791.278.07': 0.42326819896698,\n",
              " '791.407.00': 0.7256311178207397,\n",
              " '791.560.22': 0.9351792335510254,\n",
              " '791.777.03': 0.9087536931037903,\n",
              " '799.296.47': 0.3453572988510132,\n",
              " '800.680.10': 0.564385712146759,\n",
              " '800.892.39': 0.907482922077179,\n",
              " '800.985.83': 0.5921658277511597,\n",
              " '800.992.24': 0.35587459802627563,\n",
              " '801.462.49': 0.8130196928977966,\n",
              " '801.626.68': 0.9473071098327637,\n",
              " '801.635.83': 0.9855550527572632,\n",
              " '801.638.04': 0.31677404046058655,\n",
              " '801.906.28': 0.8578063249588013,\n",
              " '802.017.40': 0.5084853172302246,\n",
              " '802.019.57': 0.40645289421081543,\n",
              " '802.037.96': 0.9548192024230957,\n",
              " '802.124.18': 0.8468322157859802,\n",
              " '802.176.75': 0.9106183648109436,\n",
              " '802.180.24': 0.3968158960342407,\n",
              " '802.196.22': 0.7583869099617004,\n",
              " '802.214.27': 0.6982009410858154,\n",
              " '802.251.66': 0.7072688341140747,\n",
              " '802.312.90': 0.9449878334999084,\n",
              " '802.339.63': 0.8944650292396545,\n",
              " '802.381.83': 0.8596088290214539,\n",
              " '802.404.40': 0.8463131785392761,\n",
              " '802.415.38': 0.3649635910987854,\n",
              " '802.508.63': 0.9126578569412231,\n",
              " '802.526.64': 0.907686710357666,\n",
              " '802.600.94': 0.615925669670105,\n",
              " '802.637.09': 0.6642847657203674,\n",
              " '802.640.30': 0.9133049249649048,\n",
              " '802.643.94': 0.9979580640792847,\n",
              " '802.644.31': 0.6815285086631775,\n",
              " '802.671.37': 0.6318682432174683,\n",
              " '802.676.27': 0.50588458776474,\n",
              " '802.689.81': 0.8149842023849487,\n",
              " '802.691.03': 0.4723030924797058,\n",
              " '802.699.52': 0.3173407316207886,\n",
              " '802.745.19': 0.9638411998748779,\n",
              " '802.758.87': 0.37290486693382263,\n",
              " '802.783.67': 0.6818214654922485,\n",
              " '802.806.76': 0.8852952718734741,\n",
              " '802.820.67': 0.7878094911575317,\n",
              " '802.835.85': 0.8796268701553345,\n",
              " '802.857.87': 0.8404073119163513,\n",
              " '802.863.91': 0.43526405096054077,\n",
              " '802.917.07': 0.8738900423049927,\n",
              " '802.926.36': 0.6919620633125305,\n",
              " '802.952.77': 0.9766365885734558,\n",
              " '802.961.25': 0.315846711397171,\n",
              " '802.970.21': 0.648635745048523,\n",
              " '802.991.95': 0.36409950256347656,\n",
              " '803.034.37': 0.8340730667114258,\n",
              " '803.046.15': 0.8212270736694336,\n",
              " '803.156.52': 0.714328944683075,\n",
              " '803.182.31': 0.32807350158691406,\n",
              " '803.268.96': 0.8155826330184937,\n",
              " '803.281.45': 0.9620027542114258,\n",
              " '803.292.44': 0.508585512638092,\n",
              " '803.324.68': 0.5216314792633057,\n",
              " '832.351.10': 0.815558671951294,\n",
              " '874.894.09': 0.6571223139762878,\n",
              " '890.066.16': 0.3641788363456726,\n",
              " '890.066.83': 0.9066081047058105,\n",
              " '890.234.99': 0.9325576424598694,\n",
              " '890.289.15': 0.7825554609298706,\n",
              " '890.291.18': 0.34040728211402893,\n",
              " '891.175.20': 0.975159764289856,\n",
              " '891.236.01': 0.8403632044792175,\n",
              " '891.567.76': 0.9695239067077637,\n",
              " '899.031.09': 0.9890545606613159,\n",
              " '899.179.03': 0.40177813172340393,\n",
              " '899.206.94': 0.6056527495384216,\n",
              " '900.379.90': 0.9842398166656494,\n",
              " '900.468.43': 0.6608931422233582,\n",
              " '900.495.73': 0.9425438046455383,\n",
              " '900.990.49': 0.9859585165977478,\n",
              " '901.011.13': 0.9505673050880432,\n",
              " '901.113.91': 0.9332876801490784,\n",
              " '901.214.13': 0.32765039801597595,\n",
              " '901.452.68': 0.856975793838501,\n",
              " '901.524.47': 0.7357232570648193,\n",
              " '901.592.55': 0.9858350157737732,\n",
              " '901.598.87': 0.5062762498855591,\n",
              " '901.600.27': 0.35284581780433655,\n",
              " '901.766.41': 0.3824845254421234,\n",
              " '901.860.94': 0.9164653420448303,\n",
              " '902.049.03': 0.7154960036277771,\n",
              " '902.078.07': 0.7913557291030884,\n",
              " '902.093.64': 0.8206161260604858,\n",
              " '902.132.57': 0.6740607619285583,\n",
              " '902.137.52': 0.9837702512741089,\n",
              " '902.196.12': 0.34233608841896057,\n",
              " '902.257.93': 0.9628078937530518,\n",
              " '902.275.94': 0.9810299873352051,\n",
              " '902.276.31': 0.9281483888626099,\n",
              " '902.290.17': 0.6617861390113831,\n",
              " '902.364.90': 0.3928256630897522,\n",
              " '902.376.54': 0.8502827286720276,\n",
              " '902.387.38': 0.9730098843574524,\n",
              " '902.414.63': 0.973770797252655,\n",
              " '902.436.31': 0.9487294554710388,\n",
              " '902.530.74': 0.985727846622467,\n",
              " '902.535.35': 0.7134117484092712,\n",
              " '902.567.27': 0.4521166682243347,\n",
              " '902.591.89': 0.7939578890800476,\n",
              " '902.620.78': 0.32715708017349243,\n",
              " '902.621.96': 0.35039353370666504,\n",
              " '902.637.04': 0.6413240432739258,\n",
              " '902.642.99': 0.9046090245246887,\n",
              " '902.649.68': 0.983939528465271,\n",
              " '902.679.95': 0.5700643658638,\n",
              " '902.696.02': 0.6488868594169617,\n",
              " '902.796.15': 0.8790847063064575,\n",
              " '902.797.19': 0.9357359409332275,\n",
              " '902.807.08': 0.8741294145584106,\n",
              " '902.818.59': 0.8813104629516602,\n",
              " '902.821.80': 0.8907523155212402,\n",
              " '902.824.39': 0.6480693221092224,\n",
              " '902.857.82': 0.9742224812507629,\n",
              " '902.868.47': 0.40130048990249634,\n",
              " '902.902.03': 0.9771829843521118,\n",
              " '902.921.03': 0.3604595363140106,\n",
              " '902.947.67': 0.6533474922180176,\n",
              " '902.958.42': 0.9736620783805847,\n",
              " '902.962.62': 0.32587730884552,\n",
              " '902.965.87': 0.45697781443595886,\n",
              " '902.976.57': 0.38984477519989014,\n",
              " '902.978.36': 0.36370572447776794,\n",
              " '902.994.11': 0.9523471593856812,\n",
              " '903.007.25': 0.34326091408729553,\n",
              " '903.012.25': 0.9119653105735779,\n",
              " '903.022.39': 0.4169989824295044,\n",
              " '903.023.24': 0.984458327293396,\n",
              " '903.067.32': 0.6805562973022461,\n",
              " '903.114.94': 0.807939887046814,\n",
              " '903.116.15': 0.7240945100784302,\n",
              " '903.132.28': 0.9917377829551697,\n",
              " '903.170.85': 0.5602371096611023,\n",
              " '903.279.18': 0.4629407525062561,\n",
              " '903.292.67': 0.5051243305206299,\n",
              " '903.296.58': 0.6497226357460022,\n",
              " '903.324.58': 0.968234658241272,\n",
              " '990.062.82': 0.6852245330810547,\n",
              " '990.066.06': 0.7466139793395996,\n",
              " '990.212.87': 0.5947532057762146,\n",
              " '990.403.42': 0.4629458487033844,\n",
              " '991.278.11': 0.4366496503353119,\n",
              " '991.553.52': 0.8042571544647217,\n",
              " '991.554.08': 0.9874413013458252,\n",
              " '991.805.49': 0.9324579238891602}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck31vLsy5Mth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}