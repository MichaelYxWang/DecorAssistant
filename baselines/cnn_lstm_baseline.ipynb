{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe1FTawUffC5"
      },
      "source": [
        "# Hyperparams and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JvNGzXEy7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761d5df8-4c2d-4622-b73f-bd23339f53f4"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 17:19:48--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.204.104\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.204.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.3MB/s    in 99s     \n",
            "\n",
            "2021-10-26 17:21:28 (15.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLocJQsNqoT"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "random.seed(517)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwYoWs8Ns-S"
      },
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/DecorAssist/\"\n",
        "DATASET_DIR = ROOT_DIR + \"IKEA/text_data/\"\n",
        "IMAGES_DIR = ROOT_DIR + \"IKEA/images/all_items/\"\n",
        "\n",
        "# Global Parameter Variables\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "NUM_WORDS_TOKENIZER = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 16\n",
        "POSITIVE_SIZE = 300 # We might only use a subset of the positive pairs\n",
        "TRAIN_TEST_RATIO = 0.33\n",
        "\n",
        "# Model Hyperparameters\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LEARNING_RATE = 2e-4 # 0.001\n",
        "HIDDEN_DIM = 128 # 64\n",
        "N_LAYERS = 4 # 2\n",
        "EPOCHS = 10\n",
        "CLIP = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYbhsNmNgzV"
      },
      "source": [
        "def preprocess_img(path):\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.resize(img, (75, 115))\n",
        "  img = img.astype(np.float32) / 255\n",
        "  return img\n",
        "\n",
        "\n",
        "def read_pickle(fn):\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)\n",
        "  \n",
        "\n",
        "def random_negative_sampling(all_positive_pairs):\n",
        "  num_examples = len(all_positive_pairs) * 2\n",
        "  all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "  negative_count = 0\n",
        "  selected_negative_pairs = []\n",
        "  while negative_count < num_examples / 2:\n",
        "    random_pair = tuple(random.sample(all_item_ids, 2))\n",
        "    if random_pair in all_positive_pairs:\n",
        "      continue\n",
        "    else:\n",
        "      selected_negative_pairs.append(random_pair)\n",
        "      negative_count += 1\n",
        "  return selected_negative_pairs\n",
        "\n",
        "\n",
        "def get_embedding_matrix(word_index, weights_path=\"/content/GoogleNews-vectors-negative300.bin\"):\n",
        "  word2vecDict = KeyedVectors.load_word2vec_format(weights_path, binary=True)\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "  for word in word2vecDict.wv.vocab:\n",
        "    embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "  print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
        "        \n",
        "  embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
        "\n",
        "  embeddedCount = 0\n",
        "  for word, i in word_index.items():\n",
        "    i-=1\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "  print(\"total embedded:\", embeddedCount, \"common words\")\n",
        "  del(embeddings_index)\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRFmhR8RGJJ"
      },
      "source": [
        "# Build Train and Eval Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJPk6ZK5mSw-"
      },
      "source": [
        "#### Load raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaq9zR8RQRIp"
      },
      "source": [
        "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
        "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
        "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
        "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
        "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
        "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
        "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
        "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
        "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
        "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
        "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
        "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
        "\n",
        "# Some simple preprossing\n",
        "item_to_info = {key : value[\"type\"] + \" \" +\n",
        "                             value[\"desc\"]\n",
        "                       for key, value in item_property.items()} # remove view more info\n",
        "\n",
        "room_to_items = {}\n",
        "\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "\n",
        "  for room_url in room_url_list:\n",
        "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if room_id not in room_to_items:\n",
        "      room_to_items[room_id] = []\n",
        "    else:\n",
        "      room_to_items[room_id].append(item_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDaGXCr4mV0I"
      },
      "source": [
        "#### Construct positive and negative pairs\n",
        "\n",
        "For IR-style problem, seen and unseen can be tricky. We need to discuss whether unseen means \"unseen pairs\" or \"unseen image or text\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0MZqyUWmN87"
      },
      "source": [
        "all_positive_pairs = []\n",
        "for room, item_id_list in room_to_items.items():\n",
        "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
        "  all_positive_pairs += pairs_for_current_room\n",
        "\n",
        "all_positive_pairs = all_positive_pairs[:POSITIVE_SIZE]\n",
        "all_pairs = all_positive_pairs + random_negative_sampling(all_positive_pairs)\n",
        "y = np.array([np.array([1, 0]) for _ in range(len(all_positive_pairs))] + \n",
        "             [np.array([0, 1]) for _ in range(len(all_positive_pairs))])\n",
        "train_pairs, val_paris, y_train, y_val = train_test_split(all_pairs, y, test_size=TRAIN_TEST_RATIO, random_state=517)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRaA0zbGmeyt"
      },
      "source": [
        "#### Build PyTorch dataloader for train/val image/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckQwq5lIN82"
      },
      "source": [
        "train_image_premise_id_list = [x[0] for x in train_pairs]\n",
        "train_image_hypothesis_id_list = [x[1] for x in train_pairs]\n",
        "X_train_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_premise_id_list)))\n",
        "X_train_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), train_image_hypothesis_id_list)))\n",
        "X_train_image_premise = np.reshape(X_train_image_premise, (X_train_image_premise.shape[0], 3, 75, 115))\n",
        "X_train_image_hypothesis = np.reshape(X_train_image_hypothesis, (X_train_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai_e-I4NiFyk"
      },
      "source": [
        "val_image_premise_id_list = [x[0] for x in val_paris]\n",
        "val_image_hypothesis_id_list = [x[1] for x in val_paris]\n",
        "X_val_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_premise_id_list)))\n",
        "X_val_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), val_image_hypothesis_id_list)))\n",
        "X_val_image_premise = np.reshape(X_val_image_premise, (X_val_image_premise.shape[0], 3, 75, 115))\n",
        "X_val_image_hypothesis = np.reshape(X_val_image_hypothesis, (X_val_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO1ZkgcUK1m7",
        "outputId": "d087a8c0-d1df-4dc1-d28c-0a6d9ca13080"
      },
      "source": [
        "train_premise_texts = [item_to_info[id] for id in train_image_premise_id_list]\n",
        "train_hypothesis_texts = [item_to_info[id] for id in train_image_hypothesis_id_list]\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
        "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
        "WORD_INDEX = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
        "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
        "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
        "\n",
        "X_train_text_premise = tokenizer.texts_to_sequences(train_premise_texts)\n",
        "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_train_text_hypothesis = tokenizer.texts_to_sequences(train_hypothesis_texts)\n",
        "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 288 unique tokens.\n",
            "Max len: 100\n",
            "Loaded 3000000 word vectors.\n",
            "total embedded: 272 common words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAakls6kMkX"
      },
      "source": [
        "val_premise_texts = [item_to_info[id] for id in val_image_premise_id_list]\n",
        "val_hypothesis_texts = [item_to_info[id] for id in val_image_hypothesis_id_list]\n",
        "\n",
        "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
        "X_val_text_premise = tokenizer.texts_to_sequences(val_premise_texts)\n",
        "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_val_text_hypothesis = tokenizer.texts_to_sequences(val_hypothesis_texts)\n",
        "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vm9RcV-MWk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692c346f-b0ed-49ef-9e96-c984cc06f065"
      },
      "source": [
        "img_train_data = TensorDataset(torch.from_numpy(X_train_image_premise), torch.from_numpy(X_train_image_hypothesis), torch.from_numpy(y_train))\n",
        "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise), torch.from_numpy(X_train_text_hypothesis), torch.from_numpy(y_train))\n",
        "\n",
        "img_val_data = TensorDataset(torch.from_numpy(X_val_image_premise), torch.from_numpy(X_val_image_hypothesis), torch.from_numpy(y_val))\n",
        "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise), torch.from_numpy(X_val_text_hypothesis), torch.from_numpy(y_val))\n",
        "\n",
        "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
        "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
        "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_train_loader), len(img_train_loader))\n",
        "print(len(text_val_loader), len(img_val_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 26\n",
            "13 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW73L3zZNe1e"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7w-DGAxyiZ"
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "\n",
        "    # LSTM for the text overview\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # CNN for the posters\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.max_pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "    self.max_pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "    self.max_pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
        "    self.max_pool4 = nn.MaxPool2d(2)\n",
        "    self.cnn_dropout = nn.Dropout(0.1)\n",
        "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
        "\n",
        "    # Concat layer for the combined feature space\n",
        "    # self.combined_fc1 = nn.Linear(640, 256)\n",
        "    self.combined_fc1 = nn.Linear(640*2, 256)\n",
        "    self.combined_fc2 = nn.Linear(256, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def lstm_encoder(self, lstm_inp):\n",
        "    batch_size = lstm_inp.size(0)\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    lstm_inp = lstm_inp.long()\n",
        "    embeds = self.emb(lstm_inp)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = self.dropout(lstm_out[:, -1])\n",
        "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
        "    return lstm_out\n",
        "\n",
        "  def cnn_encoder(self, cnn_inp):\n",
        "    x = F.relu(self.conv1(cnn_inp))\n",
        "    x = self.max_pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.max_pool2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.max_pool3(x)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = self.max_pool4(x)\n",
        "    x = x.view(-1, 5 * 2 * 128)\n",
        "    x = self.cnn_dropout(x)\n",
        "    cnn_out = F.relu(self.cnn_fc(x))\n",
        "    return cnn_out\n",
        "\n",
        "  def forward(self, lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2):\n",
        "    cnn_out1, cnn_out2 = self.cnn_encoder(cnn_inp1), self.cnn_encoder(cnn_inp2)\n",
        "    lstm_out1, lstm_out2 = self.lstm_encoder(lstm_inp1), self.lstm_encoder(lstm_inp2)\n",
        "    combined_inp = torch.cat((cnn_out1, cnn_out2, lstm_out1, lstm_out2), 1)\n",
        "    x_comb = F.relu(self.combined_fc1(combined_inp))\n",
        "    x_comb = F.relu(self.combined_fc2(x_comb))\n",
        "    # out = torch.sigmoid(self.output_fc(x_comb))\n",
        "    out = self.softmax(self.output_fc(x_comb))\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    return hidden"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2rumtEQYv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bb45ee-4835-4a44-96d4-dafc6108407a"
      },
      "source": [
        "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
        "\n",
        "model = CNN_LSTM(len(WORD_INDEX)+1, WORD2VEC_EMBEDDING_MATRIX, HIDDEN_DIM, N_LAYERS, y.shape[1])\n",
        "model.to(DEVICE)\n",
        "print(\"Model Architecture {}\\n\".format(model))\n",
        "\n",
        "lr= LEARNING_RATE\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "print(\"Training Started...\")\n",
        "model.train()\n",
        "for i in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "    \n",
        "  for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE), lstm_labels.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc\n",
        "      total_loss_train += loss.item()\n",
        "  \n",
        "  train_acc = total_acc_train/len(text_train_loader)\n",
        "  train_loss = total_loss_train/len(text_train_loader)\n",
        "  model.eval()\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  with torch.no_grad():\n",
        "    for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE), lstm_labels.to(DEVICE)\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
        "      model.zero_grad()\n",
        "      output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_val += acc\n",
        "      total_loss_val += val_loss.item()\n",
        "  val_acc = total_acc_val/len(text_val_loader)\n",
        "  val_loss = total_loss_val/len(text_val_loader)\n",
        "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "  model.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device: cuda\n",
            "\n",
            "Model Architecture CNN_LSTM(\n",
            "  (emb): Embedding(289, 300)\n",
            "  (lstm): LSTM(300, 128, num_layers=4, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (lstm_fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (cnn_fc): Linear(in_features=1280, out_features=512, bias=True)\n",
            "  (combined_fc1): Linear(in_features=1280, out_features=256, bias=True)\n",
            "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (output_fc): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "Training Started...\n",
            "Epoch 1: train_loss: 0.6941 train_acc: 0.4995 | val_loss: 0.6931 val_acc: 0.5000\n",
            "Epoch 2: train_loss: 0.6933 train_acc: 0.4999 | val_loss: 0.6932 val_acc: 0.5000\n",
            "Epoch 3: train_loss: 0.6931 train_acc: 0.5000 | val_loss: 0.6931 val_acc: 0.5000\n",
            "Epoch 4: train_loss: 0.6916 train_acc: 0.5008 | val_loss: 0.6920 val_acc: 0.5008\n",
            "Epoch 5: train_loss: 0.6846 train_acc: 0.5053 | val_loss: 0.6903 val_acc: 0.5040\n",
            "Epoch 6: train_loss: 0.6715 train_acc: 0.5172 | val_loss: 0.6974 val_acc: 0.5108\n",
            "Epoch 7: train_loss: 0.6506 train_acc: 0.5368 | val_loss: 0.6955 val_acc: 0.5169\n",
            "Epoch 8: train_loss: 0.6292 train_acc: 0.5577 | val_loss: 0.6967 val_acc: 0.5268\n",
            "Epoch 9: train_loss: 0.6132 train_acc: 0.5780 | val_loss: 0.7088 val_acc: 0.5283\n",
            "Epoch 10: train_loss: 0.5928 train_acc: 0.5869 | val_loss: 0.6933 val_acc: 0.5547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRkG-JWjfCj"
      },
      "source": [
        "# Ranker\n",
        "- For image 1 image 2 text 1 text 2 as input, we generate a score ~(0,1).\n",
        "- Search engine: only given one image and one text, compute the score for this item with all the other items in the pool. E.g, if we have num_items, the score matrix will be (num_items, ). Find predicted top 5 or top 10.\n",
        "- Haocheng's ranking evaluation: for a specific item, we know all the ground truth items that are in the same room as this specific item. \n",
        "\n",
        "The output from yuanxin: {\"802.903.9234.923\" : score, xxx}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOlqTLXknYN"
      },
      "source": [
        "def single_pair_inference(premise_image_path, hypothesis_image_path, premise_text, hypothesis_text, model, tokenizer, threshold, do_plot=False):\n",
        "  premise_sequence = tokenizer.texts_to_sequences([premise_text])\n",
        "  premise_sequence = pad_sequences(premise_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  hypothesis_sequence = tokenizer.texts_to_sequences([hypothesis_text])\n",
        "  hypothesis_sequence = pad_sequences(hypothesis_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  image_premise, image_hypothesis = preprocess_img(premise_image_path), preprocess_img(hypothesis_image_path)\n",
        "\n",
        "  if do_plot:\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    ax1 = fig.add_subplot(2,2,1)\n",
        "    ax1.imshow(image_premise)\n",
        "    ax2 = fig.add_subplot(2,2,2)\n",
        "    ax2.imshow(image_hypothesis)\n",
        "    print(\"Left item description ------ {}\".format(premise_text))\n",
        "    print(\"Right item description ------  {}\".format(hypothesis_text))\n",
        "\n",
        "\n",
        "  image_premise = np.reshape(image_premise, (1, 3, 75, 115))\n",
        "  image_hypothesis = np.reshape(image_hypothesis, (1, 3, 75, 115))\n",
        "\n",
        "  img_data = TensorDataset(torch.from_numpy(image_premise), torch.from_numpy(image_hypothesis))\n",
        "  text_data = TensorDataset(torch.from_numpy(premise_sequence), torch.from_numpy(hypothesis_sequence))\n",
        "  \n",
        "  text_loader = DataLoader(text_data, batch_size=1)\n",
        "  img_loader = DataLoader(img_data, batch_size=1)\n",
        "\n",
        "  for lstm, cnn in zip(text_loader, img_loader):\n",
        "    lstm_inp1, lstm_inp2 = lstm\n",
        "    cnn_inp1, cnn_inp2 = cnn\n",
        "    lstm_inp1, lstm_inp2 = lstm_inp1.to(DEVICE), lstm_inp2.to(DEVICE)\n",
        "    cnn_inp1, cnn_inp2 = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "\n",
        "  score = output.squeeze().cpu().detach().numpy().tolist()[0]\n",
        "  if score > threshold:\n",
        "    return \"Positive\", score\n",
        "  else:\n",
        "    return \"Negative\", 1 - score\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "ft7vTrP2EEGa",
        "outputId": "b523211b-2b9e-4e0b-c0fd-40017ea33aba"
      },
      "source": [
        "random_index = random.randint(0, len(train_image_premise_id_list))\n",
        "image_id_1 = train_image_premise_id_list[random_index]\n",
        "image_id_2 = train_image_hypothesis_id_list[random_index]\n",
        "text_1 = train_premise_texts[random_index]\n",
        "text_2 = train_hypothesis_texts[random_index]\n",
        "prediction, confidence = single_pair_inference(premise_image_path=IMAGES_DIR + image_id_1 + \".jpg\",\n",
        "          hypothesis_image_path=IMAGES_DIR + image_id_2 + \".jpg\",\n",
        "          premise_text=text_1,\n",
        "          hypothesis_text=text_2,\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          threshold=0.4,\n",
        "          do_plot=True)\n",
        "print(\"Actual Label for this pair is\", \"Positive\" if y_train[random_index].tolist() == [1, 0] else \"Negative\")\n",
        "print(\"The prediction for this pair is\", prediction, \"with confidence\", confidence)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left item description ------ Children's table \n",
            "Right item description ------  Bed storage box Imagine having your own bed storage box that looks like a suitcase with both a handle and an address holder. Perfect for both toys and secrets.\n",
            "Actual Label for this pair is Positive\n",
            "The prediction for this pair is Positive with confidence 0.5573071837425232\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0qmvz0xpmci"
      },
      "source": [
        "def ranker(input_image_id, input_description, item_id_pool, model, tokenizer, threshold=0.3, top_n=5, do_plot=False):\n",
        "  # for id in item_id_pool, get image, get description, input to single_pair_inference\n",
        "  # \n",
        "  input_image_path = IMAGES_DIR + input_image_id + \".jpg\"\n",
        "  item_id_to_score = {}\n",
        "\n",
        "  for item_id in item_id_pool:\n",
        "    candidate_image_path = IMAGES_DIR + item_id + \".jpg\"\n",
        "    candidate_description = item_to_info[item_id]\n",
        "    output_prediction, output_confidence = single_pair_inference(premise_image_path=input_image_path,\n",
        "          hypothesis_image_path=candidate_image_path,\n",
        "          premise_text=input_description,\n",
        "          hypothesis_text=candidate_description,\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          threshold=threshold,\n",
        "          do_plot=False)\n",
        "    if output_prediction == \"Positive\":\n",
        "      item_id_to_score[item_id] = output_confidence\n",
        "    else:\n",
        "      continue\n",
        "  return item_id_to_score"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfYJofZAOng9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnuz8ilHN0Fv",
        "outputId": "12318412-264a-4fd4-ea34-d5dbbf287562"
      },
      "source": [
        "random_index = random.randint(0, len(train_image_premise_id_list))\n",
        "image_id = train_image_premise_id_list[random_index]\n",
        "input_description = item_to_info[image_id]\n",
        "all_item_ids = list(set([x[0] for x in all_positive_pairs] + [x[1] for x in all_positive_pairs]))\n",
        "\n",
        "ranker(image_id, input_description, all_item_ids, model, tokenizer)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'001.660.95': 0.30868789553642273,\n",
              " '003.218.69': 0.3463829755783081,\n",
              " '003.288.37': 0.7760784029960632,\n",
              " '101.327.88': 0.3226427733898163,\n",
              " '102.628.31': 0.6723896861076355,\n",
              " '200.474.50': 0.8119797110557556,\n",
              " '201.932.67': 0.640713095664978,\n",
              " '203.086.97': 0.4446046054363251,\n",
              " '203.323.53': 0.8891764283180237,\n",
              " '301.933.18': 0.7805899381637573,\n",
              " '302.980.23': 0.5240936875343323,\n",
              " '303.288.31': 0.7822014093399048,\n",
              " '403.288.40': 0.7499198317527771,\n",
              " '491.234.05': 0.4542231559753418,\n",
              " '501.158.62': 0.5011304616928101,\n",
              " '501.711.22': 0.6947008967399597,\n",
              " '502.675.58': 0.4075872302055359,\n",
              " '502.831.05': 0.5570210218429565,\n",
              " '503.323.23': 0.412087082862854,\n",
              " '503.334.07': 0.767254114151001,\n",
              " '503.334.12': 0.3457525372505188,\n",
              " '600.940.72': 0.7108875513076782,\n",
              " '603.303.47': 0.3702891170978546,\n",
              " '700.914.12': 0.5345611572265625,\n",
              " '703.323.55': 0.8891229033470154,\n",
              " '802.290.08': 0.3197900950908661,\n",
              " '802.538.09': 0.7274831533432007,\n",
              " '802.962.48': 0.44906315207481384,\n",
              " '803.292.44': 0.6919664144515991,\n",
              " '803.324.68': 0.7207645773887634,\n",
              " '901.766.41': 0.48255327343940735,\n",
              " '902.257.93': 0.49876880645751953,\n",
              " '902.290.17': 0.35109320282936096,\n",
              " '902.962.62': 0.8294358253479004,\n",
              " '902.978.36': 0.7294247150421143,\n",
              " '902.978.41': 0.7143055200576782,\n",
              " '903.296.58': 0.539664089679718}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck31vLsy5Mth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}