{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JvNGzXEy7H2",
        "outputId": "5685f9de-f212-40e6-cbfb-ebedc00a9570"
      },
      "source": [
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "# !gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 19:28:21--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.170.88\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.170.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  80.1MB/s    in 21s     \n",
            "\n",
            "2021-10-20 19:28:43 (74.1 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLocJQsNqoT"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwYoWs8Ns-S"
      },
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/DecorAssist/\"\n",
        "DATASET_DIR = ROOT_DIR + \"IKEA/text_data/\"\n",
        "IMAGES_DIR = ROOT_DIR + \"IKEA/images/all_items/\"\n",
        "\n",
        "# Global Parameter Variables\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "NUM_WORDS_TOKENIZER = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYbhsNmNgzV"
      },
      "source": [
        "def preprocess_img(path):\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.resize(img, (75, 115))\n",
        "  img = img.astype(np.float32) / 255\n",
        "  return img\n",
        "\n",
        "def read_pickle(fn):\n",
        "\twith open(fn, \"rb\") as f:\n",
        "\t\treturn pickle.load(f)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRFmhR8RGJJ"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaq9zR8RQRIp"
      },
      "source": [
        "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
        "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
        "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
        "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
        "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
        "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
        "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
        "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
        "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
        "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
        "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
        "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
        "\n",
        "# Some simple preprossing\n",
        "item_to_info = {key : value[\"type\"] + \" \" +\n",
        "                             value[\"desc\"]\n",
        "                       for key, value in item_property.items()}\n",
        "\n",
        "room_to_items = {}\n",
        "\n",
        "for item_url, room_url_list in item_to_rooms_map.items():\n",
        "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "\n",
        "  for room_url in room_url_list:\n",
        "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
        "    if room_id not in room_to_items:\n",
        "      room_to_items[room_id] = []\n",
        "    else:\n",
        "      room_to_items[room_id].append(item_id)\n",
        "\n",
        "all_positive_pairs = []\n",
        "for room, item_id_list in room_to_items.items():\n",
        "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2))\n",
        "  all_positive_pairs += pairs_for_current_room\n",
        "\n",
        "\n",
        "train_pairs = all_positive_pairs[500:650]\n",
        "val_pairs = train_pairs"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uckQwq5lIN82"
      },
      "source": [
        "image_premise_id_list = [x[0] for x in train_pairs]\n",
        "image_hypothesis_id_list = [x[1] for x in train_pairs]\n",
        "X_image_premise = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), image_premise_id_list)))\n",
        "X_image_hypothesis = np.array(list(map(lambda image_id: preprocess_img(IMAGES_DIR + image_id + \".jpg\"), image_hypothesis_id_list)))\n",
        "X_image_premise = np.reshape(X_image_premise, (X_image_premise.shape[0], 3, 75, 115))\n",
        "X_image_hypothesis = np.reshape(X_image_hypothesis, (X_image_hypothesis.shape[0], 3, 75, 115))"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf6cBgA-Lpk8"
      },
      "source": [
        "y = np.array([np.array([0, 1]) for _ in range(len(train_pairs))])"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO1ZkgcUK1m7",
        "outputId": "2f3950ef-96be-43f3-ac58-335e2b45425c"
      },
      "source": [
        "premise_texts = [item_to_info[id] for id in image_premise_id_list]\n",
        "hypothesis_texts = [item_to_info[id] for id in image_hypothesis_id_list]\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
        "tokenizer.fit_on_texts(premise_texts + hypothesis_texts)\n",
        "WORD_INDEX = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
        "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_text_premise = tokenizer.texts_to_sequences(premise_texts)\n",
        "X_text_premise = pad_sequences(X_text_premise, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_text_hypothesis = tokenizer.texts_to_sequences(hypothesis_texts)\n",
        "X_text_hypothesis = pad_sequences(X_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 217 unique tokens.\n",
            "Max len: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vm9RcV-MWk7",
        "outputId": "44ff64cd-3199-4b4c-dda8-58a34760f0fa"
      },
      "source": [
        "img_train_data = TensorDataset(torch.from_numpy(X_image_premise), torch.from_numpy(X_image_hypothesis), torch.from_numpy(y))\n",
        "text_train_data = TensorDataset(torch.from_numpy(X_text_premise), torch.from_numpy(X_text_hypothesis), torch.from_numpy(y))\n",
        "\n",
        "img_val_data = img_train_data\n",
        "text_val_data = text_train_data\n",
        "\n",
        "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
        "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
        "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(len(text_train_loader), len(img_train_loader))\n",
        "print(len(text_val_loader), len(img_val_loader))\n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 10\n",
            "10 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08RZVeuuMeH9"
      },
      "source": [
        "def get_embedding_matrix(word_index):\n",
        "  word2vecDict = KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "  embed_size = 300\n",
        "  embeddings_index = dict()\n",
        "  for word in word2vecDict.wv.vocab:\n",
        "    embeddings_index[word] = word2vecDict.word_vec(word)\n",
        "  print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
        "        \n",
        "  embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
        "\n",
        "  embeddedCount = 0\n",
        "  for word, i in word_index.items():\n",
        "    i-=1\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      embeddedCount+=1\n",
        "  print(\"total embedded:\", embeddedCount, \"common words\")\n",
        "  del(embeddings_index)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y076kVpzMeJ3",
        "outputId": "d73e5c29-2ca2-49a0-8c19-e84eaa7a3035"
      },
      "source": [
        "word2vec_embedding_matrix = get_embedding_matrix(WORD_INDEX)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3000000 word vectors.\n",
            "total embedded: 204 common words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW73L3zZNe1e"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7w-DGAxyiZ"
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "\n",
        "    # LSTM for the text overview\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # CNN for the posters\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.max_pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "    self.max_pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "    self.max_pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
        "    self.max_pool4 = nn.MaxPool2d(2)\n",
        "    self.cnn_dropout = nn.Dropout(0.1)\n",
        "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
        "\n",
        "    # Concat layer for the combined feature space\n",
        "    # self.combined_fc1 = nn.Linear(640, 256)\n",
        "    self.combined_fc1 = nn.Linear(640*2, 256)\n",
        "    self.combined_fc2 = nn.Linear(256, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "\n",
        "  def forward(self, lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2):\n",
        "      # LSTM Forward\n",
        "      batch_size = lstm_inp1.size(0)\n",
        "      hidden = self.init_hidden(batch_size)\n",
        "\n",
        "      lstm_inp1 = lstm_inp1.long()\n",
        "      lstm_inp2 = lstm_inp2.long()\n",
        "      embeds1 = self.emb(lstm_inp1)\n",
        "      embeds2 = self.emb(lstm_inp2)\n",
        "      lstm_out1, hidden = self.lstm(embeds1, hidden)\n",
        "      lstm_out1 = self.dropout(lstm_out1[:, -1])\n",
        "      lstm_out1 = F.relu(self.lstm_fc(lstm_out1))\n",
        "\n",
        "      lstm_out2, hidden = self.lstm(embeds2, hidden)\n",
        "      lstm_out2 = self.dropout(lstm_out2[:, -1])\n",
        "      lstm_out2 = F.relu(self.lstm_fc(lstm_out2))\n",
        "\n",
        "      # CNN Forward\n",
        "      x1 = F.relu(self.conv1(cnn_inp1))\n",
        "      x1 = self.max_pool1(x1)\n",
        "      x1 = F.relu(self.conv2(x1))\n",
        "      x1 = self.max_pool2(x1)\n",
        "      x1 = F.relu(self.conv3(x1))\n",
        "      x1 = self.max_pool3(x1)\n",
        "      x1 = F.relu(self.conv4(x1))\n",
        "      x1 = self.max_pool4(x1)\n",
        "      x1 = x1.view(-1, 5*2*128)\n",
        "      x1 = self.cnn_dropout(x1)\n",
        "      cnn_out1 = F.relu(self.cnn_fc(x1))\n",
        "\n",
        "      x2 = F.relu(self.conv1(cnn_inp2))\n",
        "      x2 = self.max_pool1(x2)\n",
        "      x2 = F.relu(self.conv2(x2))\n",
        "      x2 = self.max_pool2(x2)\n",
        "      x2 = F.relu(self.conv3(x2))\n",
        "      x2 = self.max_pool3(x2)\n",
        "      x2 = F.relu(self.conv4(x2))\n",
        "      x2 = self.max_pool4(x2)\n",
        "      x2 = x2.view(-1, 5*2*128)\n",
        "      x2 = self.cnn_dropout(x2)\n",
        "      cnn_out2 = F.relu(self.cnn_fc(x2))\n",
        "\n",
        "      combined_inp = torch.cat((cnn_out1, cnn_out2, lstm_out1, lstm_out2), 1)\n",
        "      x_comb = F.relu(self.combined_fc1(combined_inp))\n",
        "      x_comb = F.relu(self.combined_fc2(x_comb))\n",
        "      out = torch.sigmoid(self.output_fc(x_comb))\n",
        "\n",
        "      return out\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    return hidden"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPz2qovGyBjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614d48b4-cff0-4f5e-818e-d6cc72cb23c2"
      },
      "source": [
        "vocab_size = len(WORD_INDEX)+1\n",
        "output_size = y.shape[1]\n",
        "embedding_dim = 300\n",
        "hidden_dim = 64\n",
        "n_layers = 2\n",
        "print(output_size)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = CNN_LSTM(vocab_size, word2vec_embedding_matrix, hidden_dim, n_layers, output_size)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.001\n",
        "# criterion = nn.MultiLabelSoftMarginLoss()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "cpu\n",
            "CNN_LSTM(\n",
            "  (emb): Embedding(218, 300)\n",
            "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (cnn_fc): Linear(in_features=1280, out_features=512, bias=True)\n",
            "  (combined_fc1): Linear(in_features=1280, out_features=256, bias=True)\n",
            "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (output_fc): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX2rumtEQYv8",
        "outputId": "83a299aa-f46d-4598-ade9-8dc07ed927e0"
      },
      "source": [
        "epochs = 1\n",
        "clip = 5\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "    \n",
        "  for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "    lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n",
        "    cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "    loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc\n",
        "      total_loss_train += loss.item()\n",
        "  \n",
        "  train_acc = total_acc_train/len(text_train_loader)\n",
        "  train_loss = total_loss_train/len(text_train_loader)\n",
        "  model.eval()\n",
        "  total_acc_val = 0\n",
        "  total_loss_val = 0\n",
        "  with torch.no_grad():\n",
        "    for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
        "      lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n",
        "      cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n",
        "      model.zero_grad()\n",
        "      output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
        "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
        "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_val += acc\n",
        "      total_loss_val += val_loss.item()\n",
        "  val_acc = total_acc_val/len(text_val_loader)\n",
        "  val_loss = total_loss_val/len(text_val_loader)\n",
        "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "  model.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 0.0000 train_acc: 1.0000 | val_loss: 0.0000 val_acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TEh8Rj8ScoF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}