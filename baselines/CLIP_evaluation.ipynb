{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "D2DkS5ep8xfs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2DkS5ep8xfs",
    "outputId": "49f31ee7-e576-4204-ee94-60894a0de604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a03101-8cdb-4706-bdc4-92ead9aaa9a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94a03101-8cdb-4706-bdc4-92ead9aaa9a0",
    "outputId": "40e4f47f-262c-4ac0-ea74-6d1716a0bbab",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
      "Building wheels for collected packages: ftfy\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=f8b8ed4a3f3b8ca7bee1bb49fadc9e96e6783eeffba1d534a6da51ada7104fce\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
      "Successfully built ftfy\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.0.3\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-opzxh8dq\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-opzxh8dq\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.0.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.10.0+cu111)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369090 sha256=f7f07a21eb16814246068e356f5083770e660ab17d62934e29490e5bb741ddc6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3erb0ltk/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d596d512-a70e-4a4f-99ec-d9eb0393a482",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d596d512-a70e-4a4f-99ec-d9eb0393a482",
    "outputId": "fa2e2930-b4af-4d06-f9a0-a7bae6e8bbd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.9.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import emblaze\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "assert torch.__version__.split(\".\") >= [\"1\", \"7\", \"1\"], \"PyTorch 1.7.1 or later is required\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccf1d7e-469c-4876-936d-0c799f8ac78b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eccf1d7e-469c-4876-936d-0c799f8ac78b",
    "outputId": "da678876-8560-4207-b4de-3d82d2fa915b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78b6ff5-181a-44a6-8c68-0ff48b3f56a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f78b6ff5-181a-44a6-8c68-0ff48b3f56a7",
    "outputId": "3d13b47b-9d27-4159-f789-735aee0da54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "W38-m4DL31v4",
   "metadata": {
    "id": "W38-m4DL31v4"
   },
   "outputs": [],
   "source": [
    "# CLIP has some layers explicitly parameterized using fp16 values. We need to\n",
    "# convert them back to fp32 in order to use automatic mixed-precision training\n",
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp32\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp32(l):\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.float()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.float()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.float()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr = getattr(l, name)\n",
    "                if attr is not None:\n",
    "                    attr.data = attr.data.float()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp32)\n",
    "\n",
    "convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vvJNMnx-9pVe",
   "metadata": {
    "id": "vvJNMnx-9pVe"
   },
   "outputs": [],
   "source": [
    "class CLIPIKEA(nn.Module):\n",
    "    def __init__(self, clip_model, embedding_dim, n_out):\n",
    "        super(CLIPIKEA, self).__init__()\n",
    "\n",
    "        self.clip_model = clip_model\n",
    "        self.combined_fc1 = nn.Linear(embedding_dim * 4, 256)\n",
    "        self.output_fc = nn.Linear(256, n_out)\n",
    "\n",
    "    def forward(self, txt_1, txt_2, img_1, img_2):\n",
    "        batch_size = txt_1.size(0)\n",
    "\n",
    "        with autocast(enabled=False):\n",
    "            txt_emb_1 = self.clip_model.encode_text(txt_1)\n",
    "            txt_emb_2 = self.clip_model.encode_text(txt_2)\n",
    "            img_emb_1 = self.clip_model.encode_image(img_1)\n",
    "            img_emb_2 = self.clip_model.encode_image(img_2)\n",
    "\n",
    "        all_emb = torch.cat((txt_emb_1, txt_emb_2, img_emb_1, img_emb_2), 1)\n",
    "        x_comb = F.relu(self.combined_fc1(all_emb))\n",
    "        out = self.output_fc(x_comb)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45R7g0x8NihA",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "output_size = 1 # only output a single sigmoid value # y.shape[1]\n",
    "print(output_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "full_model = CLIPIKEA(model, 512, output_size)\n",
    "full_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9tNjmBYeon6",
   "metadata": {
    "id": "r9tNjmBYeon6",
    "tags": []
   },
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7DW6oxGPmC9-",
   "metadata": {
    "id": "7DW6oxGPmC9-"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"../dataset/\"\n",
    "DATASET_DIR = BASE_DIR + \"text_data/\"\n",
    "IMAGES_DIR = BASE_DIR + \"images/all_items/\"\n",
    "POSITIVE_SIZE = None # We might only use a subset of the positive pairs\n",
    "TRAIN_TEST_RATIO = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "qmCx_kVEmF-s",
   "metadata": {
    "id": "qmCx_kVEmF-s"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(path):\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.resize(img, (256, 256))\n",
    "  img = img.astype(np.float32) / 255\n",
    "  return img\n",
    "\n",
    "def read_pickle(fn):\n",
    "\twith open(fn, \"rb\") as f:\n",
    "\t\treturn pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abwrNXYP8ibS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abwrNXYP8ibS",
    "outputId": "87f87e2d-b55a-490f-c05e-0a9d376f90f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/890.333.75.jpg does not exist\n",
      "images/991.333.98.jpg does not exist\n",
      "images/990.612.97.jpg does not exist\n"
     ]
    }
   ],
   "source": [
    "def random_negative_sampling(products, all_positive_pairs, count=None, random_state=None):\n",
    "  selected_negative_pairs = []\n",
    "  if random_state is not None: random.seed(random_state)\n",
    "  while len(selected_negative_pairs) < (count or len(all_positive_pairs)):\n",
    "    random_pair = tuple(random.sample(products, 2))\n",
    "    if random_pair in all_positive_pairs:\n",
    "      continue\n",
    "    else:\n",
    "      selected_negative_pairs.append(random_pair)\n",
    "  return selected_negative_pairs\n",
    "  \n",
    "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
    "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
    "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
    "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
    "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
    "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
    "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
    "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
    "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
    "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
    "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
    "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
    "\n",
    "# Some simple preprossing\n",
    "item_to_info = {key : value[\"type\"] + \" \" +\n",
    "                             value[\"desc\"]\n",
    "                       for key, value in item_property.items()} # remove view more info\n",
    "\n",
    "room_to_items = {}\n",
    "\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "  if not os.path.exists(IMAGES_DIR + item_id + \".jpg\"):\n",
    "      print(item_url + \" does not exist\")\n",
    "      continue\n",
    "\n",
    "  for room_url in room_url_list:\n",
    "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if room_id not in room_to_items:\n",
    "      room_to_items[room_id] = [item_id]\n",
    "    else:\n",
    "      room_to_items[room_id].append(item_id)\n",
    "\n",
    "all_positive_pairs = set()\n",
    "for room, item_id_list in room_to_items.items():\n",
    "  pairs_for_current_room = list(itertools.combinations(room_to_items[room], 2)) # n choose 2\n",
    "  all_positive_pairs |= set(pairs_for_current_room)\n",
    "\n",
    "# if POSITIVE_SIZE is not None:\n",
    "#     sampled_positives = all_positive_pairs[:POSITIVE_SIZE] # Uncomment to subsample\n",
    "# else:\n",
    "#     sampled_positives = all_positive_pairs\n",
    "# all_pairs = sampled_positives + random_negative_sampling(all_positive_pairs, count=len(sampled_positives))\n",
    "# y = np.array([1 for _ in range(len(all_positive_pairs))] + \n",
    "#              [0 for _ in range(len(all_positive_pairs))])\n",
    "# train_pairs, val_pairs, y_train, y_val = train_test_split(all_pairs, y, test_size=TRAIN_TEST_RATIO, random_state=517)\n",
    "\n",
    "# # Shuffle now so batches are not all positive or all negative\n",
    "# train_indices = np.random.permutation(np.arange(len(train_pairs)))\n",
    "# train_pairs = [train_pairs[i] for i in train_indices]\n",
    "# y_train = y_train[train_indices]\n",
    "\n",
    "# val_indices = np.random.permutation(np.arange(len(val_pairs)))\n",
    "# val_pairs = [val_pairs[i] for i in val_indices]\n",
    "# y_val = y_val[val_indices]\n",
    "\n",
    "# len(train_pairs), len(val_pairs), y_train[:10], y_val[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "XOcblifyo3ri",
   "metadata": {
    "id": "XOcblifyo3ri"
   },
   "outputs": [],
   "source": [
    "# To read the validation sets only\n",
    "with open(BASE_DIR + \"val_data.pkl\", \"rb\") as file:\n",
    "    val_pairs, y_val = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43552922-fed5-41c1-8780-2f4bccbacb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional subsampling\n",
    "SUBSAMPLE_SIZE = 32\n",
    "val_pairs = val_pairs[:SUBSAMPLE_SIZE]\n",
    "y_val = y_val[:SUBSAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "jtQJWkJ0fEPT",
   "metadata": {
    "id": "jtQJWkJ0fEPT"
   },
   "outputs": [],
   "source": [
    "class FurnitureImagePairsDataset(Dataset):\n",
    "    \"\"\"Dataset containing pairs of furniture items.\"\"\"\n",
    "\n",
    "    def __init__(self, image_path, pairs, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_path (string): Path to the directory containing images.\n",
    "            pairs (list of tuples of strings): Pairs of image IDs to be used as training samples.\n",
    "            labels (array of integers): Labels for the training samples.\n",
    "        \"\"\"\n",
    "        super(FurnitureImagePairsDataset, self).__init__()\n",
    "        self.image_ids = list(set(x for pair in pairs for x in pair))\n",
    "        self.index_mapping = {image_id: i for i, image_id in enumerate(self.image_ids)}\n",
    "        self.images = [preprocess(Image.open(image_path + image_id + \".jpg\")) for image_id in tqdm.tqdm(self.image_ids)]\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if isinstance(idx, (list, tuple)):\n",
    "            x1, x2, y = zip(*[self[i] for i in idx])\n",
    "            return torch.stack(x1), torch.stack(x2), torch.from_numpy(np.array(y))\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "        return self.images[self.index_mapping[pair[0]]], self.images[self.index_mapping[pair[1]]], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "FQygJsUIOMeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQygJsUIOMeb",
    "outputId": "c43bde63-7efb-4ee5-b2be-ec8c09e1b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 148.36it/s]\n"
     ]
    }
   ],
   "source": [
    "product_ids = sorted(list(set(x for pair in val_pairs for x in pair)))\n",
    "val_images = torch.stack([preprocess(Image.open(IMAGES_DIR + image_id + \".jpg\")) for image_id in tqdm.tqdm(product_ids)])\n",
    "\n",
    "def tokenize(text):\n",
    "  try:\n",
    "      return clip.tokenize(text)\n",
    "  except:\n",
    "      return clip.tokenize(' '.join(text.split()[:50]))\n",
    "\n",
    "val_texts = torch.cat([tokenize(item_to_info[id]) for id in product_ids], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3Vm9RcV-MWk7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Vm9RcV-MWk7",
    "outputId": "d8c7ec3c-e9b4-4ec4-ddc9-833d9af0e9c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "val_data = TensorDataset(val_images, val_texts)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cc0a9-da72-4010-8a34-599a102c397d",
   "metadata": {},
   "source": [
    "# Embedding Comparison\n",
    "\n",
    "Let's get embeddings by CLIP at each training epoch and see how they change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d26c44c0-5cea-4f7f-9ae0-4d253c8493f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"clip_checkpoints/\"\n",
    "checkpoint_paths = [CHECKPOINT_DIR + fname for fname in [\n",
    "    \"text_image_update_initial_state.p\",\n",
    "    \"text_image_update_epoch_3_val_0.787_0.643.p\",\n",
    "    \"text_image_update_epoch_6_val_0.995_0.661.p\",\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68imyr2CN0G1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68imyr2CN0G1",
    "outputId": "8b6f7861-1e21-404b-dc84-250e1d628bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_checkpoints/text_image_update_initial_state.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:08<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 512) (62, 512)\n",
      "\n",
      "clip_checkpoints/text_image_update_epoch_3_val_0.787_0.643.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 512) (62, 512)\n",
      "\n",
      "clip_checkpoints/text_image_update_epoch_6_val_0.995_0.661.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 512) (62, 512)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_image_embeddings = []\n",
    "all_text_embeddings = []\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "    print(checkpoint_path)\n",
    "    state = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    full_model.load_state_dict(state)\n",
    "\n",
    "    full_model.eval()\n",
    "    image_embs = []\n",
    "    text_embs = []\n",
    "    with torch.no_grad():\n",
    "        for image_inp, text_inp in tqdm.tqdm(val_loader, total=len(val_loader)):\n",
    "            image_inp = image_inp.to(device)\n",
    "            text_inp = text_inp.to(device)\n",
    "            image_embs.append(full_model.clip_model.encode_image(image_inp).cpu().numpy())\n",
    "            text_embs.append(full_model.clip_model.encode_text(text_inp).cpu().numpy())\n",
    "    all_image_embeddings.append(np.concatenate(image_embs, axis=0))\n",
    "    all_text_embeddings.append(np.concatenate(text_embs, axis=0))\n",
    "    print(all_image_embeddings[-1].shape, all_text_embeddings[-1].shape)\n",
    "    print(\"\")\n",
    "    \n",
    "with open(CHECKPOINT_DIR + \"text_image_update_image_embeddings.pkl\", \"wb\") as file:\n",
    "    pickle.dump(all_image_embeddings, file)\n",
    "with open(CHECKPOINT_DIR + \"text_image_update_text_embeddings.pkl\", \"wb\") as file:\n",
    "    pickle.dump(all_text_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b83f75c-ea2d-4a2b-9906-f47101a6fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32099324\n",
      "0.13237584\n",
      "0.0761582\n",
      "1.0\n",
      "0.45838118\n",
      "0.33657658\n"
     ]
    }
   ],
   "source": [
    "for emb_set in all_image_embeddings + all_text_embeddings:\n",
    "    v1 = all_text_embeddings[0][3]\n",
    "    v2 = emb_set[3]\n",
    "    print(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91bcc8e3-819d-4a5c-9795-95c0e3806a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49408, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(full_model.clip_model.token_embedding.parameters())[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79e47f3c-a872-4549-80db-dcd85b7870b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are not 2D, skipping alignment\n"
     ]
    }
   ],
   "source": [
    "embs = emblaze.EmbeddingSet([\n",
    "    emblaze.Embedding({\n",
    "        emblaze.Field.POSITION: emb,\n",
    "        emblaze.Field.COLOR: np.ones(len(emb))\n",
    "    }, metric='cosine', label=label)\n",
    "    for emb, label in zip(all_image_embeddings, ['initial', 'epoch 3', 'epoch 6'])\n",
    "])\n",
    "embs.compute_neighbors(n_neighbors=10)\n",
    "\n",
    "projections = embs.project(method=emblaze.ProjectionTechnique.ALIGNED_UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7336c0a-6b34-48e4-95bc-78f2573ed172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                              | 0/62 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████████████████████▌                                                                                                                                                                 | 7/62 [00:00<00:00, 65.22it/s]\u001b[A\n",
      " 29%|████████████████████████████████████████████████████▌                                                                                                                                | 18/62 [00:00<00:00, 89.03it/s]\u001b[A\n",
      " 48%|███████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 30/62 [00:00<00:00, 98.72it/s]\u001b[A\n",
      " 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 41/62 [00:00<00:00, 102.90it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 101.30it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "def add_alpha_channel(img_arr):\n",
    "    return np.concatenate([img_arr, np.ones(img_arr.shape[:2] + (1,)) * 255], axis=2)\n",
    "raw_images = [add_alpha_channel(np.array(Image.open(IMAGES_DIR + image_id + \".jpg\"))) for image_id in tqdm.tqdm(product_ids)]\n",
    "thumbnails = emblaze.ImageThumbnails(raw_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb190947-a710-476a-ae65-346879d576ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846d8b5402a740718b1c1c26a3fcb894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(colorScheme='tableau', data={'data': [{'_format': 'compressed', '_idtype': 'u1', '_length': 62, 'ids': …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = emblaze.Viewer(embeddings=projections, thumbnails=thumbnails)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda32c3-3691-42c2-b11c-ace2bbc0ccc4",
   "metadata": {},
   "source": [
    "# Ranking Evaluation\n",
    "\n",
    "Build a dataset where a random sample of \"premise\" products is compared against all \"hypothesis\" products in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29981c99-e216-4f07-a0ea-cbf962ed9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100 pairs, 62 products per query\n"
     ]
    }
   ],
   "source": [
    "# Build pairs to rank\n",
    "\n",
    "NUM_QUERIES = 50\n",
    "\n",
    "val_products = sorted(list(set(x for pair in val_pairs for x in pair)))\n",
    "\n",
    "np.random.seed(1234)\n",
    "premise_products = np.random.choice(val_products, size=min(NUM_QUERIES, len(val_products)), replace=False)\n",
    "hypothesis_products = val_products\n",
    "\n",
    "ranking_pairs = list(itertools.product(premise_products, hypothesis_products))\n",
    "print(len(ranking_pairs), \"pairs,\", len(hypothesis_products), \"products per query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "43a41485-1311-4cca-8315-7aa40fbd3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground-truth\n",
    "\n",
    "ground_truth_map = {}\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "    item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if item_id not in premise_products: continue\n",
    "\n",
    "    for room_url in room_url_list:\n",
    "        room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "        ground_truth_map[item_id] = ground_truth_map.get(item_id, set()) | set(room_to_items[room_id])\n",
    "ground_truth_lists = [ground_truth_map[item_id] for item_id in premise_products]\n",
    "\n",
    "# plt.hist([len(x) for x in ground_truth_lists], bins=np.arange(0, max(len(x) for x in ground_truth_lists), 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c514d836-2840-456b-a8c6-3f045e188d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 169.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([310, 77])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rank_image = FurnitureImagePairsDataset(IMAGES_DIR, ranking_pairs, np.zeros(len(ranking_pairs)))\n",
    "\n",
    "def tokenize(text):\n",
    "  try:\n",
    "      return clip.tokenize(text)\n",
    "  except:\n",
    "      return clip.tokenize(' '.join(text.split()[:50]))\n",
    "\n",
    "X_rank_text_premise = torch.cat([tokenize(item_to_info[id]) for id, _ in ranking_pairs], 0)\n",
    "X_rank_text_hypothesis = torch.cat([tokenize(item_to_info[id]) for _, id in ranking_pairs], 0)\n",
    "X_rank_text_premise.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "67aaf16f-a89c-4a82-9cef-b568ab08c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "img_ranking_data = X_rank_image # TensorDataset(torch.from_numpy(X_val_image_premise), torch.from_numpy(X_val_image_hypothesis), torch.from_numpy(y_val))\n",
    "text_ranking_data = TensorDataset(X_rank_text_premise, X_rank_text_hypothesis, torch.zeros(len(ranking_pairs)))\n",
    "\n",
    "text_ranking_loader = DataLoader(text_ranking_data, batch_size=BATCH_SIZE)\n",
    "img_ranking_loader = DataLoader(img_ranking_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(len(text_ranking_loader), len(img_ranking_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f659d1-b8df-4175-8cbc-cc2428ea35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.eval()\n",
    "ranking_results = []\n",
    "with torch.no_grad():\n",
    "    for lstm, cnn in tqdm.tqdm(zip(text_ranking_loader, img_ranking_loader), total=len(text_ranking_loader)):\n",
    "        lstm_inp1, lstm_inp2, _ = lstm\n",
    "        cnn_inp1, cnn_inp2, _ = cnn\n",
    "        lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.to(device), lstm_inp2.to(device), lstm_labels.to(device)\n",
    "        cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(device), cnn_inp2.to(device), cnn_labels.to(device)\n",
    "        full_model.zero_grad()\n",
    "        output = full_model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
    "        ranking_results.append(output.numpy())\n",
    "ranking_results = np.concatenate(ranking_results).reshape(len(premise_products), len(hypothesis_products))\n",
    "print(ranking_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "130811f3-c839-4bad-b715-b92627ad8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Evaluator:\n",
    "    def __init__(self, GroundTruth):\n",
    "      self.GroundTruth = GroundTruth\n",
    "\n",
    "    def NDCG_Eval(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      DCGScore = 0\n",
    "      result = []\n",
    "      for i, item in enumerate(sortedRankResult[:topk]):\n",
    "        if item[0] in self.GroundTruth:\n",
    "          result.append((item, i))\n",
    "      DCGScore = sum([item[0][1]/math.log(item[1]+2, 2) for item in result])\n",
    "      IDCGScore = sum([1/math.log(i+2,2) for i in range(topk)])\n",
    "      NDCG = DCGScore / IDCGScore\n",
    "\n",
    "      return NDCG\n",
    "    \n",
    "    def Score_Eval(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      return sum(i[1] for i in sortedRankResult[:topk] if i[0] in self.GroundTruth) / topk\n",
    "    \n",
    "    def Precision(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      topkresult = sortedRankResult[:topk]\n",
    "      return len([i for i in sortedRankResult[:topk] if i[0] in self.GroundTruth]) / len(topkresult)\n",
    "\n",
    "    def Recall(self, rankresult, topk):\n",
    "      sortedRankResult = sorted(rankresult.items(), key = lambda x:x[1], reverse=True)\n",
    "      topkresult = sortedRankResult[:topk]\n",
    "      return len([i for i in sortedRankResult[:topk] if i[0] in self.GroundTruth]) / len(self.GroundTruth)\n",
    "    \n",
    "    def FValue(self, rankresult, topk):\n",
    "      precision = self.Precision(rankresult, topk)\n",
    "      recall = self.Recall(rankresult, topk)\n",
    "      return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "752e0478-0a4b-4cde-8c8c-2785152f1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.802 (95% CI -0.382-1.987)\n",
      "Score: 0.467 (95% CI -0.167-1.102)\n",
      "Precision: 0.160 (95% CI 0.003-0.317)\n",
      "Recall: 0.059 (95% CI -0.005-0.123)\n",
      "FValue: 0.079 (95% CI 0.013-0.144)\n"
     ]
    }
   ],
   "source": [
    "# Now use the evaluator\n",
    "TOP_K = 10\n",
    "\n",
    "ndcg = np.zeros(len(premise_products))\n",
    "score = np.zeros(len(premise_products))\n",
    "precision = np.zeros(len(premise_products))\n",
    "recall = np.zeros(len(premise_products))\n",
    "fvalue = np.zeros(len(premise_products))\n",
    "for i, (ground_truth, rankings) in enumerate(zip(ground_truth_lists, ranking_results)):\n",
    "    evaluator = Evaluator(ground_truth)\n",
    "    rankings = {product: output for product, output in zip(hypothesis_products, rankings)}\n",
    "    ndcg[i] = evaluator.NDCG_Eval(rankings, TOP_K)\n",
    "    score[i] = evaluator.Score_Eval(rankings, TOP_K)\n",
    "    precision[i] = evaluator.Precision(rankings, TOP_K)\n",
    "    recall[i] = evaluator.Recall(rankings, TOP_K)\n",
    "    fvalue[i] = evaluator.FValue(rankings, TOP_K)\n",
    "print(\"NDCG: {:.3f} (95% CI {:.3f}-{:.3f})\".format(ndcg.mean(), ndcg.mean() - 1.96 * ndcg.std(), ndcg.mean() + 1.96 * ndcg.std()))\n",
    "print(\"Score: {:.3f} (95% CI {:.3f}-{:.3f})\".format(score.mean(), score.mean() - 1.96 * score.std(), score.mean() + 1.96 * score.std()))\n",
    "print(\"Precision: {:.3f} (95% CI {:.3f}-{:.3f})\".format(precision.mean(), precision.mean() - 1.96 * precision.std(), precision.mean() + 1.96 * precision.std()))\n",
    "print(\"Recall: {:.3f} (95% CI {:.3f}-{:.3f})\".format(recall.mean(), recall.mean() - 1.96 * recall.std(), recall.mean() + 1.96 * recall.std()))\n",
    "print(\"FValue: {:.3f} (95% CI {:.3f}-{:.3f})\".format(fvalue.mean(), fvalue.mean() - 1.96 * fvalue.std(), fvalue.mean() + 1.96 * fvalue.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad87869-e50a-41da-8bcb-005b461358ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CLIP_finetuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
