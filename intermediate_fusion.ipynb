{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7855707-1f93-4b0a-b3d0-fce2ec7c7b7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JvNGzXEy7H2",
    "outputId": "d4b20794-c237-4124-86d5-9c070ac0d89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-20 17:56:46--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.68.70\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.68.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  79.8MB/s    in 21s     \n",
      "\n",
      "2021-11-20 17:57:07 (74.5 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f368aeaa-9c25-4f87-8204-8617ebf10746",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_WORD2VEC = \"../../../research/uncertainty_benchmark/embeddings/GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5cef3f-197e-443c-a29d-dd57e66606a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from text_preprocessing import Tokenizer, pad_sequences\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c13be69e-52a8-4980-9d02-94254986d813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.9.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7805788-e626-4e43-859a-1ae4b935f0e7",
   "metadata": {
    "id": "r9tNjmBYeon6",
    "tags": []
   },
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "333006e7-80d3-4115-8752-8f4add91c346",
   "metadata": {
    "id": "7DW6oxGPmC9-"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"dataset/\"\n",
    "DATASET_DIR = BASE_DIR + \"text_data/\"\n",
    "IMAGES_DIR = BASE_DIR + \"images/all_items/\"\n",
    "\n",
    "# Global Parameter Variables\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "NUM_WORDS_TOKENIZER = 50000\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a713ae50-dddd-4605-b09f-2222d8a56c04",
   "metadata": {
    "id": "qmCx_kVEmF-s"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(path):\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.resize(img, (256, 256))\n",
    "  img = img.astype(np.float32) / 255\n",
    "  return np.moveaxis(img, 2, 0)\n",
    "\n",
    "def read_pickle(fn):\n",
    "\twith open(fn, \"rb\") as f:\n",
    "\t\treturn pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d10f6439-4ed5-40ba-9a9e-94bd752e72fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abwrNXYP8ibS",
    "outputId": "87f87e2d-b55a-490f-c05e-0a9d376f90f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/890.333.75.jpg does not exist\n",
      "images/991.333.98.jpg does not exist\n",
      "images/990.612.97.jpg does not exist\n"
     ]
    }
   ],
   "source": [
    "# {room image url -> string of room category}; e.g.: 'ikea-town-and-country__1364308377063-s4.jpg': 'Living Room'\n",
    "room_categories = read_pickle(DATASET_DIR + \"categories_dict.p\")\n",
    "# {item image ID -> string of item category}; e.g.: '291.292.29': 'Footstool',\n",
    "item_categories = read_pickle(DATASET_DIR + \"categories_images_dict.p\")\n",
    "# {item image id -> dict of descriptions}; e.g. '202.049.06': {'color': 'Grey,black','desc': 'View more product information Concealed press studs keep the quilt in place','img': 'images/objects/202.049.06.jpg','name': 'GURLI','size': '120x180 cm','type': 'Throw'},\n",
    "item_property = read_pickle(DATASET_DIR + \"products_dict.p\")\n",
    "# {item image url -> {description, name}}; e.g: '/static/images/902.592.50.jpg': {'desc': 'The high pile dampens sound and provides a soft surface to walk on.','name': 'GSER'},\n",
    "item_to_description = read_pickle(DATASET_DIR + \"img_to_desc.p\")\n",
    "# {item image url -> list of corresponding room image url}; e.g.: 'images/001.509.85.jpg': ['images/room_scenes/ikea-wake-up-and-grow__1364335362013-s4.jpg','images/room_scenes/ikea-wake-up-and-grow-1364335370196.jpg'],\n",
    "item_to_rooms_map = read_pickle(DATASET_DIR + \"item_to_room.p\")\n",
    "# {room image url -> list of items}; e.g.: 'ikea-work-from-home-in-perfect-harmony__1364319311386-s4.jpg': ['desk','chair']\n",
    "room_to_item_categories = read_pickle(DATASET_DIR + \"room_to_items.p\")\n",
    "\n",
    "room_to_items = {}\n",
    "\n",
    "for item_url, room_url_list in item_to_rooms_map.items():\n",
    "  item_id = item_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "  if not os.path.exists(IMAGES_DIR + item_id + \".jpg\"):\n",
    "      print(item_url + \" does not exist\")\n",
    "      continue\n",
    "\n",
    "  for room_url in room_url_list:\n",
    "    room_id = room_url.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "    if room_id not in room_to_items:\n",
    "      room_to_items[room_id] = [item_id]\n",
    "    else:\n",
    "      room_to_items[room_id].append(item_id)\n",
    "    \n",
    "with open(BASE_DIR + \"train_sets_reweighted.pkl\", \"rb\") as file:\n",
    "    train_sets = pickle.load(file)\n",
    "with open(BASE_DIR + \"val_data_reweighted.pkl\", \"rb\") as file:\n",
    "    val_pairs, y_val = pickle.load(file)\n",
    "with open(BASE_DIR + \"preprocessed_text.pkl\", \"rb\") as file:\n",
    "    item_to_info = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cf40b4e-3474-49ed-9629-d385b37f8f6c",
   "metadata": {
    "id": "jtQJWkJ0fEPT"
   },
   "outputs": [],
   "source": [
    "class FurnitureImagePairsDataset(Dataset):\n",
    "    \"\"\"Dataset containing pairs of furniture items.\"\"\"\n",
    "\n",
    "    def __init__(self, image_path, pairs, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_path (string): Path to the directory containing images.\n",
    "            pairs (list of tuples of strings): Pairs of image IDs to be used as training samples.\n",
    "            labels (array of integers): Labels for the training samples.\n",
    "        \"\"\"\n",
    "        super(FurnitureImagePairsDataset, self).__init__()\n",
    "        self.image_ids = list(set(x for pair in pairs for x in pair))\n",
    "        self.index_mapping = {image_id: i for i, image_id in enumerate(self.image_ids)}\n",
    "        self.images = [preprocess_img(image_path + image_id + \".jpg\") for image_id in tqdm.tqdm(self.image_ids, ncols=80)]\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if isinstance(idx, (list, tuple)):\n",
    "            x1, x2, y = zip(*[self[i] for i in idx])\n",
    "            return torch.stack(x1), torch.stack(x2), torch.from_numpy(np.array(y))\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "        return self.images[self.index_mapping[pair[0]]], self.images[self.index_mapping[pair[1]]], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13201acd-6a0d-4673-9378-9c8673f190e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, only use the first training set. We will try using PU bagging later.\n",
    "train_pairs, y_train, train_scenes = train_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19bed716-73e8-4eac-862b-916ec8795865",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQygJsUIOMeb",
    "outputId": "c43bde63-7efb-4ee5-b2be-ec8c09e1b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1379/1379 [00:04<00:00, 308.04it/s]\n",
      "100%|████████████████████████████████████████| 680/680 [00:02<00:00, 318.58it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_image = FurnitureImagePairsDataset(IMAGES_DIR, train_pairs, y_train)\n",
    "X_val_image = FurnitureImagePairsDataset(IMAGES_DIR, val_pairs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54bd8807-924e-4174-a520-0a79509670c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lO1ZkgcUK1m7",
    "outputId": "b13d2de8-5d78-4e91-81c5-32011f50f847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2005 unique tokens.\n",
      "Max len: 100\n",
      "total embedded: 1907 common words\n"
     ]
    }
   ],
   "source": [
    "word2vecDict = KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True)\n",
    "\n",
    "def get_embedding_matrix(word_index):\n",
    "    # One for zero, one for len(WORD_INDEX) which will be used as the EOT token\n",
    "    embedding_matrix = np.random.randn(len(word_index)+2, EMBEDDING_DIM)\n",
    "    embedding_matrix /= np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "    embeddedCount = 0\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word2vecDict[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount += 1\n",
    "    print(\"total embedded:\", embeddedCount, \"common words\")\n",
    "    return embedding_matrix\n",
    "\n",
    "train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
    "train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
    "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
    "WORD_INDEX = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
    "WORD2VEC_EMBEDDING_MATRIX = get_embedding_matrix(WORD_INDEX)\n",
    "\n",
    "X_train_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_premise_texts)]\n",
    "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_train_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_hypothesis_texts)]\n",
    "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc3dfb71-3c38-4310-94f2-70f8ab100388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2030 unique tokens.\n",
      "Max len: 100\n"
     ]
    }
   ],
   "source": [
    "train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
    "train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
    "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
    "\n",
    "with open(\"text_preprocessing_info.pkl\", \"rb\") as file:\n",
    "    WORD_INDEX, WORD2VEC_EMBEDDING_MATRIX = pickle.load(file)\n",
    "    assert WORD_INDEX == tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_premise_texts)]\n",
    "X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_train_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_hypothesis_texts)]\n",
    "X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96ba8d72-5c89-4659-b0eb-405edffef6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sturdy 0.8149443975259862\n",
      "spotlight 0.8245677671757662\n",
      "hanger 0.8266930090496071\n",
      "handle 0.8275974711361982\n",
      "parasol 0.8400551754885448\n",
      "gloves 0.8456371886085474\n",
      "rails 0.8471594789895696\n",
      "lightweight 0.8498842882236268\n",
      "damper 0.8521505624686629\n",
      "hats 0.8587346957793748\n",
      "folding 0.8599620042150518\n",
      "pen 0.8599860418049521\n",
      "rack 0.8604138898429275\n",
      "rail 0.8621476551883281\n",
      "quilted 0.862658341265573\n",
      "suitable 0.8635136501430496\n",
      "off 0.8644095949152492\n",
      "slot 0.8652965613300614\n",
      "secure 0.8653805415889626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "dists = cosine_distances(WORD2VEC_EMBEDDING_MATRIX[len(WORD_INDEX) + 1].reshape(1, -1), WORD2VEC_EMBEDDING_MATRIX)\n",
    "order = list(np.argsort(dists.flatten()))[:20]\n",
    "for i in order:\n",
    "    if i == 0 or i == len(WORD_INDEX) + 1: continue\n",
    "    print(next(q for q, v in WORD_INDEX.items() if v == i), dists[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4adb553-4ab3-4023-bfc5-02e6d20f1810",
   "metadata": {
    "id": "0iAakls6kMkX"
   },
   "outputs": [],
   "source": [
    "val_premise_texts = [item_to_info[id] for id, _ in val_pairs]\n",
    "val_hypothesis_texts = [item_to_info[id] for _, id in val_pairs]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "X_val_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_premise_texts)]\n",
    "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_val_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_hypothesis_texts)]\n",
    "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c73cb04f-de78-4eeb-8b5c-69d7908e1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_preprocessing_info_reweighted.pkl\", \"wb\") as file:\n",
    "    pickle.dump((WORD_INDEX, WORD2VEC_EMBEDDING_MATRIX), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98edf959-e434-4036-a276-8e80cc33b48a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Vm9RcV-MWk7",
    "outputId": "d8c7ec3c-e9b4-4ec4-ddc9-833d9af0e9c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919 919\n",
      "218 218\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "img_train_data = X_train_image\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise),\n",
    "                                torch.from_numpy(X_train_text_hypothesis),\n",
    "                                torch.from_numpy(y_train))\n",
    "\n",
    "img_val_data = X_val_image\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise),\n",
    "                              torch.from_numpy(X_val_text_hypothesis), \n",
    "                              torch.from_numpy(y_val))\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(len(text_train_loader), len(img_train_loader))\n",
    "print(len(text_val_loader), len(img_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c85b96a-b2f6-4052-a31a-1e6d1c1b3b1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQygJsUIOMeb",
    "outputId": "c43bde63-7efb-4ee5-b2be-ec8c09e1b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2158/2158 [00:06<00:00, 337.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Also load a dataset of individual products to get embeddings\n",
    "\n",
    "val_product_ids = sorted(list(set(x for pair in train_pairs + val_pairs for x in pair)))\n",
    "single_images = torch.stack([torch.from_numpy(preprocess_img(IMAGES_DIR + image_id + \".jpg\"))\n",
    "                             for image_id in tqdm.tqdm(val_product_ids)])\n",
    "\n",
    "single_texts = [item_to_info[id] for id in val_product_ids]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "single_texts = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(single_texts)]\n",
    "single_texts = torch.from_numpy(pad_sequences(single_texts, maxlen=MAX_SEQUENCE_LENGTH, padding='post'))\n",
    "\n",
    "single_data = TensorDataset(single_images, single_texts)\n",
    "single_loader = DataLoader(single_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c4e5f-850c-4e37-a4f7-31c782ec5c1b",
   "metadata": {},
   "source": [
    "# Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57071d10-dce0-4bb3-929e-ea65d8cc20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_RELU_LAYERS = [3, 8, 15, 22, 29]\n",
    "VGG_RELU_LAYER_SIZES = [64, 128, 256, 512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "099fc49d-0bc9-4928-ad4c-91c77c556b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IntermediateFusionModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_matrix, stats_layer_idxs, stats_dims, output_dim, pre_fusion_layers=0):\n",
    "#         super(IntermediateFusionModel, self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         num_embeddings, embedding_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.pre_fusion_layers = pre_fusion_layers\n",
    "\n",
    "#         self.vgg_model = models.vgg16(pretrained=True)\n",
    "#         self.image_encoder = self.vgg_model._modules.get(\"features\")\n",
    "#         # self.image_projection = nn.Parameter(torch.empty(4096, 128))\n",
    "        \n",
    "#         # self.final_img_fc = nn.Linear(25088, 128)\n",
    "#         # self.final_img_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "#         self.stats_layer_idxs = stats_layer_idxs\n",
    "#         self.stats_dims = stats_dims\n",
    "#         self.image_fcs = nn.ModuleList()\n",
    "#         self.image_dropouts = nn.ModuleList()\n",
    "#         # self.image_projections = nn.ParameterList()\n",
    "#         for dim in self.stats_dims:\n",
    "#             self.image_fcs.append(nn.Linear(dim * 2, embedding_dim))\n",
    "#             self.image_dropouts.append(nn.Dropout(0.5))\n",
    "#             # self.image_projections.append(nn.Parameter(torch.empty(dim * 2, 128 // len(self.stats_dims))))\n",
    "\n",
    "#         self.text_emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "#         self.text_emb.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "#         self.text_emb.weight.requires_grad = True\n",
    "\n",
    "#         self.attention_layers = nn.ModuleList()\n",
    "#         self.attention_lns = nn.ModuleList()\n",
    "#         for i in range(pre_fusion_layers + len(self.stats_dims)):\n",
    "#             self.attention_layers.append(nn.MultiheadAttention(embedding_dim, 1))\n",
    "#             self.attention_lns.append(nn.LayerNorm(embedding_dim))\n",
    "#         self.text_projection = nn.Parameter(torch.empty(embedding_dim, 128))\n",
    "        \n",
    "#         # self.combined_projection = nn.Parameter(torch.empty(256, 128))\n",
    "\n",
    "#         # self.output_fc = nn.Linear(256, output_dim)\n",
    "#         self.initialize_parameters()\n",
    "        \n",
    "#     def initialize_parameters(self):\n",
    "#         if self.text_projection is not None:\n",
    "#             nn.init.normal_(self.text_projection, std=self.embedding_dim ** -0.5)\n",
    "#         # for proj in self.image_projections:\n",
    "#         #     nn.init.normal_(proj, std=proj.size(0) ** -0.5)\n",
    "#         # nn.init.normal_(self.combined_projection, std=self.combined_projection.size(0) ** -0.5)\n",
    "\n",
    "#     def encode(self, text_inp, img_inp):\n",
    "#         # enc = self.image_encoder(img_inp)\n",
    "#         # enc = self.avgpool(enc).flatten(1)\n",
    "#         # fc_1 = self.image_fc1(enc)\n",
    "#         # fc_1 = F.relu(fc_1)\n",
    "#         # fc_1 = self.image_dropout1(fc_1)\n",
    "\n",
    "#         enc_inp = img_inp\n",
    "#         stats_outputs = [None for _ in self.stats_layer_idxs]\n",
    "#         # img_outputs = [None for _ in self.stats_layer_idxs]\n",
    "#         for i in range(len(self.image_encoder)): # max(self.stats_layer_idxs) + 1):\n",
    "#             enc_inp = self.image_encoder[i](enc_inp)\n",
    "#             if i in self.stats_layer_idxs:\n",
    "#                 idx = self.stats_layer_idxs.index(i)\n",
    "#                 stat_enc = enc_inp.flatten(2)\n",
    "#                 stat_enc = torch.cat([stat_enc.mean(2), stat_enc.std(2)], 1).squeeze()\n",
    "                \n",
    "#                 # img_outputs[idx] = stat_enc @ self.image_projections[idx]\n",
    "#                 stat_fc = self.image_fcs[idx](stat_enc)\n",
    "#                 stat_fc = F.relu(stat_fc)\n",
    "#                 stat_fc = self.image_dropouts[idx](stat_fc)\n",
    "#                 stats_outputs[idx] = stat_fc\n",
    "        \n",
    "#         # avgpool = self.vgg_model._modules.get(\"avgpool\")\n",
    "#         # enc_inp = avgpool(enc_inp).flatten(1)\n",
    "#         # classifier = self.vgg_model._modules.get(\"classifier\")\n",
    "#         # for i in range(6):\n",
    "#         #     enc_inp = classifier[i](enc_inp)\n",
    "#         # img_fc = enc_inp @ self.image_projection\n",
    "        \n",
    "#         # img_fc = self.final_img_fc(enc_inp)\n",
    "#         # img_fc = F.relu(img_fc)\n",
    "#         # img_fc = self.final_img_dropout(img_fc)\n",
    "\n",
    "#         text_enc = self.text_emb(text_inp).transpose(0, 1)\n",
    "#         # text_enc shape: (seq length, batch size, embedding dim)\n",
    "#         # We will add one value to the sequence (first axis) at each layer, \n",
    "#         # representing the instance norm statistics from VGG\n",
    "    \n",
    "#         for attn, lnorm in zip(self.attention_layers[:self.pre_fusion_layers], self.attention_lns[:self.pre_fusion_layers]):\n",
    "#             text_enc, _ = attn(text_enc, text_enc, text_enc)\n",
    "#             text_enc = lnorm(text_enc.transpose(0, 1)).transpose(0, 1)\n",
    "            \n",
    "#         for stats, attn, lnorm in zip(stats_outputs, self.attention_layers[self.pre_fusion_layers:], self.attention_lns[self.pre_fusion_layers:]):\n",
    "#             text_enc = torch.cat([stats.unsqueeze(0), text_enc], 0)\n",
    "#             text_enc, _ = attn(text_enc, text_enc, text_enc)\n",
    "#             text_enc = lnorm(text_enc.transpose(0, 1)).transpose(0, 1)\n",
    "            \n",
    "#         text_enc = text_enc.transpose(0, 1)\n",
    "#         text_enc = text_enc[torch.arange(text_enc.size(0)), text_inp.argmax(dim=-1) + len(stats_outputs)] @ self.text_projection\n",
    "                \n",
    "#         return text_enc # torch.cat([*img_outputs, text_enc], 1) @ self.combined_projection\n",
    "\n",
    "#     def forward(self, text_inp1, text_inp2, img_inp1, img_inp2):\n",
    "#         emb_1 = self.encode(text_inp1, img_inp1)\n",
    "#         emb_2 = self.encode(text_inp2, img_inp2)\n",
    "#         output = (emb_1 * emb_2).sum(1) # self.output_fc(torch.cat([emb_1, emb_2], 1))\n",
    "#         return output\n",
    "        \n",
    "class IntermediateFusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_matrix, stats_layer_idxs, stats_dims, output_dim, pre_fusion_layers=0):\n",
    "        super(IntermediateFusionModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pre_fusion_layers = pre_fusion_layers\n",
    "\n",
    "        self.vgg_model = models.vgg16(pretrained=True)\n",
    "        self.image_encoder = self.vgg_model._modules.get(\"features\")\n",
    "        # self.image_projection = nn.Parameter(torch.empty(4096, 128))\n",
    "        \n",
    "        # self.final_img_fc = nn.Linear(25088, 128)\n",
    "        # self.final_img_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.stats_layer_idxs = stats_layer_idxs\n",
    "        self.stats_dims = stats_dims\n",
    "        # self.image_fcs = nn.ModuleList()\n",
    "        # self.image_dropouts = nn.ModuleList()\n",
    "        # self.image_projections = nn.ParameterList()\n",
    "        # for dim in self.stats_dims:\n",
    "            # self.image_fcs.append(nn.Linear(dim * 2, embedding_dim))\n",
    "            # self.image_dropouts.append(nn.Dropout(0.5))\n",
    "            # self.image_projections.append(nn.Parameter(torch.empty(dim * 2, 128 // len(self.stats_dims))))\n",
    "\n",
    "        self.text_emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.text_emb.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.text_emb.weight.requires_grad = True\n",
    "\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.attention_lns = nn.ModuleList()\n",
    "        for i in range(pre_fusion_layers + len(self.stats_dims)):\n",
    "            self.attention_layers.append(nn.MultiheadAttention(embedding_dim, 1))\n",
    "            self.attention_lns.append(nn.LayerNorm(embedding_dim))\n",
    "        self.text_projection = nn.Parameter(torch.empty(embedding_dim, 128))\n",
    "        \n",
    "        # self.combined_projection = nn.Parameter(torch.empty(256, 128))\n",
    "\n",
    "        # self.output_fc = nn.Linear(256, output_dim)\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.embedding_dim ** -0.5)\n",
    "        # for proj in self.image_projections:\n",
    "        #     nn.init.normal_(proj, std=proj.size(0) ** -0.5)\n",
    "        # nn.init.normal_(self.combined_projection, std=self.combined_projection.size(0) ** -0.5)\n",
    "\n",
    "    def encode(self, text_inp, img_inp):\n",
    "        # enc = self.image_encoder(img_inp)\n",
    "        # enc = self.avgpool(enc).flatten(1)\n",
    "        # fc_1 = self.image_fc1(enc)\n",
    "        # fc_1 = F.relu(fc_1)\n",
    "        # fc_1 = self.image_dropout1(fc_1)\n",
    "\n",
    "#         enc_inp = img_inp\n",
    "        stats_outputs = [None for _ in self.stats_layer_idxs]\n",
    "#         img_outputs = [None for _ in self.stats_layer_idxs]\n",
    "#         for i in range(len(self.image_encoder)): # max(self.stats_layer_idxs) + 1):\n",
    "#             enc_inp = self.image_encoder[i](enc_inp)\n",
    "#             if i in self.stats_layer_idxs:\n",
    "#                 idx = self.stats_layer_idxs.index(i)\n",
    "#                 stat_enc = enc_inp.flatten(2)\n",
    "#                 stat_enc = torch.cat([stat_enc.mean(2), stat_enc.std(2)], 1).squeeze()\n",
    "                \n",
    "#                 img_outputs[idx] = stat_enc @ self.image_projections[idx]\n",
    "#                 stat_fc = self.image_fcs[idx](stat_enc)\n",
    "#                 stat_fc = F.relu(stat_fc)\n",
    "#                 stat_fc = self.image_dropouts[idx](stat_fc)\n",
    "#                 stats_outputs[idx] = stat_fc\n",
    "        \n",
    "        # avgpool = self.vgg_model._modules.get(\"avgpool\")\n",
    "        # enc_inp = avgpool(enc_inp).flatten(1)\n",
    "        # classifier = self.vgg_model._modules.get(\"classifier\")\n",
    "        # for i in range(6):\n",
    "        #     enc_inp = classifier[i](enc_inp)\n",
    "        # img_fc = enc_inp @ self.image_projection\n",
    "        \n",
    "        # img_fc = self.final_img_fc(enc_inp)\n",
    "        # img_fc = F.relu(img_fc)\n",
    "        # img_fc = self.final_img_dropout(img_fc)\n",
    "\n",
    "        text_enc = self.text_emb(text_inp).transpose(0, 1)\n",
    "        # text_enc shape: (seq length, batch size, embedding dim)\n",
    "        # We will add one value to the sequence (first axis) at each layer, \n",
    "        # representing the instance norm statistics from VGG\n",
    "    \n",
    "        for attn, lnorm in zip(self.attention_layers[:self.pre_fusion_layers], self.attention_lns[:self.pre_fusion_layers]):\n",
    "            text_enc, _ = attn(text_enc, text_enc, text_enc)\n",
    "            text_enc = lnorm(text_enc.transpose(0, 1)).transpose(0, 1)\n",
    "            \n",
    "        # for stats, attn, lnorm in zip(stats_outputs, self.attention_layers[self.pre_fusion_layers:], self.attention_lns[self.pre_fusion_layers:]):\n",
    "        #     text_enc = torch.cat([stats.unsqueeze(0), text_enc], 0)\n",
    "        #     text_enc, _ = attn(text_enc, text_enc, text_enc)\n",
    "        #     text_enc = lnorm(text_enc.transpose(0, 1)).transpose(0, 1)\n",
    "            \n",
    "        text_enc = text_enc.transpose(0, 1)\n",
    "        text_enc = text_enc[torch.arange(text_enc.size(0)), text_inp.argmax(dim=-1) + len(stats_outputs)] @ self.text_projection\n",
    "                \n",
    "        return text_enc # torch.cat([*img_outputs, text_enc], 1) @ self.combined_projection\n",
    "\n",
    "    def forward(self, text_inp1, text_inp2, img_inp1, img_inp2):\n",
    "        emb_1 = self.encode(text_inp1, img_inp1)\n",
    "        emb_2 = self.encode(text_inp2, img_inp2)\n",
    "        output = (emb_1 * emb_2).sum(1) # self.output_fc(torch.cat([emb_1, emb_2], 1))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d850b3a-ab22-409a-bce5-e222218c7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak input parameters to model constructor here - they will be used in training loops below\n",
    "def make_model():\n",
    "#     return IntermediateFusionModel(len(WORD_INDEX)+1,\n",
    "#                                    WORD2VEC_EMBEDDING_MATRIX,\n",
    "#                                    [VGG_RELU_LAYERS[0], VGG_RELU_LAYERS[2]],\n",
    "#                                    [VGG_RELU_LAYER_SIZES[0], VGG_RELU_LAYER_SIZES[2]],\n",
    "#                                    1,\n",
    "#                                    pre_fusion_layers=0)\n",
    "    return IntermediateFusionModel(len(WORD_INDEX)+1, WORD2VEC_EMBEDDING_MATRIX,\n",
    "                                [], # [VGG_RELU_LAYERS[0], VGG_RELU_LAYERS[2]],\n",
    "                                [], # [VGG_RELU_LAYER_SIZES[0], VGG_RELU_LAYER_SIZES[2]],\n",
    "                                1, pre_fusion_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20455b78-1f61-49c5-9016-b13a82a7d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "MAIN_LEARNING_RATE = 1e-5\n",
    "FINE_TUNE_LEARNING_RATE = 1e-7\n",
    "EPOCHS = 20\n",
    "CLIP = 5\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fb64a7b-fc3d-4d1c-b8bc-9eb1be295059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, text_train_loader, img_train_loader):\n",
    "    \"\"\"Train model for one epoch and return accuracy and loss on training set\"\"\"\n",
    "    model.train()\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for lstm, cnn in tqdm.tqdm(zip(text_train_loader, img_train_loader),\n",
    "                               total=len(text_train_loader),\n",
    "                               ncols=80):\n",
    "        lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
    "        cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
    "        lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.long().to(DEVICE), lstm_inp2.long().to(DEVICE), lstm_labels.to(DEVICE)\n",
    "        cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
    "        loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "  \n",
    "    return total_acc_train / len(text_train_loader), total_loss_train / len(text_train_loader)\n",
    "\n",
    "def eval_model_pairs(model, criterion, optimizer, text_val_loader, img_val_loader):\n",
    "    \"\"\"Return accuracy and loss on a validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in tqdm.tqdm(zip(text_val_loader, img_val_loader),\n",
    "                                   total=len(text_val_loader),\n",
    "                                   ncols=80):\n",
    "            lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
    "            cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
    "            lstm_inp1, lstm_inp2, lstm_labels = lstm_inp1.long().to(DEVICE), lstm_inp2.long().to(DEVICE), lstm_labels.to(DEVICE)\n",
    "            cnn_inp1, cnn_inp2, cnn_labels = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE), cnn_labels.to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            output = model(lstm_inp1, lstm_inp2, cnn_inp1, cnn_inp2)\n",
    "            val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += val_loss.item()\n",
    "    return total_acc_val / len(text_val_loader), total_loss_val / len(text_val_loader)\n",
    "\n",
    "def eval_model_single(model, single_loader):\n",
    "    \"\"\"Returns embeddings for each product in the single product loader.\"\"\"\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for image_inp, text_inp in tqdm.tqdm(single_loader,\n",
    "                                             total=len(single_loader),\n",
    "                                             ncols=80):\n",
    "            image_inp = image_inp.to(DEVICE)\n",
    "            text_inp = text_inp.long().to(DEVICE)\n",
    "            embs.append(model.encode(text_inp, image_inp).cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29b8c3-b431-4d5d-a169-1904630066fd",
   "metadata": {},
   "source": [
    "# Single Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf0473-b624-4ed0-bd9f-d0213f957141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "xX2rumtEQYv8",
    "outputId": "3df96bb3-06ce-4d95-bac3-d61fe628a3ac"
   },
   "outputs": [],
   "source": [
    "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
    "\n",
    "model = make_model()\n",
    "# state = torch.load(\"checkpoint_intermediate_fusion.p\", map_location=torch.device('cpu'))\n",
    "# model.load_state_dict(state)\n",
    "\n",
    "for param in model.vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in model.image_encoder.parameters():\n",
    "#     param.requires_grad = True\n",
    "model.to(DEVICE)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MAIN_LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "print(\"Training Started...\")\n",
    "last_loss = 1e9\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    train_acc, train_loss = train_model(model, criterion, optimizer, text_train_loader, img_train_loader)\n",
    "    val_acc, val_loss = eval_model_pairs(model, criterion, optimizer, text_val_loader, img_val_loader)\n",
    "    \n",
    "    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    torch.cuda.empty_cache()\n",
    "    if val_loss < last_loss:\n",
    "        torch.save(model.state_dict(), \"checkpoint_fine_tune.p\")\n",
    "    else:\n",
    "        print(\"Loss increased - early stopping.\")\n",
    "        break\n",
    "    last_loss = val_loss\n",
    "    \n",
    "embs = eval_model_single(model, single_loader)\n",
    "\n",
    "with open(\"embeddings_fine_tune_{}.p\".format(model_number), \"wb\") as file:\n",
    "    pickle.dump((val_product_ids, embs), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b12da47-3b94-4b67-be19-4c8fd313489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:07<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "state = torch.load(\"intermediate_fusion_controls/checkpoint_text_only.p\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state)\n",
    "\n",
    "embs = eval_model_single(model, single_loader)\n",
    "\n",
    "with open(\"intermediate_fusion_controls/embeddings_text_only.p\", \"wb\") as file:\n",
    "    pickle.dump((val_product_ids, embs), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9539a75-3b6b-4120-8d81-6c856b20ccea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PU Bagging Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8996e01d-d311-499c-a0ee-ba0bf3828178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2030 unique tokens.\n",
      "Max len: 100\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer for text processing based on the first training set\n",
    "train_premise_texts = [item_to_info[id] for id, _ in train_sets[0][0]]\n",
    "train_hypothesis_texts = [item_to_info[id] for _, id in train_sets[0][0]]\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS_TOKENIZER, lower=True)\n",
    "tokenizer.fit_on_texts(train_premise_texts + train_hypothesis_texts)\n",
    "\n",
    "with open(\"text_preprocessing_info.pkl\", \"rb\") as file:\n",
    "    WORD_INDEX, WORD2VEC_EMBEDDING_MATRIX = pickle.load(file)\n",
    "    assert WORD_INDEX == tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(WORD_INDEX))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fc6c3e7-07d1-4394-8f6e-df1f88ab9aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 704/704 [00:02<00:00, 282.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize validation data - same as above\n",
    "\n",
    "X_val_image = FurnitureImagePairsDataset(IMAGES_DIR, val_pairs, y_val)\n",
    "\n",
    "val_premise_texts = [item_to_info[id] for id, _ in val_pairs]\n",
    "val_hypothesis_texts = [item_to_info[id] for _, id in val_pairs]\n",
    "\n",
    "# Please notice that: tokenizer is ONLY used on training set to build vocab\n",
    "X_val_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_premise_texts)]\n",
    "X_val_text_premise = pad_sequences(X_val_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "X_val_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(val_hypothesis_texts)]\n",
    "X_val_text_hypothesis = pad_sequences(X_val_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "img_val_data = X_val_image\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_val_text_premise),\n",
    "                              torch.from_numpy(X_val_text_hypothesis), \n",
    "                              torch.from_numpy(y_val))\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=BATCH_SIZE)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1488c8c-b20c-463b-81e6-8ff9db793cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:04<00:00, 294.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.78it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.0953 train_acc: 0.5168 | val_loss: 0.7048 val_acc: 0.5391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.78it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6968 train_acc: 0.5397 | val_loss: 0.6911 val_acc: 0.5557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6789 train_acc: 0.5676 | val_loss: 0.6809 val_acc: 0.5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6626 train_acc: 0.5953 | val_loss: 0.6727 val_acc: 0.5873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.73it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6468 train_acc: 0.6167 | val_loss: 0.6675 val_acc: 0.5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6325 train_acc: 0.6365 | val_loss: 0.6609 val_acc: 0.6110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:38<00:00,  5.79it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss: 0.6182 train_acc: 0.6545 | val_loss: 0.6621 val_acc: 0.6168\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5977 train_acc: 0.6774 | val_loss: 0.6534 val_acc: 0.6186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.5944 train_acc: 0.6799 | val_loss: 0.6534 val_acc: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.5939 train_acc: 0.6812 | val_loss: 0.6534 val_acc: 0.6204\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:05<00:00, 281.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.0403 train_acc: 0.5185 | val_loss: 0.6996 val_acc: 0.5341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6928 train_acc: 0.5379 | val_loss: 0.6914 val_acc: 0.5479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6810 train_acc: 0.5624 | val_loss: 0.6865 val_acc: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6671 train_acc: 0.5896 | val_loss: 0.6762 val_acc: 0.5860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6518 train_acc: 0.6135 | val_loss: 0.6743 val_acc: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6364 train_acc: 0.6368 | val_loss: 0.6759 val_acc: 0.6013\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.6155 train_acc: 0.6623 | val_loss: 0.6583 val_acc: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6132 train_acc: 0.6660 | val_loss: 0.6582 val_acc: 0.6106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6124 train_acc: 0.6675 | val_loss: 0.6583 val_acc: 0.6111\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:05<00:00, 288.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.0637 train_acc: 0.5192 | val_loss: 0.7034 val_acc: 0.5203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6929 train_acc: 0.5368 | val_loss: 0.6931 val_acc: 0.5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6805 train_acc: 0.5577 | val_loss: 0.6857 val_acc: 0.5474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:38<00:00,  5.80it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6655 train_acc: 0.5903 | val_loss: 0.6787 val_acc: 0.5643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6498 train_acc: 0.6165 | val_loss: 0.6751 val_acc: 0.5755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6346 train_acc: 0.6371 | val_loss: 0.6687 val_acc: 0.5873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss: 0.6199 train_acc: 0.6556 | val_loss: 0.6680 val_acc: 0.5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss: 0.6091 train_acc: 0.6676 | val_loss: 0.6653 val_acc: 0.6028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_loss: 0.5961 train_acc: 0.6799 | val_loss: 0.6656 val_acc: 0.6052\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5758 train_acc: 0.7011 | val_loss: 0.6574 val_acc: 0.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.5734 train_acc: 0.7011 | val_loss: 0.6574 val_acc: 0.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.5717 train_acc: 0.7044 | val_loss: 0.6576 val_acc: 0.6138\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:04<00:00, 291.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 2.2026 train_acc: 0.5249 | val_loss: 0.7044 val_acc: 0.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6936 train_acc: 0.5408 | val_loss: 0.6925 val_acc: 0.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.75it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6801 train_acc: 0.5595 | val_loss: 0.6858 val_acc: 0.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss: 0.6656 train_acc: 0.5869 | val_loss: 0.6801 val_acc: 0.5783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss: 0.6509 train_acc: 0.6108 | val_loss: 0.6773 val_acc: 0.5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss: 0.6363 train_acc: 0.6323 | val_loss: 0.6756 val_acc: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss: 0.6236 train_acc: 0.6473 | val_loss: 0.6724 val_acc: 0.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:40<00:00,  5.74it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss: 0.6096 train_acc: 0.6647 | val_loss: 0.6751 val_acc: 0.6101\n",
      "Loss increased - early stopping.\n",
      "Trainable parameters: 16278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:38<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5913 train_acc: 0.6874 | val_loss: 0.6570 val_acc: 0.6193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:37<00:00,  2.73it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.5859 train_acc: 0.6899 | val_loss: 0.6569 val_acc: 0.6204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [05:37<00:00,  2.72it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.5864 train_acc: 0.6909 | val_loss: 0.6569 val_acc: 0.6216\n",
      "Loss increased - early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:05<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1454/1454 [00:05<00:00, 289.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1564200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.77it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:34<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 1.7823 train_acc: 0.5198 | val_loss: 0.7122 val_acc: 0.5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:34<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss: 0.6933 train_acc: 0.5407 | val_loss: 0.7009 val_acc: 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 919/919 [02:39<00:00,  5.76it/s]\n",
      "100%|█████████████████████████████████████████| 218/218 [00:35<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss: 0.6800 train_acc: 0.5622 | val_loss: 0.6922 val_acc: 0.5401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▌                           | 303/919 [00:52<01:46,  5.80it/s]"
     ]
    }
   ],
   "source": [
    "def training_loop(model, lr, n_epochs, text_train_loader, img_train_loader, text_val_loader, img_val_loader):\n",
    "    criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    last_loss = 1e9\n",
    "    for i in range(n_epochs):\n",
    "        train_acc, train_loss = train_model(model, criterion, optimizer, text_train_loader, img_train_loader)\n",
    "        val_acc, val_loss = eval_model_pairs(model, criterion, optimizer, text_val_loader, img_val_loader)\n",
    "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if val_loss > last_loss:\n",
    "            print(\"Loss increased - early stopping.\")\n",
    "            break\n",
    "        last_loss = val_loss\n",
    "\n",
    "for train_set_num, (train_pairs, y_train, train_scenes) in enumerate(train_sets):\n",
    "    print(\"TRAINING SET\", train_set_num)\n",
    "    # Empty caches\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize image dataset\n",
    "    img_train_data = FurnitureImagePairsDataset(IMAGES_DIR, train_pairs, y_train)\n",
    "    \n",
    "    # Initialize text dataset\n",
    "    train_premise_texts = [item_to_info[id] for id, _ in train_pairs]\n",
    "    train_hypothesis_texts = [item_to_info[id] for _, id in train_pairs]\n",
    "\n",
    "    X_train_text_premise = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_premise_texts)]\n",
    "    X_train_text_premise = pad_sequences(X_train_text_premise, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    X_train_text_hypothesis = [s + [len(WORD_INDEX) + 1] for s in tokenizer.texts_to_sequences(train_hypothesis_texts)]\n",
    "    X_train_text_hypothesis = pad_sequences(X_train_text_hypothesis, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    text_train_data = TensorDataset(torch.from_numpy(X_train_text_premise),\n",
    "                                    torch.from_numpy(X_train_text_hypothesis),\n",
    "                                    torch.from_numpy(y_train))\n",
    "\n",
    "    text_train_loader = DataLoader(text_train_data, batch_size=BATCH_SIZE)\n",
    "    img_train_loader = DataLoader(img_train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Build model\n",
    "    model = make_model()\n",
    "    for param in model.vgg_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.to(DEVICE)\n",
    "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    training_loop(model, MAIN_LEARNING_RATE, EPOCHS,\n",
    "                  text_train_loader, img_train_loader,\n",
    "                  text_val_loader, img_val_loader)\n",
    "    for param in model.image_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    training_loop(model, FINE_TUNE_LEARNING_RATE, EPOCHS,\n",
    "                  text_train_loader, img_train_loader,\n",
    "                  text_val_loader, img_val_loader)\n",
    "    \n",
    "    embs = eval_model_single(model, single_loader)\n",
    "    with open(\"embeddings_{}.p\".format(train_set_num), \"wb\") as file:\n",
    "        pickle.dump((val_product_ids, embs), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3ce50-e058-4895-95eb-14e8a0214078",
   "metadata": {},
   "source": [
    "# Baseline Classifier for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb8cede2-3955-4c85-85cd-760d57c1cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 218/218 [00:15<00:00, 13.94it/s]\n"
     ]
    }
   ],
   "source": [
    "embs_1 = []\n",
    "embs_2 = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for lstm, cnn in tqdm.tqdm(zip(text_val_loader, img_val_loader), total=len(text_val_loader)):\n",
    "        lstm_inp1, lstm_inp2, lstm_labels = lstm\n",
    "        cnn_inp1, cnn_inp2, cnn_labels = cnn\n",
    "        lstm_inp1, lstm_inp2 = lstm_inp1.long().to(DEVICE), lstm_inp2.long().to(DEVICE)\n",
    "        cnn_inp1, cnn_inp2 = cnn_inp1.to(DEVICE), cnn_inp2.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        emb_1 = model.encode(lstm_inp1, cnn_inp1).cpu()\n",
    "        emb_2 = model.encode(lstm_inp2, cnn_inp2).cpu()\n",
    "        # output = (emb_1 * emb_2).sum(1) / (torch.linalg.norm(emb_1, dim=1) * torch.linalg.norm(emb_2, dim=1))\n",
    "        embs_1.append(emb_1)\n",
    "        embs_2.append(emb_2)\n",
    "        labels.append(lstm_labels)\n",
    "        \n",
    "embs_1 = torch.cat(embs_1).numpy()\n",
    "embs_2 = torch.cat(embs_2).numpy()\n",
    "labels = torch.cat(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c597a270-c40b-4fcb-8f0f-f113fa4b94b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6950,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embs_1, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "78200306-8fe3-41d5-ba5d-a28f77a5649a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6950, 256)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "16dd65b8-b01a-40cf-9c8c-7fb330fe11b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(torch.sigmoid((torch.from_numpy(embs_1) * torch.from_numpy(embs_2)).sum(1)).numpy(), labels, alpha=0.1)\n",
    "# plt.scatter([cosine_similarity(x.reshape(1,-1), y.reshape(1,-1)) for x, y in zip(embs_1, embs_2)], labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1997010-6e20-46f1-826b-14a6f79c4264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
